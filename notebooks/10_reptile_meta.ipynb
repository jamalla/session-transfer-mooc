{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea349d8",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Reptile meta-learning starter for session-based recommendation. Uses tasks built from pretraining datasets (Yoochoose / Amazon categories). Produces meta-model that can be adapted quickly to MARS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab62ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\n"
     ]
    }
   ],
   "source": [
    "# Quick (unsafe) workaround to avoid the libiomp5md.dll crash.\n",
    "# Use this only to continue working in the notebook quickly.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "print(\"Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb73b8",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Cell 1: imports & config\n",
    "- Cell 2: hashing utils + map one example\n",
    "- Cell 3: build tasks (scan prefix-target parquet files, convert tokens -> hashed ids, create tasks)\n",
    "- Cell 4: quick task sanity report\n",
    "- Cell 5: SASRecSmall model (same as earlier)\n",
    "- Cell 6: Reptile training loop (train meta-initialization)\n",
    "- Cell 7: Few-shot adaptation to MARS & evaluation\n",
    "- Cell 8: Save results and tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9b879",
   "metadata": {},
   "source": [
    "### Cell 1 — Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25765463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found candidate prefix-target files: 164\n",
      "Device: cuda K= 200000\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - imports & config\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Paths (adjust to your repo layout)\n",
    "ROOT = Path('..')\n",
    "DATA_DIR = ROOT / 'data' / 'processed'\n",
    "META_DIR = DATA_DIR / 'meta_vocab'\n",
    "META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TASKS_OUT = META_DIR / 'tasks_reduced_hashed_top200k.pt'\n",
    "TASKS_CSV = META_DIR / 'tasks_summary_hashed_top200k.csv'\n",
    "\n",
    "# find all candidate prefix-target parquet files recursively (includes amazon parts)\n",
    "prefix_glob = list((DATA_DIR).rglob(\"*prefix*target*.parquet\"))   # recursive search\n",
    "# if you used a different folder for amazon parts, e.g. ../data/processed/amazon_prefix_parts, use:\n",
    "# prefix_glob = list((DATA_DIR/'amazon_prefix_parts').glob(\"*.parquet\"))\n",
    "print(\"Found candidate prefix-target files:\", len(prefix_glob))\n",
    "\n",
    "# Hash vocab size\n",
    "K = 200_000\n",
    "PAD_IDX = 0\n",
    "HASH_MOD = K\n",
    "\n",
    "# Task builder params\n",
    "MIN_PAIRS_PER_TASK = 50   # keep tasks with >= this many pairs\n",
    "MAX_TASKS = 300           # reduce number of tasks for quicker experiments (tunable)\n",
    "MAX_PREFIX_LEN = 20\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE, 'K=', K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1abff",
   "metadata": {},
   "source": [
    "### Cell 2 — Hashing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3edcc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B002KQ6BT6', 22123), ('12345', 97916), ('movie_abc', 96575), ('42', 195175)]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - deterministic hash -> id\n",
    "def token_to_hash_id(token: str, K=HASH_MOD):\n",
    "    # deterministic MD5 hash mapping, returns 1..K (0 reserved for PAD)\n",
    "    if token is None or token == '':\n",
    "        return PAD_IDX\n",
    "    # normalize token to str\n",
    "    s = str(token)\n",
    "    h = hashlib.md5(s.encode('utf-8')).hexdigest()\n",
    "    idx = (int(h, 16) % K) + 1\n",
    "    return idx\n",
    "\n",
    "# quick check\n",
    "examples = ['B002KQ6BT6', '12345', 'movie_abc', '42']\n",
    "print([ (t, token_to_hash_id(t)) for t in examples ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c98b3",
   "metadata": {},
   "source": [
    "### Cell 3 — Build hashed tasks from prefix-target files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ddd22d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning files: 100%|██████████| 164/164 [25:05<00:00,  9.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built tasks: 164\n",
      "Saved tasks: ..\\data\\processed\\meta_vocab\\tasks_reduced_hashed_top200k.pt summary: ..\\data\\processed\\meta_vocab\\tasks_summary_hashed_top200k.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - build tasks\n",
    "from collections import defaultdict\n",
    "tasks = []    # each task: {'name': name, 'P': LongTensor (N, L), 'T': LongTensor (N)}\n",
    "\n",
    "def process_file_to_task(path: Path, max_prefix_len=MAX_PREFIX_LEN):\n",
    "    df = pd.read_parquet(path)\n",
    "    # expect columns 'prefix' and 'target' (prefix as space-separated tokens or list)\n",
    "    # tolerate both formats\n",
    "    P_list = []\n",
    "    T_list = []\n",
    "    for _, r in df.iterrows():\n",
    "        pref = r.get('prefix', '')\n",
    "        # if prefix already stored as list-like, handle; else treat as string\n",
    "        if isinstance(pref, (list, tuple)):\n",
    "            tokens = [str(x) for x in pref if x is not None and x != '']\n",
    "        else:\n",
    "            # assume space-separated token ids or ASINs; handle empty string\n",
    "            tokens = [t for t in str(pref).split() if t != '']\n",
    "        # map tokens via hashing\n",
    "        ids = [token_to_hash_id(t) for t in tokens]\n",
    "        if len(ids) > max_prefix_len:\n",
    "            ids = ids[-max_prefix_len:]\n",
    "        # remove leading PADs if they were created by empty tokens; but keep lengths for nonzeros\n",
    "        if len(ids) == 0:\n",
    "            padded = [PAD_IDX] * max_prefix_len\n",
    "            nonzero_len = 0\n",
    "        else:\n",
    "            padded = [PAD_IDX] * (max_prefix_len - len(ids)) + ids\n",
    "            nonzero_len = sum(1 for x in ids if x != PAD_IDX)\n",
    "        # if target missing, skip\n",
    "        target = r.get('target', None)\n",
    "        if target is None:\n",
    "            continue\n",
    "        # map target to hashed id (string target will be hashed)\n",
    "        tid = token_to_hash_id(target)\n",
    "        P_list.append(padded)\n",
    "        T_list.append(int(tid))\n",
    "    if len(P_list) < MIN_PAIRS_PER_TASK:\n",
    "        return None\n",
    "    P_t = torch.LongTensor(P_list)\n",
    "    T_t = torch.LongTensor(T_list)\n",
    "    # compute example nonzero length stats quickly\n",
    "    nonzero_example_len = (P_t != PAD_IDX).sum(dim=1).clamp(max=MAX_PREFIX_LEN)\n",
    "    return {'name': path.name, 'P': P_t, 'T': T_t, 'n_pairs': P_t.size(0),\n",
    "            'median_nonzero_len': int(nonzero_example_len.median().item()),\n",
    "            'frac_nonzero_gt0': float((nonzero_example_len>0).float().mean().item())}\n",
    "\n",
    "# iterate\n",
    "count = 0\n",
    "for p in tqdm(prefix_glob, desc=\"Scanning files\"):\n",
    "    t = process_file_to_task(p)\n",
    "    if t is None:\n",
    "        continue\n",
    "    tasks.append(t)\n",
    "    count += 1\n",
    "    if MAX_TASKS and count >= MAX_TASKS:\n",
    "        break\n",
    "\n",
    "print(\"Built tasks:\", len(tasks))\n",
    "# Save tasks in compact format: store P and T as tensors (could be large)\n",
    "torch.save(tasks, TASKS_OUT)\n",
    "# Also write CSV summary\n",
    "rows = [{'name': t['name'], 'pairs': t['n_pairs'],\n",
    "         'median_nonzero_len': t['median_nonzero_len'],\n",
    "         'frac_nonzero_gt0': t['frac_nonzero_gt0']} for t in tasks]\n",
    "pd.DataFrame(rows).to_csv(TASKS_CSV, index=False)\n",
    "print(\"Saved tasks:\", TASKS_OUT, \"summary:\", TASKS_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa702d3",
   "metadata": {},
   "source": [
    "### Cell 4 — Quick task sanity report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bfea964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21408\\2817925936.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tasks = torch.load(TASKS_OUT)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tasks loaded: 164\n",
      "amazon_prefix_target_part0000.parquet pairs= 200000 median_nonzero_len= 4 frac_nonzero_gt0= 1.000\n",
      "amazon_prefix_target_part0001.parquet pairs= 200000 median_nonzero_len= 4 frac_nonzero_gt0= 1.000\n",
      "amazon_prefix_target_part0003.parquet pairs= 200000 median_nonzero_len= 4 frac_nonzero_gt0= 1.000\n",
      "amazon_prefix_target_part0004.parquet pairs= 200000 median_nonzero_len= 4 frac_nonzero_gt0= 1.000\n",
      "amazon_prefix_target_part0006.parquet pairs= 200000 median_nonzero_len= 4 frac_nonzero_gt0= 1.000\n",
      "amazon_prefix_target_part0007.parquet pairs= 200000 median_nonzero_len= 4 frac_nonzero_gt0= 1.000\n",
      "amazon_prefix_target_part0009.parquet pairs= 200000 median_nonzero_len= 4 frac_nonzero_gt0= 1.000\n",
      "amazon_prefix_target_part0010.parquet pairs= 200000 median_nonzero_len= 4 frac_nonzero_gt0= 1.000\n",
      "amazon_prefix_target_part0012.parquet pairs= 200000 median_nonzero_len= 4 frac_nonzero_gt0= 1.000\n",
      "amazon_prefix_target_part0013.parquet pairs= 200000 median_nonzero_len= 4 frac_nonzero_gt0= 1.000\n",
      "frac_nonzero_gt0 median: 1.0 mean: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 - Sanity checks\n",
    "tasks = torch.load(TASKS_OUT)\n",
    "print(\"Total tasks loaded:\", len(tasks))\n",
    "# show top 10 by pairs\n",
    "sorted_tasks = sorted(tasks, key=lambda x: x['n_pairs'], reverse=True)\n",
    "for t in sorted_tasks[:10]:\n",
    "    print(t['name'], \"pairs=\", t['n_pairs'], \"median_nonzero_len=\", t['median_nonzero_len'],\n",
    "          \"frac_nonzero_gt0=\", f\"{t['frac_nonzero_gt0']:.3f}\")\n",
    "# distribution of frac_nonzero_gt0\n",
    "fracs = [t['frac_nonzero_gt0'] for t in tasks]\n",
    "print(\"frac_nonzero_gt0 median:\", np.median(fracs), \"mean:\", np.mean(fracs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d086f22",
   "metadata": {},
   "source": [
    "### Cell 5 — SASRecSmall (same architecture as pretrain) — instantiate meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e9f411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-model created. Vocab: 200001\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - SASRecSmall\n",
    "class SASRecSmall(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, max_len=20, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        self.item_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dim_feedforward=2048, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        B, L = x.size()\n",
    "        pos_ids = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        seq = self.item_emb(x) + self.pos_emb(pos_ids)\n",
    "        seq = self.encoder(seq)\n",
    "        last = seq[:, -1, :]\n",
    "        logits = self.out(last)\n",
    "        return logits, last\n",
    "\n",
    "# Create meta-model with hashed vocab\n",
    "META_VOCAB = K + 1  # 0..K\n",
    "meta_model = SASRecSmall(vocab_size=META_VOCAB, embed_dim=64, max_len=MAX_PREFIX_LEN).to(DEVICE)\n",
    "print(\"Meta-model created. Vocab:\", META_VOCAB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07365e6d",
   "metadata": {},
   "source": [
    "### Cell 6 — Reptile meta-training loop (simple version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7445a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21408\\2266945949.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tasks = torch.load(TASKS_OUT)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Reptile meta-training (tasks: 164 )\n",
      "[Reptile] iter 1/1000 quick_recall@20=0.0001\n",
      "[Reptile] iter 50/1000 quick_recall@20=0.0004\n",
      "[Reptile] iter 100/1000 quick_recall@20=0.0003\n",
      "[Reptile] iter 150/1000 quick_recall@20=0.0036\n",
      "[Reptile] iter 200/1000 quick_recall@20=0.0035\n",
      "[Reptile] iter 250/1000 quick_recall@20=0.0007\n",
      "[Reptile] iter 300/1000 quick_recall@20=0.0000\n",
      "[Reptile] iter 350/1000 quick_recall@20=0.0013\n",
      "[Reptile] iter 400/1000 quick_recall@20=0.0035\n",
      "[Reptile] iter 450/1000 quick_recall@20=0.0013\n",
      "[Reptile] iter 500/1000 quick_recall@20=0.0007\n",
      "[Reptile] iter 550/1000 quick_recall@20=0.0026\n",
      "[Reptile] iter 600/1000 quick_recall@20=0.0037\n",
      "[Reptile] iter 650/1000 quick_recall@20=0.0012\n",
      "[Reptile] iter 700/1000 quick_recall@20=0.0008\n",
      "[Reptile] iter 750/1000 quick_recall@20=0.0082\n",
      "[Reptile] iter 800/1000 quick_recall@20=0.0038\n",
      "[Reptile] iter 850/1000 quick_recall@20=0.0008\n",
      "[Reptile] iter 900/1000 quick_recall@20=0.0061\n",
      "[Reptile] iter 950/1000 quick_recall@20=0.0166\n",
      "[Reptile] iter 1000/1000 quick_recall@20=0.0077\n",
      "Saved meta init to: ..\\data\\processed\\meta_vocab\\reptile_meta_state_top200k.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 - Reptile\n",
    "# hyperparams (tune)\n",
    "META_ITERS = 1000            # number of meta-iterations\n",
    "TASK_BATCH = 16              # number of tasks sampled per meta-iteration \n",
    "INNER_STEPS = 16             # SGD steps per task (support) \n",
    "SUPPORT_BATCH = 128          # batch size for support updates\n",
    "INNER_LR = 5e-3\n",
    "META_STEP = 0.05             # step size to move meta weights toward adapted weights\n",
    "VAL_TASKS_SAMPLE = 50       # tasks to evaluate on during meta training (optional)\n",
    "\n",
    "# helper to get minibatches from a task\n",
    "def task_sampler_from_task(tdict):\n",
    "    P = tdict['P']\n",
    "    T = tdict['T']\n",
    "    N = P.size(0)\n",
    "    idxs = np.arange(N)\n",
    "    def gen(batch_size=SUPPORT_BATCH):\n",
    "        np.random.shuffle(idxs)\n",
    "        for i in range(0, N, batch_size):\n",
    "            sel = idxs[i:i+batch_size]\n",
    "            yield P[sel], T[sel]\n",
    "    return gen\n",
    "\n",
    "# utility: copy model parameters (state_dict)\n",
    "def clone_state_dict(state):\n",
    "    return {k: v.clone().detach() for k,v in state.items()}\n",
    "\n",
    "# training loop\n",
    "tasks = torch.load(TASKS_OUT)\n",
    "opt_null = None\n",
    "print(\"Starting Reptile meta-training (tasks:\", len(tasks), \")\")\n",
    "meta_state = meta_model.state_dict()\n",
    "\n",
    "for it in range(META_ITERS):\n",
    "    sampled = np.random.choice(len(tasks), size=min(TASK_BATCH, len(tasks)), replace=False)\n",
    "    meta_state_before = clone_state_dict(meta_state)\n",
    "    adapted_states = []\n",
    "    for tid in sampled:\n",
    "        tinfo = tasks[tid]\n",
    "        # build a small copy model\n",
    "        local_model = SASRecSmall(vocab_size=META_VOCAB, embed_dim=64, max_len=MAX_PREFIX_LEN).to(DEVICE)\n",
    "        local_model.load_state_dict(meta_state)  # start from meta\n",
    "        local_opt = torch.optim.AdamW(local_model.parameters(), lr=INNER_LR, weight_decay=1e-6)\n",
    "        # inner-loop: iterate INNER_STEPS over support batches\n",
    "        gen = task_sampler_from_task(tinfo)()\n",
    "        step = 0\n",
    "        try:\n",
    "            while step < INNER_STEPS:\n",
    "                Xb, yb = next(gen)\n",
    "                Xb = Xb.to(DEVICE)\n",
    "                yb = yb.to(DEVICE)\n",
    "                local_model.train()\n",
    "                _, final = local_model(Xb)\n",
    "                # sampled softmax loss\n",
    "                V = local_model.item_emb.weight.size(0)\n",
    "                pos_scores = (final * local_model.item_emb.weight[yb]).sum(dim=1)\n",
    "                neg_idx = torch.randint(0, V, (Xb.size(0), 32), device=DEVICE)\n",
    "                neg_w = local_model.item_emb.weight[neg_idx]\n",
    "                neg_scores = (neg_w * final.unsqueeze(1)).sum(dim=2)\n",
    "                logits = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)\n",
    "                labels = torch.zeros(Xb.size(0), dtype=torch.long, device=DEVICE)\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "                local_opt.zero_grad(); loss.backward(); local_opt.step()\n",
    "                step += 1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        adapted_states.append(clone_state_dict(local_model.state_dict()))\n",
    "        # free local model\n",
    "        del local_model, local_opt\n",
    "\n",
    "    # meta-update: move meta_state toward average adapted_state\n",
    "    avg_state = {}\n",
    "    for k in meta_state:\n",
    "        stacked = torch.stack([s[k].to('cpu') for s in adapted_states], dim=0)\n",
    "        avg = torch.mean(stacked, dim=0)\n",
    "        avg_state[k] = avg.to(meta_state[k].device)\n",
    "    # apply reptile update: meta = meta + eps * (avg - meta)\n",
    "    for k in meta_state:\n",
    "        meta_state[k] = meta_state[k] + META_STEP * (avg_state[k].to(meta_state[k].device) - meta_state[k])\n",
    "\n",
    "    # every N iterations optionally evaluate quick validation\n",
    "    if (it+1) % 50 == 0 or it == 0:\n",
    "        # compute a very cheap diagnostic: random task few-shot adapt and eval on its heldout pairs\n",
    "        # we'll do one quick task eval to monitor progress\n",
    "        idx = np.random.randint(len(tasks))\n",
    "        tdiag = tasks[idx]\n",
    "        # split task into support/query\n",
    "        N = tdiag['P'].size(0)\n",
    "        qn = max(1, int(0.2 * N))\n",
    "        perm = np.random.permutation(N)\n",
    "        sup_idx = perm[:-qn]; qry_idx = perm[-qn:]\n",
    "        # adapt from meta_state for a few steps\n",
    "        tmp_model = SASRecSmall(vocab_size=META_VOCAB, embed_dim=64, max_len=MAX_PREFIX_LEN).to(DEVICE)\n",
    "        tmp_model.load_state_dict(meta_state)\n",
    "        tmp_opt = torch.optim.AdamW(tmp_model.parameters(), lr=INNER_LR)\n",
    "        # support steps\n",
    "        for s in range(5):\n",
    "            sel = sup_idx[s::5][:SUPPORT_BATCH] if len(sup_idx)>0 else sup_idx\n",
    "            if len(sel)==0: break\n",
    "            Xb = tdiag['P'][sel].to(DEVICE)\n",
    "            yb = tdiag['T'][sel].to(DEVICE)\n",
    "            _, final = tmp_model(Xb)\n",
    "            pos_scores = (final * tmp_model.item_emb.weight[yb]).sum(dim=1)\n",
    "            neg_idx = torch.randint(0, tmp_model.item_emb.weight.size(0), (Xb.size(0), 32), device=DEVICE)\n",
    "            neg_w = tmp_model.item_emb.weight[neg_idx]\n",
    "            neg_scores = (neg_w * final.unsqueeze(1)).sum(dim=2)\n",
    "            logits = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)\n",
    "            loss = F.cross_entropy(logits, torch.zeros(Xb.size(0), dtype=torch.long, device=DEVICE))\n",
    "            tmp_opt.zero_grad(); loss.backward(); tmp_opt.step()\n",
    "        # evaluate on query\n",
    "        hits = 0; total = 0\n",
    "        for qi in qry_idx:\n",
    "            Xq = tdiag['P'][qi].unsqueeze(0).to(DEVICE)\n",
    "            tq = int(tdiag['T'][qi].item())\n",
    "            _, final = tmp_model(Xq)\n",
    "            scores = torch.matmul(final, tmp_model.item_emb.weight.t())\n",
    "            topk = scores.topk(20, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            total += 1\n",
    "            if tq in topk: hits += 1\n",
    "        quick_recall = hits / total if total>0 else 0.0\n",
    "        print(f\"[Reptile] iter {it+1}/{META_ITERS} quick_recall@20={quick_recall:.4f}\")\n",
    "\n",
    "# After meta loop - save meta_state as meta initialization\n",
    "meta_model.load_state_dict(meta_state)\n",
    "torch.save({'meta_state': meta_state}, META_DIR / 'reptile_meta_state_top200k.pt')\n",
    "print(\"Saved meta init to:\", META_DIR / 'reptile_meta_state_top200k.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff57e4",
   "metadata": {},
   "source": [
    "### Cell 7 — Few-shot adaptation to MARS and final eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69190a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21408\\710123239.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mp = torch.load(MARS_SHARD_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built hashed MARS pairs: 2380\n",
      "MARS splits: train 1904 val 238 test 238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21408\\710123239.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta_ck = torch.load(META_DIR / 'reptile_meta_state_top200k.pt', map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot K=10 -> Recall@20=0.0000, MRR=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21408\\710123239.py:78: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rank = int((topk == target).nonzero()[0]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot K=50 -> Recall@20=0.0042, MRR=0.0003\n",
      "Few-shot K=100 -> Recall@20=0.0000, MRR=0.0000\n",
      "Few-shot K=200 -> Recall@20=0.0000, MRR=0.0000\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 - Adapt to MARS (few-shot) and evaluate\n",
    "# load MARS shard (built earlier), map using same hashing\n",
    "MARS_SHARD_FILE = DATA_DIR / 'mars_shards' / 'mars_shard_full.pt'\n",
    "if not MARS_SHARD_FILE.exists():\n",
    "    raise FileNotFoundError(\"Please ensure MARS shard exists (built in 07_transfer_to_mars).\")\n",
    "\n",
    "mp = torch.load(MARS_SHARD_FILE)\n",
    "P_all = mp['prefix']   # NOTE: these were constructed earlier with original item2id mapping; we re-hash tokens here\n",
    "# We need to rebuild a hashed MARS shard to be consistent. If your MARS_shard already uses text tokens, rebuild; otherwise convert.\n",
    "# For simplicity assume mars_prefix_target.parquet exists and we rebuild hashed prefixes here\n",
    "MARS_PAIRS = DATA_DIR / 'mars_prefix_target.parquet'\n",
    "if not MARS_PAIRS.exists():\n",
    "    raise FileNotFoundError(\"Please create mars_prefix_target.parquet first.\")\n",
    "df_mars_pairs = pd.read_parquet(MARS_PAIRS)\n",
    "# build hashed MARS tensors\n",
    "P_list = []\n",
    "T_list = []\n",
    "for _, r in df_mars_pairs.iterrows():\n",
    "    pref = r['prefix'] if isinstance(r['prefix'], str) else ''\n",
    "    tokens = [t for t in str(pref).split() if t!='']\n",
    "    ids = [token_to_hash_id(t) for t in tokens]\n",
    "    if len(ids) > MAX_PREFIX_LEN: ids = ids[-MAX_PREFIX_LEN:]\n",
    "    padded = [PAD_IDX]*(MAX_PREFIX_LEN - len(ids)) + ids\n",
    "    P_list.append(padded)\n",
    "    T_list.append(token_to_hash_id(r['target']))\n",
    "P_H = torch.LongTensor(P_list)\n",
    "T_H = torch.LongTensor(T_list)\n",
    "print(\"Built hashed MARS pairs:\", P_H.size(0))\n",
    "\n",
    "# Split train/val/test\n",
    "n = P_H.size(0)\n",
    "test_n = max(1, int(0.1*n))\n",
    "val_n = max(1, int(0.1*n))\n",
    "train_n = n - val_n - test_n\n",
    "train_P, train_T = P_H[:train_n].to(DEVICE), T_H[:train_n].to(DEVICE)\n",
    "val_P, val_T = P_H[train_n:train_n+val_n].to(DEVICE), T_H[train_n:train_n+val_n].to(DEVICE)\n",
    "test_P, test_T = P_H[train_n+val_n:].to(DEVICE), T_H[train_n+val_n:].to(DEVICE)\n",
    "print(\"MARS splits: train\", train_P.size(0), \"val\", val_P.size(0), \"test\", test_P.size(0))\n",
    "\n",
    "# Load meta init\n",
    "meta_ck = torch.load(META_DIR / 'reptile_meta_state_top200k.pt', map_location=DEVICE)\n",
    "meta_state = meta_ck['meta_state']\n",
    "adapt_model = SASRecSmall(vocab_size=META_VOCAB, embed_dim=64, max_len=MAX_PREFIX_LEN).to(DEVICE)\n",
    "adapt_model.load_state_dict(meta_state)\n",
    "\n",
    "# Few-shot fine-tune (support small K shots) — try different K_shots\n",
    "def adapt_and_eval(K_shots=50, adapt_steps=10, lr=1e-4):\n",
    "    # sample K_shots from train\n",
    "    idxs = np.random.choice(train_P.size(0), size=min(K_shots, train_P.size(0)), replace=False)\n",
    "    Xs = train_P[idxs]\n",
    "    ys = train_T[idxs]\n",
    "    model = SASRecSmall(vocab_size=META_VOCAB, embed_dim=64, max_len=MAX_PREFIX_LEN).to(DEVICE)\n",
    "    model.load_state_dict(meta_state)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    for s in range(adapt_steps):\n",
    "        model.train()\n",
    "        _, final = model(Xs)\n",
    "        pos_scores = (final * model.item_emb.weight[ys]).sum(dim=1)\n",
    "        neg_idx = torch.randint(0, model.item_emb.weight.size(0), (Xs.size(0), 32), device=DEVICE)\n",
    "        neg_w = model.item_emb.weight[neg_idx]\n",
    "        neg_scores = (neg_w * final.unsqueeze(1)).sum(dim=2)\n",
    "        logits = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)\n",
    "        loss = F.cross_entropy(logits, torch.zeros(Xs.size(0), dtype=torch.long, device=DEVICE))\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    # evaluate on test\n",
    "    model.eval()\n",
    "    hits = 0; rr_sum = 0.0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(test_P.size(0)):\n",
    "            Xq = test_P[i].unsqueeze(0)\n",
    "            target = int(test_T[i].item())\n",
    "            _, final = model(Xq)\n",
    "            scores = torch.matmul(final, model.item_emb.weight.t())\n",
    "            topk = scores.topk(20, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            total += 1\n",
    "            if target in topk:\n",
    "                hits += 1\n",
    "                rank = int((topk == target).nonzero()[0]) + 1\n",
    "                rr_sum += 1.0 / rank\n",
    "    recall = hits/total if total>0 else 0.0\n",
    "    mrr = rr_sum/total if total>0 else 0.0\n",
    "    return recall, mrr\n",
    "\n",
    "for K_shots in [10, 50, 100, 200]:\n",
    "    r, m = adapt_and_eval(K_shots=K_shots, adapt_steps=10, lr=1e-4)\n",
    "    print(f\"Few-shot K={K_shots} -> Recall@20={r:.4f}, MRR={m:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b5eca",
   "metadata": {},
   "source": [
    "### Cell 8 — Save & short tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021cdcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved meta info.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - Save and tips\n",
    "torch.save({'meta_state': meta_state, 'K': K, 'pad': PAD_IDX}, META_DIR / 'meta_info_top200k.pt')\n",
    "print(\"Saved meta info.\")\n",
    "\n",
    "# Quick tips:\n",
    "# - If many tasks still have very low frac_nonzero_gt0 (<0.05) consider increasing K or using per-file remapping.\n",
    "# - Reptile hyperparams: TASK_BATCH, INNER_STEPS, META_STEP - tune depending on compute.\n",
    "# - Consider adapter layers (09_adapters) combined with Reptile: adapt fewer params and get robust few-shot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5224b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tasks from: ..\\data\\processed\\meta_vocab\\tasks_reduced_hashed_top200k.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21408\\2818185.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tasks = torch.load(TASKS_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens counted: 200000\n",
      "Built meta map with 200000 tokens (plus <OOV>). Saving to: ..\\data\\processed\\meta_vocab\\meta_item2id_top200k.json\n",
      "sample meta overlap (first 50 tasks): 3179118 3179168 0.9999842726147219\n",
      "sample mapping entries: [('<OOV>', 0), ('154710', 1), ('34011', 2), ('31351', 3), ('100781', 4), ('76434', 5), ('108491', 6), ('29454', 7), ('127543', 8), ('188382', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Build meta_item2id_top200k.json from tasks and re-run the overlap diagnostic\n",
    "# Paste and run this in the same notebook/kernel after your previous cells.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path('..')\n",
    "META_DIR = ROOT / 'data' / 'processed' / 'meta_vocab'\n",
    "META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TASKS_FILE = META_DIR / 'tasks_reduced_hashed_top200k.pt'   # adjust if your tasks file has a different name\n",
    "OUT_MAP = META_DIR / 'meta_item2id_top200k.json'\n",
    "TOPK = 200000   # your chosen K (you set K=200k earlier)\n",
    "\n",
    "print(\"Loading tasks from:\", TASKS_FILE)\n",
    "tasks = torch.load(TASKS_FILE)\n",
    "\n",
    "# Count token frequencies across tasks (tokens stored as ints in P tensors)\n",
    "cnt = Counter()\n",
    "for t in tasks:\n",
    "    P = t['P']  # torch.LongTensor (n_pairs, L)\n",
    "    # flatten and count non-zero tokens (exclude padding 0)\n",
    "    flat = P.view(-1).cpu().numpy()\n",
    "    flat_nonzero = flat[flat != 0]\n",
    "    cnt.update(flat_nonzero.tolist())\n",
    "\n",
    "print(\"Unique tokens counted:\", len(cnt))\n",
    "\n",
    "# select top-K most frequent tokens\n",
    "most_common = cnt.most_common(TOPK)\n",
    "meta_item2id = {}\n",
    "# reserve 0 for OOV/pad\n",
    "meta_item2id['<OOV>'] = 0\n",
    "next_id = 1\n",
    "for token, freq in most_common:\n",
    "    # store keys as strings (your downstream code expects string keys)\n",
    "    meta_item2id[str(int(token))] = next_id\n",
    "    next_id += 1\n",
    "\n",
    "print(f\"Built meta map with {len(meta_item2id)-1} tokens (plus <OOV>). Saving to:\", OUT_MAP)\n",
    "with open(OUT_MAP, 'w') as f:\n",
    "    json.dump(meta_item2id, f)\n",
    "\n",
    "# Quick overlap diagnostic (same as before) to confirm\n",
    "hits = 0\n",
    "tot = 0\n",
    "sample_tasks = tasks[:min(len(tasks),50)]\n",
    "for t in sample_tasks:\n",
    "    P = t['P']\n",
    "    flat = np.unique(P.view(-1).cpu().numpy())\n",
    "    tot += len(flat)\n",
    "    hits += sum(1 for v in flat if str(int(v)) in meta_item2id)\n",
    "print(\"sample meta overlap (first 50 tasks):\", hits, tot, (hits/tot if tot>0 else 0.0))\n",
    "\n",
    "# Print a few entries to sanity-check\n",
    "some_keys = list(meta_item2id.items())[:10]\n",
    "print(\"sample mapping entries:\", some_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81fbc147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21408\\3550003532.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tasks = torch.load(TASKS_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded: 164\n",
      "        task_idx          pairs  frac_nonzero_prefix\n",
      "count  164.00000     164.000000                164.0\n",
      "mean    81.50000  135930.170732                  1.0\n",
      "std     47.48684   89265.090386                  0.0\n",
      "min      0.00000    2380.000000                  1.0\n",
      "25%     40.75000   12800.000000                  1.0\n",
      "50%     81.50000  200000.000000                  1.0\n",
      "75%    122.25000  200000.000000                  1.0\n",
      "max    163.00000  200000.000000                  1.0\n",
      "Examples (first 10):\n",
      "   task_idx   pairs  frac_nonzero_prefix\n",
      "0         0    2380                  1.0\n",
      "1         1  200000                  1.0\n",
      "2         2  200000                  1.0\n",
      "3         3   12544                  1.0\n",
      "4         4  200000                  1.0\n",
      "5         5  200000                  1.0\n",
      "6         6   10989                  1.0\n",
      "7         7  200000                  1.0\n",
      "8         8  200000                  1.0\n",
      "9         9   12115                  1.0\n",
      "meta vocab size: 200001\n",
      "sample meta overlap (first 50 tasks): 3179118 3179168 0.9999842726147219\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTICS: tasks sanity & vocab overlap\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TASKS_FILE = Path(\"..\") / \"data\" / \"processed\" / \"meta_vocab\" / \"tasks_reduced_hashed_top200k.pt\"\n",
    "META_MAP = Path(\"..\") / \"data\" / \"processed\" / \"meta_vocab\" / \"meta_item2id_top200k.json\"\n",
    "MARS_VOCAB = Path(\"..\") / \"data\" / \"processed\" / \"vocab_mars\" / \"item2id_mars.json\"\n",
    "\n",
    "tasks = torch.load(TASKS_FILE)\n",
    "print(\"Tasks loaded:\", len(tasks))\n",
    "\n",
    "# per-task stats\n",
    "rows = []\n",
    "for i, t in enumerate(tasks):\n",
    "    p = t['P'] if isinstance(t, dict) else t['P']\n",
    "    nonzero = (p.sum(dim=1) > 0).float().mean().item()\n",
    "    rows.append((i, len(p), float(nonzero)))\n",
    "df = pd.DataFrame(rows, columns=['task_idx','pairs','frac_nonzero_prefix'])\n",
    "print(df.describe())\n",
    "print(\"Examples (first 10):\")\n",
    "print(df.head(10))\n",
    "\n",
    "# vocab overlap\n",
    "meta_map = json.load(open(META_MAP))\n",
    "meta_vsize = len(meta_map)\n",
    "print(\"meta vocab size:\", meta_vsize)\n",
    "\n",
    "# check sample of task tokens => how many map to meta vocab\n",
    "def sample_vocab_overlap(tasks, ncheck=1000):\n",
    "    hits=0; tot=0\n",
    "    for t in tasks[:min(len(tasks),50)]:  # sample first 50 tasks\n",
    "        P = t['P']\n",
    "        flat = P.view(-1).unique().cpu().numpy()\n",
    "        tot += len(flat)\n",
    "        hits += sum(1 for v in flat if str(int(v)) in meta_map)  # tokens saved as strings\n",
    "    return hits, tot, hits/tot if tot>0 else 0.0\n",
    "\n",
    "hits,tot,frac = sample_vocab_overlap(tasks)\n",
    "print(\"sample meta overlap (first 50 tasks):\", hits, tot, frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc4569ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTERS + Reptile meta-training (adapter inner updates)\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import random\n",
    "from tqdm.auto import trange\n",
    "\n",
    "# small adapter module\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim, bottleneck=32):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(dim, bottleneck)\n",
    "        self.act = nn.ReLU()\n",
    "        self.up = nn.Linear(bottleneck, dim)\n",
    "    def forward(self, x):\n",
    "        return self.up(self.act(self.down(x))) + x\n",
    "\n",
    "# attach adapters to a SASRecSmall instance (in-place)\n",
    "def attach_adapters(model, bottleneck=32):\n",
    "    adapters = {}\n",
    "    # find encoder layers\n",
    "    for i,layer in enumerate(model.encoder.layers):\n",
    "        a = Adapter(model.embed_dim, bottleneck=bottleneck)\n",
    "        layer.adapter = a\n",
    "        adapters[f\"encoder.layers.{i}.adapter\"] = a\n",
    "    # also add adapter on final projection if desired\n",
    "    model.adapters = adapters\n",
    "    return model\n",
    "\n",
    "# apply adapter forward: modify SASRecSmall.forward to use adapter if present\n",
    "_orig_forward = SASRecSmall.forward\n",
    "def _forward_with_adapters(self, x):\n",
    "    B, L = x.size()\n",
    "    pos_ids = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "    seq = self.item_emb(x) + self.pos_emb(pos_ids)\n",
    "    # pass through encoder layers and apply adapter after each encoder layer\n",
    "    for i,layer in enumerate(self.encoder.layers):\n",
    "        seq = layer(seq)\n",
    "        if hasattr(layer, \"adapter\"):\n",
    "            seq = layer.adapter(seq)\n",
    "    last = seq\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
