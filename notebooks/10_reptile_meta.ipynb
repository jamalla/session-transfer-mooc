{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea349d8",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Reptile meta-learning starter for session-based recommendation. Uses tasks built from pretraining datasets (Yoochoose / Amazon categories). Produces meta-model that can be adapted quickly to MARS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab62ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\n"
     ]
    }
   ],
   "source": [
    "# Quick (unsafe) workaround to avoid the libiomp5md.dll crash.\n",
    "# Use this only to continue working in the notebook quickly.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "print(\"Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb73b8",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Cell 1: imports & config\n",
    "- Cell 2: hashing utils + map one example\n",
    "- Cell 3: build tasks (scan prefix-target parquet files, convert tokens -> hashed ids, create tasks)\n",
    "- Cell 4: quick task sanity report\n",
    "- Cell 5: SASRecSmall model (same as earlier)\n",
    "- Cell 6: Reptile training loop (train meta-initialization)\n",
    "- Cell 7: Few-shot adaptation to MARS & evaluation\n",
    "- Cell 8: Save results and tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9b879",
   "metadata": {},
   "source": [
    "### Cell 1 — Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25765463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found candidate prefix-target files: 1\n",
      "Device: cuda K= 200000\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - imports & config\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Paths (adjust to your repo layout)\n",
    "ROOT = Path('..')\n",
    "DATA_DIR = ROOT / 'data' / 'processed'\n",
    "META_DIR = DATA_DIR / 'meta_vocab'\n",
    "META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TASKS_OUT = META_DIR / 'tasks_reduced_hashed_top200k.pt'\n",
    "TASKS_CSV = META_DIR / 'tasks_summary_hashed_top200k.csv'\n",
    "\n",
    "# Found prefix-target files (update pattern if different)\n",
    "prefix_glob = list((DATA_DIR).glob(\"*prefix*target*.parquet\"))  # adjust if your naming differs\n",
    "print(\"Found candidate prefix-target files:\", len(prefix_glob))\n",
    "\n",
    "# Hash vocab size\n",
    "K = 200_000\n",
    "PAD_IDX = 0\n",
    "HASH_MOD = K\n",
    "\n",
    "# Task builder params\n",
    "MIN_PAIRS_PER_TASK = 50   # keep tasks with >= this many pairs\n",
    "MAX_TASKS = 300           # reduce number of tasks for quicker experiments (tunable)\n",
    "MAX_PREFIX_LEN = 20\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE, 'K=', K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1abff",
   "metadata": {},
   "source": [
    "### Cell 2 — Hashing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3edcc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B002KQ6BT6', 22123), ('12345', 97916), ('movie_abc', 96575), ('42', 195175)]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - deterministic hash -> id\n",
    "def token_to_hash_id(token: str, K=HASH_MOD):\n",
    "    # deterministic MD5 hash mapping, returns 1..K (0 reserved for PAD)\n",
    "    if token is None or token == '':\n",
    "        return PAD_IDX\n",
    "    # normalize token to str\n",
    "    s = str(token)\n",
    "    h = hashlib.md5(s.encode('utf-8')).hexdigest()\n",
    "    idx = (int(h, 16) % K) + 1\n",
    "    return idx\n",
    "\n",
    "# quick check\n",
    "examples = ['B002KQ6BT6', '12345', 'movie_abc', '42']\n",
    "print([ (t, token_to_hash_id(t)) for t in examples ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c98b3",
   "metadata": {},
   "source": [
    "### Cell 3 — Build hashed tasks from prefix-target files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ddd22d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning files: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built tasks: 1\n",
      "Saved tasks: ..\\data\\processed\\meta_vocab\\tasks_reduced_hashed_top200k.pt summary: ..\\data\\processed\\meta_vocab\\tasks_summary_hashed_top200k.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - build tasks\n",
    "from collections import defaultdict\n",
    "tasks = []    # each task: {'name': name, 'P': LongTensor (N, L), 'T': LongTensor (N)}\n",
    "\n",
    "def process_file_to_task(path: Path, max_prefix_len=MAX_PREFIX_LEN):\n",
    "    df = pd.read_parquet(path)\n",
    "    # expect columns 'prefix' and 'target' (prefix as space-separated tokens or list)\n",
    "    # tolerate both formats\n",
    "    P_list = []\n",
    "    T_list = []\n",
    "    for _, r in df.iterrows():\n",
    "        pref = r.get('prefix', '')\n",
    "        # if prefix already stored as list-like, handle; else treat as string\n",
    "        if isinstance(pref, (list, tuple)):\n",
    "            tokens = [str(x) for x in pref if x is not None and x != '']\n",
    "        else:\n",
    "            # assume space-separated token ids or ASINs; handle empty string\n",
    "            tokens = [t for t in str(pref).split() if t != '']\n",
    "        # map tokens via hashing\n",
    "        ids = [token_to_hash_id(t) for t in tokens]\n",
    "        if len(ids) > max_prefix_len:\n",
    "            ids = ids[-max_prefix_len:]\n",
    "        # remove leading PADs if they were created by empty tokens; but keep lengths for nonzeros\n",
    "        if len(ids) == 0:\n",
    "            padded = [PAD_IDX] * max_prefix_len\n",
    "            nonzero_len = 0\n",
    "        else:\n",
    "            padded = [PAD_IDX] * (max_prefix_len - len(ids)) + ids\n",
    "            nonzero_len = sum(1 for x in ids if x != PAD_IDX)\n",
    "        # if target missing, skip\n",
    "        target = r.get('target', None)\n",
    "        if target is None:\n",
    "            continue\n",
    "        # map target to hashed id (string target will be hashed)\n",
    "        tid = token_to_hash_id(target)\n",
    "        P_list.append(padded)\n",
    "        T_list.append(int(tid))\n",
    "    if len(P_list) < MIN_PAIRS_PER_TASK:\n",
    "        return None\n",
    "    P_t = torch.LongTensor(P_list)\n",
    "    T_t = torch.LongTensor(T_list)\n",
    "    # compute example nonzero length stats quickly\n",
    "    nonzero_example_len = (P_t != PAD_IDX).sum(dim=1).clamp(max=MAX_PREFIX_LEN)\n",
    "    return {'name': path.name, 'P': P_t, 'T': T_t, 'n_pairs': P_t.size(0),\n",
    "            'median_nonzero_len': int(nonzero_example_len.median().item()),\n",
    "            'frac_nonzero_gt0': float((nonzero_example_len>0).float().mean().item())}\n",
    "\n",
    "# iterate\n",
    "count = 0\n",
    "for p in tqdm(prefix_glob, desc=\"Scanning files\"):\n",
    "    t = process_file_to_task(p)\n",
    "    if t is None:\n",
    "        continue\n",
    "    tasks.append(t)\n",
    "    count += 1\n",
    "    if MAX_TASKS and count >= MAX_TASKS:\n",
    "        break\n",
    "\n",
    "print(\"Built tasks:\", len(tasks))\n",
    "# Save tasks in compact format: store P and T as tensors (could be large)\n",
    "torch.save(tasks, TASKS_OUT)\n",
    "# Also write CSV summary\n",
    "rows = [{'name': t['name'], 'pairs': t['n_pairs'],\n",
    "         'median_nonzero_len': t['median_nonzero_len'],\n",
    "         'frac_nonzero_gt0': t['frac_nonzero_gt0']} for t in tasks]\n",
    "pd.DataFrame(rows).to_csv(TASKS_CSV, index=False)\n",
    "print(\"Saved tasks:\", TASKS_OUT, \"summary:\", TASKS_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa702d3",
   "metadata": {},
   "source": [
    "### Cell 4 — Quick task sanity report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bfea964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tasks loaded: 1\n",
      "mars_prefix_target.parquet pairs= 2380 median_nonzero_len= 4 frac_nonzero_gt0= 1.000\n",
      "frac_nonzero_gt0 median: 1.0 mean: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_22352\\2817925936.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tasks = torch.load(TASKS_OUT)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 - Sanity checks\n",
    "tasks = torch.load(TASKS_OUT)\n",
    "print(\"Total tasks loaded:\", len(tasks))\n",
    "# show top 10 by pairs\n",
    "sorted_tasks = sorted(tasks, key=lambda x: x['n_pairs'], reverse=True)\n",
    "for t in sorted_tasks[:10]:\n",
    "    print(t['name'], \"pairs=\", t['n_pairs'], \"median_nonzero_len=\", t['median_nonzero_len'],\n",
    "          \"frac_nonzero_gt0=\", f\"{t['frac_nonzero_gt0']:.3f}\")\n",
    "# distribution of frac_nonzero_gt0\n",
    "fracs = [t['frac_nonzero_gt0'] for t in tasks]\n",
    "print(\"frac_nonzero_gt0 median:\", np.median(fracs), \"mean:\", np.mean(fracs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d086f22",
   "metadata": {},
   "source": [
    "### Cell 5 — SASRecSmall (same architecture as pretrain) — instantiate meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e9f411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-model created. Vocab: 200001\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - SASRecSmall\n",
    "class SASRecSmall(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, max_len=20, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        self.item_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dim_feedforward=2048, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        B, L = x.size()\n",
    "        pos_ids = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        seq = self.item_emb(x) + self.pos_emb(pos_ids)\n",
    "        seq = self.encoder(seq)\n",
    "        last = seq[:, -1, :]\n",
    "        logits = self.out(last)\n",
    "        return logits, last\n",
    "\n",
    "# Create meta-model with hashed vocab\n",
    "META_VOCAB = K + 1  # 0..K\n",
    "meta_model = SASRecSmall(vocab_size=META_VOCAB, embed_dim=64, max_len=MAX_PREFIX_LEN).to(DEVICE)\n",
    "print(\"Meta-model created. Vocab:\", META_VOCAB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07365e6d",
   "metadata": {},
   "source": [
    "### Cell 6 — Reptile meta-training loop (simple version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7445a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_22352\\160974898.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tasks = torch.load(TASKS_OUT)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Reptile meta-training (tasks: 1 )\n",
      "[Reptile] iter 1/500 quick_recall@20=0.0021\n",
      "[Reptile] iter 50/500 quick_recall@20=0.0042\n",
      "[Reptile] iter 100/500 quick_recall@20=0.0000\n",
      "[Reptile] iter 150/500 quick_recall@20=0.0084\n",
      "[Reptile] iter 200/500 quick_recall@20=0.0042\n",
      "[Reptile] iter 250/500 quick_recall@20=0.0210\n",
      "[Reptile] iter 300/500 quick_recall@20=0.0084\n",
      "[Reptile] iter 350/500 quick_recall@20=0.0357\n",
      "[Reptile] iter 400/500 quick_recall@20=0.0252\n",
      "[Reptile] iter 450/500 quick_recall@20=0.0399\n",
      "[Reptile] iter 500/500 quick_recall@20=0.0315\n",
      "Saved meta init to: ..\\data\\processed\\meta_vocab\\reptile_meta_state_top200k.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 - Reptile\n",
    "# hyperparams (tune)\n",
    "META_ITERS = 500            # number of meta-iterations\n",
    "TASK_BATCH = 4              # number of tasks sampled per meta-iteration\n",
    "INNER_STEPS = 5             # SGD steps per task (support)\n",
    "SUPPORT_BATCH = 64          # batch size for support updates\n",
    "INNER_LR = 1e-3\n",
    "META_STEP = 0.1             # step size to move meta weights toward adapted weights\n",
    "VAL_TASKS_SAMPLE = 50       # tasks to evaluate on during meta training (optional)\n",
    "\n",
    "# helper to get minibatches from a task\n",
    "def task_sampler_from_task(tdict):\n",
    "    P = tdict['P']\n",
    "    T = tdict['T']\n",
    "    N = P.size(0)\n",
    "    idxs = np.arange(N)\n",
    "    def gen(batch_size=SUPPORT_BATCH):\n",
    "        np.random.shuffle(idxs)\n",
    "        for i in range(0, N, batch_size):\n",
    "            sel = idxs[i:i+batch_size]\n",
    "            yield P[sel], T[sel]\n",
    "    return gen\n",
    "\n",
    "# utility: copy model parameters (state_dict)\n",
    "def clone_state_dict(state):\n",
    "    return {k: v.clone().detach() for k,v in state.items()}\n",
    "\n",
    "# training loop\n",
    "tasks = torch.load(TASKS_OUT)\n",
    "opt_null = None\n",
    "print(\"Starting Reptile meta-training (tasks:\", len(tasks), \")\")\n",
    "meta_state = meta_model.state_dict()\n",
    "\n",
    "for it in range(META_ITERS):\n",
    "    sampled = np.random.choice(len(tasks), size=min(TASK_BATCH, len(tasks)), replace=False)\n",
    "    meta_state_before = clone_state_dict(meta_state)\n",
    "    adapted_states = []\n",
    "    for tid in sampled:\n",
    "        tinfo = tasks[tid]\n",
    "        # build a small copy model\n",
    "        local_model = SASRecSmall(vocab_size=META_VOCAB, embed_dim=64, max_len=MAX_PREFIX_LEN).to(DEVICE)\n",
    "        local_model.load_state_dict(meta_state)  # start from meta\n",
    "        local_opt = torch.optim.AdamW(local_model.parameters(), lr=INNER_LR, weight_decay=1e-6)\n",
    "        # inner-loop: iterate INNER_STEPS over support batches\n",
    "        gen = task_sampler_from_task(tinfo)()\n",
    "        step = 0\n",
    "        try:\n",
    "            while step < INNER_STEPS:\n",
    "                Xb, yb = next(gen)\n",
    "                Xb = Xb.to(DEVICE)\n",
    "                yb = yb.to(DEVICE)\n",
    "                local_model.train()\n",
    "                _, final = local_model(Xb)\n",
    "                # sampled softmax loss\n",
    "                V = local_model.item_emb.weight.size(0)\n",
    "                pos_scores = (final * local_model.item_emb.weight[yb]).sum(dim=1)\n",
    "                neg_idx = torch.randint(0, V, (Xb.size(0), 32), device=DEVICE)\n",
    "                neg_w = local_model.item_emb.weight[neg_idx]\n",
    "                neg_scores = (neg_w * final.unsqueeze(1)).sum(dim=2)\n",
    "                logits = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)\n",
    "                labels = torch.zeros(Xb.size(0), dtype=torch.long, device=DEVICE)\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "                local_opt.zero_grad(); loss.backward(); local_opt.step()\n",
    "                step += 1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        adapted_states.append(clone_state_dict(local_model.state_dict()))\n",
    "        # free local model\n",
    "        del local_model, local_opt\n",
    "\n",
    "    # meta-update: move meta_state toward average adapted_state\n",
    "    avg_state = {}\n",
    "    for k in meta_state:\n",
    "        stacked = torch.stack([s[k].to('cpu') for s in adapted_states], dim=0)\n",
    "        avg = torch.mean(stacked, dim=0)\n",
    "        avg_state[k] = avg.to(meta_state[k].device)\n",
    "    # apply reptile update: meta = meta + eps * (avg - meta)\n",
    "    for k in meta_state:\n",
    "        meta_state[k] = meta_state[k] + META_STEP * (avg_state[k].to(meta_state[k].device) - meta_state[k])\n",
    "\n",
    "    # every N iterations optionally evaluate quick validation\n",
    "    if (it+1) % 50 == 0 or it == 0:\n",
    "        # compute a very cheap diagnostic: random task few-shot adapt and eval on its heldout pairs\n",
    "        # we'll do one quick task eval to monitor progress\n",
    "        idx = np.random.randint(len(tasks))\n",
    "        tdiag = tasks[idx]\n",
    "        # split task into support/query\n",
    "        N = tdiag['P'].size(0)\n",
    "        qn = max(1, int(0.2 * N))\n",
    "        perm = np.random.permutation(N)\n",
    "        sup_idx = perm[:-qn]; qry_idx = perm[-qn:]\n",
    "        # adapt from meta_state for a few steps\n",
    "        tmp_model = SASRecSmall(vocab_size=META_VOCAB, embed_dim=64, max_len=MAX_PREFIX_LEN).to(DEVICE)\n",
    "        tmp_model.load_state_dict(meta_state)\n",
    "        tmp_opt = torch.optim.AdamW(tmp_model.parameters(), lr=INNER_LR)\n",
    "        # support steps\n",
    "        for s in range(5):\n",
    "            sel = sup_idx[s::5][:SUPPORT_BATCH] if len(sup_idx)>0 else sup_idx\n",
    "            if len(sel)==0: break\n",
    "            Xb = tdiag['P'][sel].to(DEVICE)\n",
    "            yb = tdiag['T'][sel].to(DEVICE)\n",
    "            _, final = tmp_model(Xb)\n",
    "            pos_scores = (final * tmp_model.item_emb.weight[yb]).sum(dim=1)\n",
    "            neg_idx = torch.randint(0, tmp_model.item_emb.weight.size(0), (Xb.size(0), 32), device=DEVICE)\n",
    "            neg_w = tmp_model.item_emb.weight[neg_idx]\n",
    "            neg_scores = (neg_w * final.unsqueeze(1)).sum(dim=2)\n",
    "            logits = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)\n",
    "            loss = F.cross_entropy(logits, torch.zeros(Xb.size(0), dtype=torch.long, device=DEVICE))\n",
    "            tmp_opt.zero_grad(); loss.backward(); tmp_opt.step()\n",
    "        # evaluate on query\n",
    "        hits = 0; total = 0\n",
    "        for qi in qry_idx:\n",
    "            Xq = tdiag['P'][qi].unsqueeze(0).to(DEVICE)\n",
    "            tq = int(tdiag['T'][qi].item())\n",
    "            _, final = tmp_model(Xq)\n",
    "            scores = torch.matmul(final, tmp_model.item_emb.weight.t())\n",
    "            topk = scores.topk(20, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            total += 1\n",
    "            if tq in topk: hits += 1\n",
    "        quick_recall = hits / total if total>0 else 0.0\n",
    "        print(f\"[Reptile] iter {it+1}/{META_ITERS} quick_recall@20={quick_recall:.4f}\")\n",
    "\n",
    "# After meta loop - save meta_state as meta initialization\n",
    "meta_model.load_state_dict(meta_state)\n",
    "torch.save({'meta_state': meta_state}, META_DIR / 'reptile_meta_state_top200k.pt')\n",
    "print(\"Saved meta init to:\", META_DIR / 'reptile_meta_state_top200k.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff57e4",
   "metadata": {},
   "source": [
    "### Cell 7 — Few-shot adaptation to MARS and final eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69190a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_22352\\710123239.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mp = torch.load(MARS_SHARD_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built hashed MARS pairs: 2380\n",
      "MARS splits: train 1904 val 238 test 238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_22352\\710123239.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta_ck = torch.load(META_DIR / 'reptile_meta_state_top200k.pt', map_location=DEVICE)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_22352\\710123239.py:78: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rank = int((topk == target).nonzero()[0]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot K=10 -> Recall@20=0.1303, MRR=0.0251\n",
      "Few-shot K=50 -> Recall@20=0.0924, MRR=0.0146\n",
      "Few-shot K=100 -> Recall@20=0.1261, MRR=0.0205\n",
      "Few-shot K=200 -> Recall@20=0.1429, MRR=0.0209\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 - Adapt to MARS (few-shot) and evaluate\n",
    "# load MARS shard (built earlier), map using same hashing\n",
    "MARS_SHARD_FILE = DATA_DIR / 'mars_shards' / 'mars_shard_full.pt'\n",
    "if not MARS_SHARD_FILE.exists():\n",
    "    raise FileNotFoundError(\"Please ensure MARS shard exists (built in 07_transfer_to_mars).\")\n",
    "\n",
    "mp = torch.load(MARS_SHARD_FILE)\n",
    "P_all = mp['prefix']   # NOTE: these were constructed earlier with original item2id mapping; we re-hash tokens here\n",
    "# We need to rebuild a hashed MARS shard to be consistent. If your MARS_shard already uses text tokens, rebuild; otherwise convert.\n",
    "# For simplicity assume mars_prefix_target.parquet exists and we rebuild hashed prefixes here\n",
    "MARS_PAIRS = DATA_DIR / 'mars_prefix_target.parquet'\n",
    "if not MARS_PAIRS.exists():\n",
    "    raise FileNotFoundError(\"Please create mars_prefix_target.parquet first.\")\n",
    "df_mars_pairs = pd.read_parquet(MARS_PAIRS)\n",
    "# build hashed MARS tensors\n",
    "P_list = []\n",
    "T_list = []\n",
    "for _, r in df_mars_pairs.iterrows():\n",
    "    pref = r['prefix'] if isinstance(r['prefix'], str) else ''\n",
    "    tokens = [t for t in str(pref).split() if t!='']\n",
    "    ids = [token_to_hash_id(t) for t in tokens]\n",
    "    if len(ids) > MAX_PREFIX_LEN: ids = ids[-MAX_PREFIX_LEN:]\n",
    "    padded = [PAD_IDX]*(MAX_PREFIX_LEN - len(ids)) + ids\n",
    "    P_list.append(padded)\n",
    "    T_list.append(token_to_hash_id(r['target']))\n",
    "P_H = torch.LongTensor(P_list)\n",
    "T_H = torch.LongTensor(T_list)\n",
    "print(\"Built hashed MARS pairs:\", P_H.size(0))\n",
    "\n",
    "# Split train/val/test\n",
    "n = P_H.size(0)\n",
    "test_n = max(1, int(0.1*n))\n",
    "val_n = max(1, int(0.1*n))\n",
    "train_n = n - val_n - test_n\n",
    "train_P, train_T = P_H[:train_n].to(DEVICE), T_H[:train_n].to(DEVICE)\n",
    "val_P, val_T = P_H[train_n:train_n+val_n].to(DEVICE), T_H[train_n:train_n+val_n].to(DEVICE)\n",
    "test_P, test_T = P_H[train_n+val_n:].to(DEVICE), T_H[train_n+val_n:].to(DEVICE)\n",
    "print(\"MARS splits: train\", train_P.size(0), \"val\", val_P.size(0), \"test\", test_P.size(0))\n",
    "\n",
    "# Load meta init\n",
    "meta_ck = torch.load(META_DIR / 'reptile_meta_state_top200k.pt', map_location=DEVICE)\n",
    "meta_state = meta_ck['meta_state']\n",
    "adapt_model = SASRecSmall(vocab_size=META_VOCAB, embed_dim=64, max_len=MAX_PREFIX_LEN).to(DEVICE)\n",
    "adapt_model.load_state_dict(meta_state)\n",
    "\n",
    "# Few-shot fine-tune (support small K shots) — try different K_shots\n",
    "def adapt_and_eval(K_shots=50, adapt_steps=10, lr=1e-4):\n",
    "    # sample K_shots from train\n",
    "    idxs = np.random.choice(train_P.size(0), size=min(K_shots, train_P.size(0)), replace=False)\n",
    "    Xs = train_P[idxs]\n",
    "    ys = train_T[idxs]\n",
    "    model = SASRecSmall(vocab_size=META_VOCAB, embed_dim=64, max_len=MAX_PREFIX_LEN).to(DEVICE)\n",
    "    model.load_state_dict(meta_state)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    for s in range(adapt_steps):\n",
    "        model.train()\n",
    "        _, final = model(Xs)\n",
    "        pos_scores = (final * model.item_emb.weight[ys]).sum(dim=1)\n",
    "        neg_idx = torch.randint(0, model.item_emb.weight.size(0), (Xs.size(0), 32), device=DEVICE)\n",
    "        neg_w = model.item_emb.weight[neg_idx]\n",
    "        neg_scores = (neg_w * final.unsqueeze(1)).sum(dim=2)\n",
    "        logits = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)\n",
    "        loss = F.cross_entropy(logits, torch.zeros(Xs.size(0), dtype=torch.long, device=DEVICE))\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    # evaluate on test\n",
    "    model.eval()\n",
    "    hits = 0; rr_sum = 0.0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(test_P.size(0)):\n",
    "            Xq = test_P[i].unsqueeze(0)\n",
    "            target = int(test_T[i].item())\n",
    "            _, final = model(Xq)\n",
    "            scores = torch.matmul(final, model.item_emb.weight.t())\n",
    "            topk = scores.topk(20, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            total += 1\n",
    "            if target in topk:\n",
    "                hits += 1\n",
    "                rank = int((topk == target).nonzero()[0]) + 1\n",
    "                rr_sum += 1.0 / rank\n",
    "    recall = hits/total if total>0 else 0.0\n",
    "    mrr = rr_sum/total if total>0 else 0.0\n",
    "    return recall, mrr\n",
    "\n",
    "for K_shots in [10, 50, 100, 200]:\n",
    "    r, m = adapt_and_eval(K_shots=K_shots, adapt_steps=10, lr=1e-4)\n",
    "    print(f\"Few-shot K={K_shots} -> Recall@20={r:.4f}, MRR={m:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b5eca",
   "metadata": {},
   "source": [
    "### Cell 8 — Save & short tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "021cdcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved meta info.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - Save and tips\n",
    "torch.save({'meta_state': meta_state, 'K': K, 'pad': PAD_IDX}, META_DIR / 'meta_info_top200k.pt')\n",
    "print(\"Saved meta info.\")\n",
    "\n",
    "# Quick tips:\n",
    "# - If many tasks still have very low frac_nonzero_gt0 (<0.05) consider increasing K or using per-file remapping.\n",
    "# - Reptile hyperparams: TASK_BATCH, INNER_STEPS, META_STEP - tune depending on compute.\n",
    "# - Consider adapter layers (09_adapters) combined with Reptile: adapt fewer params and get robust few-shot.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
