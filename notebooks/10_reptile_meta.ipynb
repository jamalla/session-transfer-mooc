{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea349d8",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Reptile meta-learning starter for session-based recommendation. Uses tasks built from pretraining datasets (Yoochoose / Amazon categories). Produces meta-model that can be adapted quickly to MARS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab62ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\n"
     ]
    }
   ],
   "source": [
    "# Quick (unsafe) workaround to avoid the libiomp5md.dll crash.\n",
    "# Use this only to continue working in the notebook quickly.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "print(\"Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4c89f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found candidate files: 491\n",
      "Scanned 491 files; unique item tokens found: 3754480\n",
      "Saved meta_item2id.json with size 3754481\n",
      "Total tasks built: 491\n",
      "task[0] name=amazon_prefix_target_part0000::amazon_books_2023 pairs=200000 example_prefix_nonzero_len=1\n",
      "task[1] name=amazon_prefix_target_part0001::amazon_books_2023 pairs=200000 example_prefix_nonzero_len=7\n",
      "task[2] name=amazon_prefix_target_part0002::amazon_books_2023 pairs=12544 example_prefix_nonzero_len=1\n",
      "task[3] name=amazon_prefix_target_part0003::amazon_books_2023 pairs=200000 example_prefix_nonzero_len=1\n",
      "task[4] name=amazon_prefix_target_part0004::amazon_books_2023 pairs=200000 example_prefix_nonzero_len=19\n",
      "task[5] name=amazon_prefix_target_part0005::amazon_books_2023 pairs=10989 example_prefix_nonzero_len=1\n",
      "task[6] name=amazon_prefix_target_part0006::amazon_books_2023 pairs=200000 example_prefix_nonzero_len=1\n",
      "task[7] name=amazon_prefix_target_part0007::amazon_books_2023 pairs=200000 example_prefix_nonzero_len=8\n",
      "task[8] name=amazon_prefix_target_part0008::amazon_books_2023 pairs=12115 example_prefix_nonzero_len=8\n",
      "task[9] name=amazon_prefix_target_part0009::amazon_books_2023 pairs=200000 example_prefix_nonzero_len=1\n",
      "Saved tasks summary: ..\\data\\processed\\meta_vocab\\meta_tasks_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Robust task builder: create unified meta_item2id, map prefixes+targets to ints, then build tasks.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "ROOT = Path('..')\n",
    "DATA_DIR = ROOT/'data'/'processed'\n",
    "CKPT_DIR = ROOT/'models'\n",
    "META_VOCAB_DIR = DATA_DIR/'meta_vocab'\n",
    "META_VOCAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# find candidate prefix/pair files\n",
    "candidates = list(DATA_DIR.glob('**/*prefix*part*.parquet')) + list(DATA_DIR.glob('**/*prefix_target*.parquet')) + list(DATA_DIR.glob('**/*prefix_target*.parquet'))\n",
    "print(\"Found candidate files:\", len(candidates))\n",
    "\n",
    "# 1) Two-pass: collect all unique item ids (strings) from prefix and target columns\n",
    "unique_items = set()\n",
    "n_files = 0\n",
    "for p in candidates:\n",
    "    try:\n",
    "        df = pd.read_parquet(p, columns=['prefix','target'] if 'prefix' in pd.read_parquet(p, nrows=1).columns else df.columns)\n",
    "    except Exception as e:\n",
    "        # try reading full file if subset failed\n",
    "        try:\n",
    "            df = pd.read_parquet(p)\n",
    "        except Exception as e2:\n",
    "            print(\"Skipping unreadable file:\", p.name)\n",
    "            continue\n",
    "    n_files += 1\n",
    "    # iterate rows (vectorize when possible)\n",
    "    if 'prefix' in df.columns:\n",
    "        # prefixes may be strings like \"12 34 56\" or lists; normalize to str tokens\n",
    "        prefs = df['prefix'].dropna().astype(str)\n",
    "        for s in prefs:\n",
    "            if not s: continue\n",
    "            # split on whitespace to get tokens (works for \"a b c\" or \"['a','b']\" crude)\n",
    "            toks = s.strip().split()\n",
    "            for t in toks:\n",
    "                unique_items.add(t)\n",
    "    # targets\n",
    "    if 'target' in df.columns:\n",
    "        for t in df['target'].dropna().astype(str):\n",
    "            unique_items.add(t)\n",
    "print(f\"Scanned {n_files} files; unique item tokens found: {len(unique_items)}\")\n",
    "\n",
    "# build mapping\n",
    "meta_item2id = {}\n",
    "for i, item in enumerate(sorted(unique_items)):\n",
    "    meta_item2id[item] = i+1   # reserve 0 for padding/OOV\n",
    "meta_item2id['<OOV>'] = 0\n",
    "\n",
    "# save mapping\n",
    "with open(META_VOCAB_DIR/'meta_item2id.json','w') as fh:\n",
    "    json.dump(meta_item2id, fh)\n",
    "print(\"Saved meta_item2id.json with size\", len(meta_item2id))\n",
    "\n",
    "# 2) Second pass: build tasks using mapped integer ids\n",
    "def str_prefix_to_id_list(pref_str, mapping, max_len=20):\n",
    "    if not isinstance(pref_str, str): return []\n",
    "    toks = pref_str.strip().split()\n",
    "    ids = [ mapping.get(t, 0) for t in toks ]  # unknown -> 0\n",
    "    if len(ids) > max_len: ids = ids[-max_len:]\n",
    "    return ids\n",
    "\n",
    "tasks = []\n",
    "MAX_TASKS = 500\n",
    "MIN_PAIRS = 50\n",
    "max_per_file_tasks = 100\n",
    "\n",
    "for p in candidates:\n",
    "    try:\n",
    "        df = pd.read_parquet(p)\n",
    "    except Exception:\n",
    "        continue\n",
    "    # ensure prefix and target columns exist\n",
    "    if 'prefix' not in df.columns or 'target' not in df.columns:\n",
    "        # try to infer columns with similar names\n",
    "        cols = df.columns\n",
    "        if any('prefix' in c for c in cols) and any('target' in c for c in cols):\n",
    "            # map columns\n",
    "            pref_col = [c for c in cols if 'prefix' in c][0]\n",
    "            targ_col = [c for c in cols if 'target' in c][0]\n",
    "            df = df.rename(columns={pref_col: 'prefix', targ_col: 'target'})\n",
    "        else:\n",
    "            # skip\n",
    "            continue\n",
    "\n",
    "    # optional grouping column\n",
    "    group_col = None\n",
    "    for col in ['category','dataset','label','course_id']:\n",
    "        if col in df.columns:\n",
    "            group_col = col\n",
    "            break\n",
    "\n",
    "    if group_col:\n",
    "        groups = df.groupby(group_col)\n",
    "        cnt = 0\n",
    "        for name, g in groups:\n",
    "            # map and build (prefix, target) lists\n",
    "            P=[]; T=[]\n",
    "            for _, r in g.iterrows():\n",
    "                pref = r['prefix'] if pd.notna(r['prefix']) else ''\n",
    "                pref_ids = str_prefix_to_id_list(str(pref), meta_item2id)\n",
    "                if len(pref_ids) == 0: continue\n",
    "                padded = [0]*(20-len(pref_ids)) + pref_ids\n",
    "                target_token = str(r['target'])\n",
    "                tgt_id = meta_item2id.get(target_token, 0)\n",
    "                P.append(padded); T.append(tgt_id)\n",
    "            if len(P) >= MIN_PAIRS:\n",
    "                tasks.append({'P': torch.LongTensor(P), 'T': torch.LongTensor(T), 'name': f\"{p.stem}::{name}\"})\n",
    "                cnt += 1\n",
    "            if cnt >= max_per_file_tasks: break\n",
    "    else:\n",
    "        # chunking fallback\n",
    "        rows = df.to_dict(orient='records')\n",
    "        # build prefix-target rows list first\n",
    "        seqs = []\n",
    "        for r in rows:\n",
    "            pref = r.get('prefix','')\n",
    "            if pref is None: pref = ''\n",
    "            pref_ids = str_prefix_to_id_list(str(pref), meta_item2id)\n",
    "            if len(pref_ids) == 0: continue\n",
    "            padded = [0]*(20-len(pref_ids)) + pref_ids\n",
    "            tgt_id = meta_item2id.get(str(r.get('target', '')), 0)\n",
    "            seqs.append((padded, tgt_id))\n",
    "        # chunk into tasks\n",
    "        for start in range(0, len(seqs), MIN_PAIRS):\n",
    "            sub = seqs[start:start+MIN_PAIRS*5]\n",
    "            if len(sub) < MIN_PAIRS: break\n",
    "            P = [x[0] for x in sub]; T = [x[1] for x in sub]\n",
    "            tasks.append({'P': torch.LongTensor(P), 'T': torch.LongTensor(T), 'name': f\"{p.stem}::chunk_{start}\"})\n",
    "            if len(tasks) >= MAX_TASKS:\n",
    "                break\n",
    "    if len(tasks) >= MAX_TASKS:\n",
    "        break\n",
    "\n",
    "print(\"Total tasks built:\", len(tasks))\n",
    "\n",
    "# Diagnostics: print first 10 tasks\n",
    "for i, t in enumerate(tasks[:10]):\n",
    "    nonzero = (t['P'][0] != 0).sum().item()\n",
    "    print(f\"task[{i}] name={t['name']} pairs={t['P'].size(0)} example_prefix_nonzero_len={nonzero}\")\n",
    "\n",
    "# Save a small summary file\n",
    "summary = [{'name': t['name'], 'pairs': int(t['P'].size(0))} for t in tasks]\n",
    "pd.DataFrame(summary).to_csv(META_VOCAB_DIR/'meta_tasks_summary.csv', index=False)\n",
    "print(\"Saved tasks summary:\", META_VOCAB_DIR/'meta_tasks_summary.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
