{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38bc21c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing amazon_books_2023 ---\n",
      "Saved sessionized interactions to: ..\\data\\processed\\amazon_books_2023_sessions.parquet\n",
      "Saved session summary to: ..\\data\\processed\\amazon_books_2023_session_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prefix-target: 100%|██████████| 2788253/2788253 [07:49<00:00, 5934.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prefix->target pairs to: ..\\data\\processed\\amazon_books_2023_prefix_target.parquet\n",
      "Session summary stats (before filtering):\n",
      "count    2.162191e+07\n",
      "mean     1.252362e+00\n",
      "std      1.170748e+00\n",
      "min      1.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      1.000000e+00\n",
      "75%      1.000000e+00\n",
      "max      2.150000e+02\n",
      "Name: session_length, dtype: float64\n",
      "After filtering sessions: 2788255\n",
      "Prefix-target pairs generated: 5441026\n",
      "\n",
      "--- Processing mars ---\n",
      "Saved sessionized interactions to: ..\\data\\processed\\mars_sessions.parquet\n",
      "Saved session summary to: ..\\data\\processed\\mars_session_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prefix-target: 100%|██████████| 549/549 [00:00<00:00, 5083.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prefix->target pairs to: ..\\data\\processed\\mars_prefix_target.parquet\n",
      "Session summary stats (before filtering):\n",
      "count    1275.000000\n",
      "mean        2.869804\n",
      "std         4.694463\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         3.000000\n",
      "max        50.000000\n",
      "Name: session_length, dtype: float64\n",
      "After filtering sessions: 549\n",
      "Prefix-target pairs generated: 2384\n",
      "\n",
      "=== amazon_books_2023 ===\n",
      "Interactions after sessionization: 8244397\n",
      "Unique sessions: 21621909\n",
      "Prefix-target pairs: 5441026\n",
      "Sample session summary head:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     session_id_real            dataset  \\\n",
      "0   AE22236AFRRSMQIKGG7TPTB75QEA__s1  amazon_books_2023   \n",
      "1  AE22236AFRRSMQIKGG7TPTB75QEA__s10  amazon_books_2023   \n",
      "2  AE22236AFRRSMQIKGG7TPTB75QEA__s11  amazon_books_2023   \n",
      "3  AE22236AFRRSMQIKGG7TPTB75QEA__s12  amazon_books_2023   \n",
      "4  AE22236AFRRSMQIKGG7TPTB75QEA__s13  amazon_books_2023   \n",
      "\n",
      "                        user_id  session_idx          start_time  \\\n",
      "0  AE22236AFRRSMQIKGG7TPTB75QEA            1 1999-07-30 03:53:39   \n",
      "1  AE22236AFRRSMQIKGG7TPTB75QEA           10 2012-01-16 02:07:51   \n",
      "2  AE22236AFRRSMQIKGG7TPTB75QEA           11 2012-01-21 16:09:22   \n",
      "3  AE22236AFRRSMQIKGG7TPTB75QEA           12 2012-08-04 01:19:59   \n",
      "4  AE22236AFRRSMQIKGG7TPTB75QEA           13 2012-12-09 02:35:09   \n",
      "\n",
      "             end_time  session_length  \n",
      "0 1999-07-30 03:53:39               1  \n",
      "1 2012-01-16 02:07:51               1  \n",
      "2 2012-01-21 16:09:22               1  \n",
      "3 2012-08-04 01:19:59               1  \n",
      "4 2012-12-09 02:35:09               1  \n",
      "Sample prefix-target head:\n",
      "             dataset                       user_id  \\\n",
      "0  amazon_books_2023  AE22236AFRRSMQIKGG7TPTB75QEA   \n",
      "1  amazon_books_2023  AE2224FH27S7UZNNXCGD5SZ5OM4Q   \n",
      "2  amazon_books_2023  AE2224IMTLGXRXJITJJOW5RSMW5Q   \n",
      "3  amazon_books_2023  AE2224IMTLGXRXJITJJOW5RSMW5Q   \n",
      "4  amazon_books_2023  AE2227ICQPWG234XWVOKP6KPFNGQ   \n",
      "\n",
      "                     session_id_real  prefix_len      prefix      target  \n",
      "0  AE22236AFRRSMQIKGG7TPTB75QEA__s14           1  B007GCYB0U  B008P3JNHU  \n",
      "1   AE2224FH27S7UZNNXCGD5SZ5OM4Q__s1           1  0061147966  0062068253  \n",
      "2   AE2224IMTLGXRXJITJJOW5RSMW5Q__s1           1  1602397724  006170301X  \n",
      "3   AE2224IMTLGXRXJITJJOW5RSMW5Q__s2           1  1402775075  0316227706  \n",
      "4   AE2227ICQPWG234XWVOKP6KPFNGQ__s3           1  173733710X  098633250X  \n",
      "\n",
      "=== mars ===\n",
      "Interactions after sessionization: 2933\n",
      "Unique sessions: 1275\n",
      "Prefix-target pairs: 2384\n",
      "Sample session summary head:\n",
      "  session_id_real dataset user_id  session_idx          start_time  \\\n",
      "0      104074__s1    mars  104074            1 2018-12-19 15:39:36   \n",
      "1      104074__s2    mars  104074            2 2019-07-22 15:27:08   \n",
      "2      104074__s3    mars  104074            3 2021-05-19 11:09:51   \n",
      "3      104074__s4    mars  104074            4 2021-05-21 12:37:56   \n",
      "4      104074__s5    mars  104074            5 2021-05-24 15:38:08   \n",
      "\n",
      "             end_time  session_length  \n",
      "0 2018-12-19 15:39:36               1  \n",
      "1 2019-07-22 17:04:53              28  \n",
      "2 2021-05-19 11:23:41               4  \n",
      "3 2021-05-21 12:37:56               1  \n",
      "4 2021-05-24 16:49:28               7  \n",
      "Sample prefix-target head:\n",
      "  dataset user_id session_id_real  prefix_len                         prefix  \\\n",
      "0    mars  104074      104074__s2           1                          52609   \n",
      "1    mars  104074      104074__s2           2                    52609 52616   \n",
      "2    mars  104074      104074__s2           3              52609 52616 52615   \n",
      "3    mars  104074      104074__s2           4        52609 52616 52615 52610   \n",
      "4    mars  104074      104074__s2           5  52609 52616 52615 52610 52614   \n",
      "\n",
      "  target  \n",
      "0  52616  \n",
      "1  52615  \n",
      "2  52610  \n",
      "3  52614  \n",
      "4  52618  \n"
     ]
    }
   ],
   "source": [
    "# notebooks/04_sessionize_and_prefix_target.ipynb\n",
    "# --- Sessionization + Prefix->Target generation (1 hour gap) ---\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Parameters\n",
    "TIME_GAP_SECONDS = 60 * 60   # 1 hour\n",
    "MIN_SESSION_LEN = 2\n",
    "MAX_SESSION_LEN = 200        # truncate or drop very long sessions\n",
    "MAX_PREFIX_LEN = 50          # optional cap for prefix length when saving\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "\n",
    "def sessionize_df(df, user_col=\"user_id\", time_col=\"timestamp\", gap_seconds=TIME_GAP_SECONDS):\n",
    "    \"\"\"\n",
    "    Input: df with columns [dataset, user_id, session_id (placeholder), item_id, timestamp, interaction_type]\n",
    "    Returns: df with new column 'session_id_real' and session-level summary DF\n",
    "    \"\"\"\n",
    "    # Ensure timestamp is datetime\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    df = df.sort_values([user_col, time_col])\n",
    "\n",
    "    # compute time diff per user\n",
    "    df['time_diff'] = df.groupby(user_col)[time_col].diff().dt.total_seconds()\n",
    "    # start new session when time_diff is NaN (first event) or > gap\n",
    "    df['new_session'] = (df['time_diff'].isna()) | (df['time_diff'] > gap_seconds)\n",
    "    # cumulative sum per user to get session index\n",
    "    df['session_idx'] = df.groupby(user_col)['new_session'].cumsum().astype(int)\n",
    "\n",
    "    # build a real session id: f\"{user_id}__s{session_idx}\"\n",
    "    df['session_id_real'] = df[user_col].astype(str) + \"__s\" + df['session_idx'].astype(str)\n",
    "\n",
    "    # session summary\n",
    "    session_summary = (\n",
    "        df.groupby('session_id_real').agg(\n",
    "            dataset=('dataset', 'first'),\n",
    "            user_id=(user_col, 'first'),\n",
    "            session_idx=('session_idx', 'first'),\n",
    "            start_time=(time_col, 'min'),\n",
    "            end_time=(time_col, 'max'),\n",
    "            session_length=('item_id', 'size')\n",
    "        ).reset_index()\n",
    "    )\n",
    "\n",
    "    # filter by length\n",
    "    valid_sessions = session_summary[\n",
    "        (session_summary['session_length'] >= MIN_SESSION_LEN) &\n",
    "        (session_summary['session_length'] <= MAX_SESSION_LEN)\n",
    "    ]['session_id_real']\n",
    "\n",
    "    df = df[df['session_id_real'].isin(valid_sessions)].copy()\n",
    "    df = df.drop(columns=['time_diff', 'new_session', 'session_idx'])\n",
    "\n",
    "    return df, session_summary\n",
    "\n",
    "def generate_prefix_target(df, item_col='item_id', session_col='session_id_real', max_prefix_len=MAX_PREFIX_LEN):\n",
    "    \"\"\"\n",
    "    For each session (ordered by timestamp), generate prefix->target pairs:\n",
    "      for seq [i1, i2, i3], generate:\n",
    "        ([i1], i2), ([i1,i2], i3)\n",
    "    Return dataframe with columns:\n",
    "      dataset, user_id, session_id_real, prefix (list or str), prefix_len, target\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    # ensure ordering\n",
    "    df = df.sort_values([session_col, 'timestamp'])\n",
    "    grouped = df.groupby(session_col)\n",
    "    for session_id, g in tqdm.tqdm(grouped, desc=\"Generating prefix-target\"):\n",
    "        items = g[item_col].astype(str).tolist()\n",
    "        if len(items) < 2:\n",
    "            continue\n",
    "        # optional: cap session items to last N\n",
    "        if len(items) > max_prefix_len + 1:\n",
    "            items = items[-(max_prefix_len+1):]  # keep last (max_prefix_len + 1) items\n",
    "        for t in range(1, len(items)):\n",
    "            prefix = items[:t]\n",
    "            target = items[t]\n",
    "            rows.append({\n",
    "                'dataset': g['dataset'].iloc[0],\n",
    "                'user_id': g['user_id'].iloc[0],\n",
    "                'session_id_real': session_id,\n",
    "                'prefix_len': len(prefix),\n",
    "                # store prefix as space-separated string to save space; you can parse back with .split()\n",
    "                'prefix': \" \".join(prefix),\n",
    "                'target': target\n",
    "            })\n",
    "    out = pd.DataFrame(rows)\n",
    "    return out\n",
    "\n",
    "# ------- Run for a dataset function ----------\n",
    "def process_dataset(dataset_name):\n",
    "    print(f\"\\n--- Processing {dataset_name} ---\")\n",
    "    in_path = DATA_DIR / f\"{dataset_name}_interactions.parquet\"\n",
    "    assert in_path.exists(), f\"{in_path} not found\"\n",
    "    df = pd.read_parquet(in_path)\n",
    "\n",
    "    # sessionize only if session_id currently equals user_id placeholder\n",
    "    # (For yoochoose we already have real session ids; but it's safe to run — we'll detect that)\n",
    "    # If there's already many distinct session ids that are not just equal to user_id, you can skip.\n",
    "    df_sessionized, session_summary = sessionize_df(df, user_col='user_id', time_col='timestamp')\n",
    "\n",
    "    # Save sessionized interactions\n",
    "    out_sessions_path = DATA_DIR / f\"{dataset_name}_sessions.parquet\"\n",
    "    df_sessionized.to_parquet(out_sessions_path, index=False)\n",
    "    print(f\"Saved sessionized interactions to: {out_sessions_path}\")\n",
    "\n",
    "    # Save session summary\n",
    "    out_summary = DATA_DIR / f\"{dataset_name}_session_summary.csv\"\n",
    "    session_summary.to_csv(out_summary, index=False)\n",
    "    print(f\"Saved session summary to: {out_summary}\")\n",
    "\n",
    "    # Generate prefix-target pairs\n",
    "    pairs = generate_prefix_target(df_sessionized, item_col='item_id', session_col='session_id_real')\n",
    "    out_pairs = DATA_DIR / f\"{dataset_name}_prefix_target.parquet\"\n",
    "    pairs.to_parquet(out_pairs, index=False)\n",
    "    print(f\"Saved prefix->target pairs to: {out_pairs}\")\n",
    "\n",
    "    # Print quick stats\n",
    "    print(\"Session summary stats (before filtering):\")\n",
    "    print(session_summary['session_length'].describe())\n",
    "    print(\"After filtering sessions:\", session_summary[session_summary['session_length'] >= MIN_SESSION_LEN].shape[0])\n",
    "    print(\"Prefix-target pairs generated:\", len(pairs))\n",
    "    return df_sessionized, session_summary, pairs\n",
    "\n",
    "# Datasets to process:\n",
    "datasets = [\"amazon_books_2023\", \"mars\"]  # yoochoose already ok\n",
    "results = {}\n",
    "for ds_name in datasets:\n",
    "    df_sess, sess_summary, pairs = process_dataset(ds_name)\n",
    "    results[ds_name] = (df_sess, sess_summary, pairs)\n",
    "\n",
    "# Quick sanity check prints\n",
    "for ds_name, (df_sess, summary, pairs) in results.items():\n",
    "    print(f\"\\n=== {ds_name} ===\")\n",
    "    print(\"Interactions after sessionization:\", len(df_sess))\n",
    "    print(\"Unique sessions:\", summary.shape[0])\n",
    "    print(\"Prefix-target pairs:\", len(pairs))\n",
    "    print(\"Sample session summary head:\")\n",
    "    print(summary.head())\n",
    "    print(\"Sample prefix-target head:\")\n",
    "    print(pairs.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
