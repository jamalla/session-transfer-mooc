{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c73d08b",
   "metadata": {},
   "source": [
    "### 05 — Sessionization & Prefix→Target Generation\n",
    "\n",
    "**Goal:**  \n",
    "- For **Amazon Books 2023**: build synthetic sessions using **sliding windows** (Option A) to convert sparse user timelines into many sequence training examples.\n",
    "- For **MARS**: sessionize by **1 hour gap** (as decided in Step 04).\n",
    "- For **YOOCHOOSE**: use existing session IDs (already processed).\n",
    "- Generate `prefix -> target` pairs for training (next-item prediction).\n",
    "- Save outputs and report diagnostics.\n",
    "\n",
    "Outputs saved to `data/processed/`:\n",
    "- `amazon_windows_sessions.parquet` (or chunked files)\n",
    "- `amazon_prefix_target.parquet` (or chunked)\n",
    "- `mars_sessions.parquet`\n",
    "- `mars_prefix_target.parquet`\n",
    "- summary CSVs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed9ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: imports & settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "OUT_DIR = DATA_DIR\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parameters (tune as needed)\n",
    "WINDOW_SIZE = 20    # number of items in a sliding window\n",
    "STRIDE = 10         # step between windows\n",
    "MIN_WINDOW_ITEMS = 2  # min items to keep a window (>=2 gives at least one pair)\n",
    "MAX_PREFIX_LEN = 50  # cap prefix length when generating prefix-target\n",
    "MIN_SESSION_LEN = 2\n",
    "CHUNK_WRITE = True  # if True, write parquet in chunk files to avoid excessive RAM use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27816fd6",
   "metadata": {},
   "source": [
    "#### A — Helper functions\n",
    "- `build_sliding_windows()` : builds sliding-window sessions for Amazon (memory-efficient).\n",
    "- `sessionize_by_gap()` : sessionize a dataset by time-gap (used for MARS).\n",
    "- `generate_prefix_target()` : generate prefix→target pairs from a sessions DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3f893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: helper function definitions\n",
    "def build_sliding_windows(df, user_col='user_id', item_col='item_id',\n",
    "                          time_col='timestamp', window_size=WINDOW_SIZE, stride=STRIDE,\n",
    "                          out_session_col='session_id_real'):\n",
    "    \"\"\"\n",
    "    Build sliding-window synthetic sessions from a user timeline DataFrame.\n",
    "    Returns a DataFrame with columns: dataset, user_id, session_id_real, item_id, timestamp\n",
    "    This version streams per-user and yields rows to reduce peak memory usage.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    # Ensure sorted\n",
    "    df = df.sort_values([user_col, time_col])\n",
    "    users = df[user_col].unique()\n",
    "    total_users = len(users)\n",
    "    print(f\"Users to process: {total_users:,}\")\n",
    "    \n",
    "    # iterate per user\n",
    "    for user in tqdm(users, desc=\"Building windows per user\"):\n",
    "        user_slice = df[df[user_col] == user]\n",
    "        items = user_slice[item_col].astype(str).tolist()\n",
    "        times = pd.to_datetime(user_slice[time_col]).tolist()\n",
    "        n = len(items)\n",
    "        if n < MIN_WINDOW_ITEMS:\n",
    "            continue\n",
    "        start = 0\n",
    "        win_idx = 0\n",
    "        while start < n:\n",
    "            end = min(start + window_size, n)\n",
    "            window_items = items[start:end]\n",
    "            window_times = times[start:end]\n",
    "            if len(window_items) >= MIN_WINDOW_ITEMS:\n",
    "                sess_id = f\"{user}__w{win_idx}_{start}\"\n",
    "                for it, ts in zip(window_items, window_times):\n",
    "                    rows.append({\n",
    "                        \"dataset\": \"amazon_books_2023\",\n",
    "                        \"user_id\": str(user),\n",
    "                        out_session_col: sess_id,\n",
    "                        \"item_id\": str(it),\n",
    "                        \"timestamp\": pd.to_datetime(ts)\n",
    "                    })\n",
    "                win_idx += 1\n",
    "            start += stride\n",
    "            # if rows accumulate too large, yield in batches (handled by caller)\n",
    "            if len(rows) >= 500_000:\n",
    "                yield pd.DataFrame(rows)\n",
    "                rows = []\n",
    "    # final flush\n",
    "    if rows:\n",
    "        yield pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def sessionize_by_gap(df, gap_seconds=3600, user_col='user_id', time_col='timestamp',\n",
    "                      out_session_col='session_id_real'):\n",
    "    \"\"\"\n",
    "    Sessionize by gap_seconds per user. Returns sessionized df and session summary.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([user_col, time_col]).copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    df['prev_ts'] = df.groupby(user_col)[time_col].shift(1)\n",
    "    df['gap_s'] = (df[time_col] - df['prev_ts']).dt.total_seconds()\n",
    "    df['new_session'] = (df['gap_s'].isna()) | (df['gap_s'] > gap_seconds)\n",
    "    df['sess_idx'] = df.groupby(user_col)['new_session'].cumsum().astype(int)\n",
    "    df[out_session_col] = df[user_col].astype(str) + \"__s\" + df['sess_idx'].astype(str)\n",
    "    # summary\n",
    "    session_summary = df.groupby(out_session_col).agg(\n",
    "        dataset=('dataset','first'),\n",
    "        user_id=(user_col,'first'),\n",
    "        start_time=(time_col,'min'),\n",
    "        end_time=(time_col,'max'),\n",
    "        session_length=('item_id','size')\n",
    "    ).reset_index()\n",
    "    # filter short and very long sessions\n",
    "    valid_sessions = session_summary[\n",
    "        (session_summary['session_length'] >= MIN_SESSION_LEN)\n",
    "    ][out_session_col].tolist()\n",
    "    df = df[df[out_session_col].isin(valid_sessions)].copy()\n",
    "    df = df.drop(columns=['prev_ts','gap_s','new_session','sess_idx'], errors='ignore')\n",
    "    return df, session_summary\n",
    "\n",
    "def generate_prefix_target(df, session_col='session_id_real', item_col='item_id', max_prefix_len=MAX_PREFIX_LEN):\n",
    "    \"\"\"\n",
    "    From a sessionized df ordered by timestamp, generate prefix->target pairs.\n",
    "    Returns DataFrame with columns: dataset, user_id, session_id_real, prefix_len, prefix (space-separated), target\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    df = df.sort_values([session_col, 'timestamp'])\n",
    "    grouped = df.groupby(session_col)\n",
    "    for session_id, g in tqdm(grouped, desc=\"Generating prefix-target\"):\n",
    "        items = g[item_col].astype(str).tolist()\n",
    "        if len(items) < 2: continue\n",
    "        # optionally truncate to last (max_prefix_len + 1) items to limit explosion\n",
    "        if len(items) > max_prefix_len + 1:\n",
    "            items = items[-(max_prefix_len + 1):]\n",
    "        for t in range(1, len(items)):\n",
    "            prefix = items[:t]\n",
    "            target = items[t]\n",
    "            rows.append({\n",
    "                'dataset': g['dataset'].iloc[0],\n",
    "                'user_id': g['user_id'].iloc[0],\n",
    "                session_col: session_id,\n",
    "                'prefix_len': len(prefix),\n",
    "                'prefix': \" \".join(prefix),\n",
    "                'target': str(target)\n",
    "            })\n",
    "        # flush periodically if very large\n",
    "        if len(rows) >= 500_000:\n",
    "            yield pd.DataFrame(rows)\n",
    "            rows = []\n",
    "    if rows:\n",
    "        yield pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e445f9dc",
   "metadata": {},
   "source": [
    "#### B — Process Amazon using sliding windows (streaming)\n",
    "This cell will build windowed sessions and save them to disk in chunked parquet files to avoid using too much RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567c992b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Amazon interactions: 27078467\n",
      "Users to process: 9,614,012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building windows per user:   0%|          | 16/9614012 [00:22<3671:01:18,  1.37s/it]"
     ]
    }
   ],
   "source": [
    "# Cell: process Amazon in streaming mode and write chunked parquet files\n",
    "amazon_path = DATA_DIR / \"amazon_books_2023_interactions.parquet\"\n",
    "assert amazon_path.exists(), \"Amazon parquet not found; check path.\"\n",
    "\n",
    "amazon_df = pd.read_parquet(amazon_path)  # full file; ensure enough RAM or use query to limit users\n",
    "print(\"Loaded Amazon interactions:\", len(amazon_df))\n",
    "\n",
    "# Optional: focus on most active users to reduce processing time (uncomment to use)\n",
    "# active_users = amazon_df['user_id'].value_counts().nlargest(500000).index.tolist()\n",
    "# amazon_df = amazon_df[amazon_df['user_id'].isin(active_users)].copy()\n",
    "\n",
    "out_base = OUT_DIR / \"amazon_windows_sessions\"\n",
    "# remove old files if rerunning\n",
    "for p in out_base.parent.glob(\"amazon_windows_sessions*.parquet\"):\n",
    "    try:\n",
    "        p.unlink()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# stream generator yields DataFrame chunks\n",
    "gen = build_sliding_windows(amazon_df, window_size=WINDOW_SIZE, stride=STRIDE)\n",
    "chunk_i = 0\n",
    "total_rows = 0\n",
    "for chunk in gen:\n",
    "    if len(chunk)==0:\n",
    "        continue\n",
    "    path = out_base.with_name(f\"amazon_windows_sessions_part{chunk_i:03d}.parquet\")\n",
    "    chunk.to_parquet(path, index=False)\n",
    "    chunk_i += 1\n",
    "    total_rows += len(chunk)\n",
    "    print(f\"Wrote {path} rows={len(chunk):,}; total so far={total_rows:,}\")\n",
    "\n",
    "print(\"Done. Total rows written:\", total_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42429876",
   "metadata": {},
   "source": [
    "#### C — Combine chunked windows (optional) and inspect session stats\n",
    "If you created chunked parquet parts you can inspect their stats and optionally concatenate into one file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156149b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: read chunk files, compute session summary stats and optionally combine into single parquet (careful on RAM)\n",
    "parts = sorted(OUT_DIR.glob(\"amazon_windows_sessions_part*.parquet\"))\n",
    "print(\"Found parts:\", len(parts))\n",
    "if len(parts)==0:\n",
    "    raise FileNotFoundError(\"No amazon windows parts found. Did previous cell run?\")\n",
    "\n",
    "# compute per-part session summary (stream)\n",
    "sess_rows = []\n",
    "total_events = 0\n",
    "for p in parts:\n",
    "    df_part = pd.read_parquet(p)\n",
    "    total_events += len(df_part)\n",
    "    summary = df_part.groupby('session_id_real').size().reset_index(name='length')\n",
    "    sess_rows.append(summary)\n",
    "    print(p, \"events:\", len(df_part), \"sessions:\", len(summary))\n",
    "\n",
    "# concatenate summaries to get global stats (session_ids unique by construction)\n",
    "sess_all = pd.concat(sess_rows, ignore_index=True)\n",
    "print(\"Approx sessions (may include duplicates across parts if same session spans parts):\", len(sess_all))\n",
    "print(\"Session length describe:\")\n",
    "print(sess_all['length'].describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99]))\n",
    "# optionally write a single combined sessions file (if RAM allows)\n",
    "combined_path = OUT_DIR / \"amazon_windows_sessions_combined.parquet\"\n",
    "# caution: concatenating all parts may require RAM; skip if huge\n",
    "do_combine = False\n",
    "if do_combine:\n",
    "    frames = [pd.read_parquet(p) for p in parts]\n",
    "    df_combined = pd.concat(frames, ignore_index=True)\n",
    "    df_combined.to_parquet(combined_path, index=False)\n",
    "    print(\"Wrote combined sessions to:\", combined_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff477ff0",
   "metadata": {},
   "source": [
    "#### D — Generate prefix→target pairs for Amazon (stream chunked parts)\n",
    "Read each window-session chunk and generate prefix-target pairs streaming to chunked parquet files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: generate prefix-target for amazon parts and write chunked parquet\n",
    "out_pairs_base = OUT_DIR / \"amazon_prefix_target_part\"\n",
    "pair_i = 0\n",
    "total_pairs = 0\n",
    "\n",
    "for p in parts:\n",
    "    df_part = pd.read_parquet(p)\n",
    "    # generate prefix-target in streaming yields\n",
    "    for chunk_pairs in generate_prefix_target(df_part, session_col='session_id_real', item_col='item_id', max_prefix_len=MAX_PREFIX_LEN):\n",
    "        path = out_pairs_base.with_name(f\"amazon_prefix_target_part{pair_i:03d}.parquet\")\n",
    "        chunk_pairs.to_parquet(path, index=False)\n",
    "        pair_i += 1\n",
    "        total_pairs += len(chunk_pairs)\n",
    "        print(f\"Wrote {path} (#pairs={len(chunk_pairs):,}); total pairs so far={total_pairs:,}\")\n",
    "\n",
    "print(\"Done generating amazon prefix-target pairs. Total pairs:\", total_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7276da00",
   "metadata": {},
   "source": [
    "#### E — Process MARS using 1-hour gap, then generate prefix-targets\n",
    "This uses the sessionization-by-gap function and then generate prefix-target pairs (streamed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: sessionize MARS by 1 hour gap\n",
    "mars_path = DATA_DIR / \"mars_interactions.parquet\"\n",
    "assert mars_path.exists(), \"MARS parquet not found; check path\"\n",
    "\n",
    "mars_df = pd.read_parquet(mars_path)\n",
    "print(\"Loaded MARS rows:\", len(mars_df))\n",
    "\n",
    "mars_session_gap = 60*60  # 1 hour\n",
    "mars_sess_df, mars_summary = sessionize_by_gap(mars_df, gap_seconds=mars_session_gap, user_col='user_id', time_col='timestamp', out_session_col='session_id_real')\n",
    "\n",
    "# Save sessionized mars\n",
    "mars_sessions_path = OUT_DIR / \"mars_sessions.parquet\"\n",
    "mars_sess_df.to_parquet(mars_sessions_path, index=False)\n",
    "mars_summary_path = OUT_DIR / \"mars_session_summary.csv\"\n",
    "mars_summary.to_csv(mars_summary_path, index=False)\n",
    "print(\"Saved mars sessions and summary:\", mars_sessions_path, mars_summary_path)\n",
    "print(\"MARS sessions total:\", mars_summary.shape[0])\n",
    "print(mars_summary['session_length'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: generate prefix-target for MARS and save (small, in-memory ok)\n",
    "mars_pairs = []\n",
    "for chunk_pairs in generate_prefix_target(mars_sess_df, session_col='session_id_real', item_col='item_id', max_prefix_len=MAX_PREFIX_LEN):\n",
    "    mars_pairs.append(chunk_pairs)\n",
    "if len(mars_pairs):\n",
    "    mars_pairs_df = pd.concat(mars_pairs, ignore_index=True)\n",
    "else:\n",
    "    mars_pairs_df = pd.DataFrame(columns=['dataset','user_id','session_id_real','prefix_len','prefix','target'])\n",
    "\n",
    "mars_pairs_path = OUT_DIR / \"mars_prefix_target.parquet\"\n",
    "mars_pairs_df.to_parquet(mars_pairs_path, index=False)\n",
    "print(\"Saved MARS prefix-target pairs:\", mars_pairs_path)\n",
    "print(\"MARS pairs:\", len(mars_pairs_df))\n",
    "mars_pairs_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067f2c4e",
   "metadata": {},
   "source": [
    "#### F — Quick diagnostics to print & paste back for review\n",
    "\n",
    "Please paste the outputs (numbers & small table) here after you run:\n",
    "- # amazon parts written, total events written\n",
    "- approximate # amazon sessions (from summaries) and session length stats\n",
    "- total amazon prefix-target pairs generated\n",
    "- mars sessions count, mars session length describe\n",
    "- mars prefix-target pairs count and pairs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745995f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: print quick diagnostics summary (amazon and mars)\n",
    "parts = sorted(OUT_DIR.glob(\"amazon_windows_sessions_part*.parquet\"))\n",
    "amazon_pairs_parts = sorted(OUT_DIR.glob(\"amazon_prefix_target_part*.parquet\"))\n",
    "\n",
    "print(\"Amazon window parts:\", len(parts))\n",
    "total_amz_events = sum(pd.read_parquet(p).shape[0] for p in parts)\n",
    "print(\"Total amazon window events (sum of parts):\", total_amz_events)\n",
    "\n",
    "# session length approx from earlier step (sess_all may be available)\n",
    "try:\n",
    "    print(\"Approx amazon sessions (summary parts concat):\", len(sess_all))\n",
    "    print(sess_all['length'].describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99]))\n",
    "except NameError:\n",
    "    print(\"Session summary not available in memory; check part outputs.\")\n",
    "\n",
    "# amazon pairs\n",
    "total_amz_pairs = 0\n",
    "for p in amazon_pairs_parts:\n",
    "    dfp = pd.read_parquet(p)\n",
    "    total_amz_pairs += len(dfp)\n",
    "print(\"Amazon prefix-target parts:\", len(amazon_pairs_parts), \"total pairs:\", total_amz_pairs)\n",
    "\n",
    "# MARS stats\n",
    "print(\"\\nMARS sessions:\", mars_summary.shape[0])\n",
    "print(mars_summary['session_length'].describe())\n",
    "print(\"MARS pairs:\", len(mars_pairs_df))\n",
    "mars_pairs_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319bc20",
   "metadata": {},
   "source": [
    "#### G — Save metadata (paths & counts) for downstream notebooks\n",
    "\n",
    "Write a small JSON manifest so later notebooks can find files easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = {\n",
    "    \"amazon_windows_parts\": [str(p) for p in sorted(OUT_DIR.glob(\"amazon_windows_sessions_part*.parquet\"))],\n",
    "    \"amazon_prefix_target_parts\": [str(p) for p in sorted(OUT_DIR.glob(\"amazon_prefix_target_part*.parquet\"))],\n",
    "    \"mars_sessions\": str(OUT_DIR / \"mars_sessions.parquet\"),\n",
    "    \"mars_session_summary\": str(OUT_DIR / \"mars_session_summary.csv\"),\n",
    "    \"mars_prefix_target\": str(OUT_DIR / \"mars_prefix_target.parquet\")\n",
    "}\n",
    "with open(OUT_DIR / \"sessionization_manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(\"Saved manifest:\", OUT_DIR / \"sessionization_manifest.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
