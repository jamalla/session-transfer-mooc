{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505345df",
   "metadata": {},
   "source": [
    "Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ba816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05B] Output dir: C:\\Users\\User\\Documents\\ml-workspace\\session-transfer-mooc\\data\\processed\\tensor_shards\n"
     ]
    }
   ],
   "source": [
    "# CELL 1 — Imports & config\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "OUT = DATA_DIR / \"tensor_shards\"\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "MAX_PREFIX_LEN = 20\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "SHARD_SIZE = 250_000\n",
    "\n",
    "print(\"[05B] Output dir:\", OUT.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259cf93",
   "metadata": {},
   "source": [
    "Load manifest & splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad0e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05B] Domains in splits: dict_keys(['mars'])\n"
     ]
    }
   ],
   "source": [
    "# CELL 2 — Load manifest & splits\n",
    "\n",
    "with open(DATA_DIR / \"sessionization_manifest.json\") as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "splits = manifest[\"splits\"]\n",
    "\n",
    "print(\"[05B] Domains in splits:\", splits.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150aa4c9",
   "metadata": {},
   "source": [
    "Build vocabulary helper (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11ce4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2.1 — Vocabulary builder\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab_from_prefix_parts(files, top_k=None):\n",
    "    counter = Counter()\n",
    "\n",
    "    for p in files:\n",
    "        df = pd.read_parquet(p, columns=[\"prefix\", \"target\"])\n",
    "        for row in df.itertuples(index=False):\n",
    "            if isinstance(row.prefix, str):\n",
    "                counter.update(row.prefix.split())\n",
    "            counter.update([str(row.target)])\n",
    "\n",
    "    # Reserve PAD=0, UNK=1\n",
    "    item2id = {\"<PAD>\": PAD_ID, \"<UNK>\": UNK_ID}\n",
    "\n",
    "    for item, _ in counter.most_common(top_k):\n",
    "        if item not in item2id:\n",
    "            item2id[item] = len(item2id)\n",
    "\n",
    "    return item2id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d97fec",
   "metadata": {},
   "source": [
    "Build SOURCE vocabulary (Amazon + YooChoose TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba243e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] splits keys: dict_keys(['mars'])\n",
      "[DEBUG] domain = mars\n",
      "        subkeys: dict_keys(['train', 'val', 'test'])\n"
     ]
    }
   ],
   "source": [
    "# CELL 2.1.5 — Debug splits keys\n",
    "\n",
    "print(\"[DEBUG] splits keys:\", splits.keys())\n",
    "\n",
    "for k, v in splits.items():\n",
    "    print(f\"[DEBUG] domain = {k}\")\n",
    "    if isinstance(v, dict):\n",
    "        print(\"        subkeys:\", v.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b30ec235",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'amazon'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# CELL 2.2 — Build SOURCE vocabulary\u001b[39;00m\n\u001b[32m      3\u001b[39m source_train_files = (\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamazon\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m] +\n\u001b[32m      5\u001b[39m     splits[\u001b[33m\"\u001b[39m\u001b[33myoochoose\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      6\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[VOCAB][SOURCE] Number of train files:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(source_train_files))\n\u001b[32m     11\u001b[39m item2id_source = build_vocab_from_prefix_parts(\n\u001b[32m     12\u001b[39m     source_train_files,\n\u001b[32m     13\u001b[39m     top_k=\u001b[32m200_000\u001b[39m  \u001b[38;5;66;03m# keep consistent with earlier design\u001b[39;00m\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[31mKeyError\u001b[39m: 'amazon'"
     ]
    }
   ],
   "source": [
    "# CELL 2.2 — Build SOURCE vocabulary\n",
    "\n",
    "source_train_files = (\n",
    "    splits[\"amazon\"][\"train\"] +\n",
    "    splits[\"yoochoose\"][\"train\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"[VOCAB][SOURCE] Number of train files:\", len(source_train_files))\n",
    "\n",
    "item2id_source = build_vocab_from_prefix_parts(\n",
    "    source_train_files,\n",
    "    top_k=200_000  # keep consistent with earlier design\n",
    ")\n",
    "\n",
    "print(\"[VOCAB][SOURCE] Vocab size:\", len(item2id_source))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f2c6a",
   "metadata": {},
   "source": [
    "Load vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06beb08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3 — Load vocabularies\n",
    "\n",
    "item2id_source = json.load(open(DATA_DIR / \"vocab_topn/item2id_source.json\"))\n",
    "item2id_target = json.load(open(DATA_DIR / \"vocab_topn/item2id_target.json\"))\n",
    "\n",
    "print(\"[05B] Source vocab size:\", len(item2id_source))\n",
    "print(\"[05B] Target vocab size:\", len(item2id_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7475c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd9d280c",
   "metadata": {},
   "source": [
    "Tensor builder (core logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ff6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4 — Tensor builder\n",
    "\n",
    "def build_tensors(domain, split, files, item2id):\n",
    "    buffer = {\n",
    "        \"prefix\": [],\n",
    "        \"target\": [],\n",
    "        \"length\": [],\n",
    "        \"attn_mask\": [],\n",
    "        \"pos_ids\": []\n",
    "    }\n",
    "    shard_id = 0\n",
    "\n",
    "    def flush():\n",
    "        nonlocal shard_id\n",
    "        if not buffer[\"prefix\"]:\n",
    "            return\n",
    "        pt = {k: torch.LongTensor(v) for k, v in buffer.items()}\n",
    "        out_path = OUT / f\"{domain}_{split}_shard_{shard_id:03d}.pt\"\n",
    "        torch.save(pt, out_path)\n",
    "        print(f\"[05B] Saved {out_path}\")\n",
    "        shard_id += 1\n",
    "        for k in buffer:\n",
    "            buffer[k].clear()\n",
    "\n",
    "    for part in files:\n",
    "        df = pd.read_parquet(part, columns=[\"prefix\", \"target\"])\n",
    "        for row in df.itertuples(index=False):\n",
    "            pref = row.prefix.split() if isinstance(row.prefix, str) else []\n",
    "            ids = [item2id.get(x, UNK_ID) for x in pref]\n",
    "\n",
    "            if len(ids) > MAX_PREFIX_LEN:\n",
    "                ids = ids[-MAX_PREFIX_LEN:]\n",
    "\n",
    "            length = len(ids)\n",
    "            pad_len = MAX_PREFIX_LEN - length\n",
    "\n",
    "            padded = [PAD_ID]*pad_len + ids\n",
    "            attn_mask = [0]*pad_len + [1]*length\n",
    "            pos_ids = list(range(MAX_PREFIX_LEN))\n",
    "\n",
    "            buffer[\"prefix\"].append(padded)\n",
    "            buffer[\"target\"].append(item2id.get(str(row.target), UNK_ID))\n",
    "            buffer[\"length\"].append(length)\n",
    "            buffer[\"attn_mask\"].append(attn_mask)\n",
    "            buffer[\"pos_ids\"].append(pos_ids)\n",
    "\n",
    "            if len(buffer[\"prefix\"]) >= SHARD_SIZE:\n",
    "                flush()\n",
    "\n",
    "    flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
