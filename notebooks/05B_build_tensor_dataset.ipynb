{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168288f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\n"
     ]
    }
   ],
   "source": [
    "# Quick (unsafe) workaround to avoid the libiomp5md.dll crash.\n",
    "# Use this only to continue working in the notebook quickly.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "print(\"Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d89bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating tensor shards.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "MAX_PREFIX_LEN = 20\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "manifest = json.load(open(DATA_DIR / \"sessionization_manifest.json\"))\n",
    "item2id = json.load(open(DATA_DIR / \"vocab_topn/item2id_top200000.json\"))\n",
    "\n",
    "pair_parts = manifest[\"amazon_prefix_parts\"]\n",
    "\n",
    "OUT = DATA_DIR / \"tensor_shards\"\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "shard_size = 250_000\n",
    "buffer = {\"prefix\": [], \"target\": [], \"length\": []}\n",
    "shard_id = 0\n",
    "\n",
    "def flush():\n",
    "    global shard_id\n",
    "    if not buffer[\"prefix\"]:\n",
    "        return\n",
    "    pt = {\n",
    "        \"prefix\": torch.LongTensor(buffer[\"prefix\"]),\n",
    "        \"target\": torch.LongTensor(buffer[\"target\"]),\n",
    "        \"length\": torch.LongTensor(buffer[\"length\"]),\n",
    "    }\n",
    "    torch.save(pt, OUT / f\"shard_{shard_id:03d}.pt\")\n",
    "    shard_id += 1\n",
    "    buffer[\"prefix\"].clear()\n",
    "    buffer[\"target\"].clear()\n",
    "    buffer[\"length\"].clear()\n",
    "\n",
    "for part in pair_parts:\n",
    "    df = pd.read_parquet(part, columns=[\"prefix\",\"target\"])\n",
    "    for _, row in df.iterrows():\n",
    "        pref = str(row[\"prefix\"]).split() if isinstance(row[\"prefix\"], str) else []\n",
    "        ids = [ item2id.get(x, 0) for x in pref ]\n",
    "        if len(ids) > MAX_PREFIX_LEN:\n",
    "            ids = ids[-MAX_PREFIX_LEN:]\n",
    "        padded = [0]*(MAX_PREFIX_LEN - len(ids)) + ids\n",
    "        \n",
    "        buffer[\"prefix\"].append(padded)\n",
    "        buffer[\"target\"].append(item2id.get(str(row[\"target\"]), 0))\n",
    "        buffer[\"length\"].append(len(ids))\n",
    "        \n",
    "        if len(buffer[\"prefix\"]) >= shard_size:\n",
    "            flush()\n",
    "\n",
    "flush()\n",
    "print(\"Finished creating tensor shards.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
