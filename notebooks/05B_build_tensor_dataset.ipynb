{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0027c30",
   "metadata": {},
   "source": [
    "Imports & environment notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36264ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452931ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05B-01] Starting 05B_build_tensor_dataset.ipynb\n",
      "[05B-01] pandas: 2.3.3\n",
      "[05B-01] torch: 2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "# CELL [05B-01] — Imports & environment notes\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"[05B-01] Starting 05B_build_tensor_dataset.ipynb\")\n",
    "print(\"[05B-01] pandas:\", pd.__version__)\n",
    "print(\"[05B-01] torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1cd68a",
   "metadata": {},
   "source": [
    "Config, paths, and output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1daca34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05B-02] DATA_DIR: C:\\Users\\User\\Documents\\ml-workspace\\session-transfer-mooc\\data\\processed\n",
      "[05B-02] OUT_DIR: C:\\Users\\User\\Documents\\ml-workspace\\session-transfer-mooc\\data\\processed\\tensor_shards_v2\n",
      "[05B-02] VOCAB_DIR: C:\\Users\\User\\Documents\\ml-workspace\\session-transfer-mooc\\data\\processed\\vocab_topn\n",
      "[05B-02] MAX_PREFIX_LEN: 20\n",
      "[05B-02] SHARD_SIZE: 250000\n"
     ]
    }
   ],
   "source": [
    "# CELL [05B-02] — Config & paths\n",
    "\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "MANIFEST_PATH = DATA_DIR / \"sessionization_manifest.json\"\n",
    "\n",
    "OUT_DIR = DATA_DIR / \"tensor_shards_v2\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "VOCAB_DIR = DATA_DIR / \"vocab_topn\"\n",
    "VOCAB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MAX_PREFIX_LEN = 20\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "\n",
    "SHARD_SIZE = 250_000  # number of examples per .pt shard\n",
    "\n",
    "print(\"[05B-02] DATA_DIR:\", DATA_DIR.resolve())\n",
    "print(\"[05B-02] OUT_DIR:\", OUT_DIR.resolve())\n",
    "print(\"[05B-02] VOCAB_DIR:\", VOCAB_DIR.resolve())\n",
    "print(\"[05B-02] MAX_PREFIX_LEN:\", MAX_PREFIX_LEN)\n",
    "print(\"[05B-02] SHARD_SIZE:\", SHARD_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a199d",
   "metadata": {},
   "source": [
    "Load manifest and verify splits exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f38f323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05B-03] Splits domains: ['amazon', 'yoochoose', 'mars']\n",
      "[05B-03] amazon splits keys: ['train', 'val', 'test']\n",
      "[05B-03] yoochoose splits keys: ['train', 'val', 'test']\n",
      "[05B-03] mars splits keys: ['train', 'val', 'test']\n"
     ]
    }
   ],
   "source": [
    "# CELL [05B-03] — Load manifest & validate\n",
    "\n",
    "with open(MANIFEST_PATH, \"r\") as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "assert \"splits\" in manifest, \"[05B-03] ERROR: manifest has no 'splits'. Run 05A again.\"\n",
    "\n",
    "splits = manifest[\"splits\"]\n",
    "\n",
    "print(\"[05B-03] Splits domains:\", list(splits.keys()))\n",
    "for d in [\"amazon\", \"yoochoose\", \"mars\"]:\n",
    "    assert d in splits, f\"[05B-03] ERROR: Missing splits for domain: {d}\"\n",
    "    print(f\"[05B-03] {d} splits keys:\", list(splits[d].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ccdc46",
   "metadata": {},
   "source": [
    "Utility: normalize split file lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039ddc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL [05B-04] — Helper to normalize split file lists\n",
    "\n",
    "def as_file_list(x):\n",
    "    \"\"\"Ensure split entries become a list of file paths.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    return [x]\n",
    "\n",
    "def assert_files_exist(files, label):\n",
    "    missing = [p for p in files if not Path(p).exists()]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"[05B-04] Missing files for {label}: {missing[:3]} ... total missing={len(missing)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe402123",
   "metadata": {},
   "source": [
    "Build vocab helper (fast, deterministic, with logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac10289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL [05B-05] — Vocabulary builder (TRAIN only)\n",
    "\n",
    "def build_vocab_from_prefix_parts(files, top_k=None, log_every=2):\n",
    "    \"\"\"\n",
    "    Build item2id from prefix+target tokens in parquet files.\n",
    "    Reserves PAD_ID=0 and UNK_ID=1.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    files = list(files)\n",
    "\n",
    "    for i, p in enumerate(files, 1):\n",
    "        df = pd.read_parquet(p, columns=[\"prefix\", \"target\"])\n",
    "        for row in df.itertuples(index=False):\n",
    "            if isinstance(row.prefix, str) and row.prefix:\n",
    "                counter.update(row.prefix.split())\n",
    "            counter.update([str(row.target)])\n",
    "\n",
    "        if (i % log_every == 0) or (i == len(files)):\n",
    "            print(f\"[05B-05][VOCAB] {i}/{len(files)} files | unique_items_so_far={len(counter)}\")\n",
    "\n",
    "    item2id = {\"<PAD>\": PAD_ID, \"<UNK>\": UNK_ID}\n",
    "\n",
    "    for item, _ in counter.most_common(top_k):\n",
    "        if item not in item2id:\n",
    "            item2id[item] = len(item2id)\n",
    "\n",
    "    return item2id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda06569",
   "metadata": {},
   "source": [
    "Build and save source vocabulary (Amazon+YooChoose TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "101921fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05B-06] Source train files: 33\n",
      "[05B-06] Example file: ..\\data\\processed\\amazon_prefix_parts\\amazon_prefix_target_part0000.parquet\n",
      "[05B-05][VOCAB] 2/33 files | unique_items_so_far=1058402\n",
      "[05B-05][VOCAB] 4/33 files | unique_items_so_far=1655641\n",
      "[05B-05][VOCAB] 6/33 files | unique_items_so_far=2109510\n",
      "[05B-05][VOCAB] 8/33 files | unique_items_so_far=2485306\n",
      "[05B-05][VOCAB] 10/33 files | unique_items_so_far=2810132\n",
      "[05B-05][VOCAB] 12/33 files | unique_items_so_far=3094795\n",
      "[05B-05][VOCAB] 14/33 files | unique_items_so_far=3354653\n",
      "[05B-05][VOCAB] 16/33 files | unique_items_so_far=3381969\n",
      "[05B-05][VOCAB] 18/33 files | unique_items_so_far=3387102\n",
      "[05B-05][VOCAB] 20/33 files | unique_items_so_far=3390741\n",
      "[05B-05][VOCAB] 22/33 files | unique_items_so_far=3393720\n",
      "[05B-05][VOCAB] 24/33 files | unique_items_so_far=3396216\n",
      "[05B-05][VOCAB] 26/33 files | unique_items_so_far=3398247\n",
      "[05B-05][VOCAB] 28/33 files | unique_items_so_far=3399956\n",
      "[05B-05][VOCAB] 30/33 files | unique_items_so_far=3401302\n",
      "[05B-05][VOCAB] 32/33 files | unique_items_so_far=3402357\n",
      "[05B-05][VOCAB] 33/33 files | unique_items_so_far=3403075\n",
      "[05B-06] Saved source vocab: ..\\data\\processed\\vocab_topn\\item2id_source.json\n",
      "[05B-06] Source vocab size: 200002\n"
     ]
    }
   ],
   "source": [
    "# CELL [05B-06] — Build SOURCE vocab (amazon+yoochoose TRAIN)\n",
    "\n",
    "amazon_train_files = as_file_list(splits[\"amazon\"][\"train\"])\n",
    "yoo_train_files = as_file_list(splits[\"yoochoose\"][\"train\"])\n",
    "\n",
    "assert_files_exist(amazon_train_files, \"amazon/train\")\n",
    "assert_files_exist(yoo_train_files, \"yoochoose/train\")\n",
    "\n",
    "source_train_files = amazon_train_files + yoo_train_files\n",
    "\n",
    "print(\"[05B-06] Source train files:\", len(source_train_files))\n",
    "print(\"[05B-06] Example file:\", source_train_files[0])\n",
    "\n",
    "item2id_source = build_vocab_from_prefix_parts(\n",
    "    source_train_files,\n",
    "    top_k=200_000,\n",
    "    log_every=2\n",
    ")\n",
    "\n",
    "source_vocab_path = VOCAB_DIR / \"item2id_source.json\"\n",
    "with open(source_vocab_path, \"w\") as f:\n",
    "    json.dump(item2id_source, f)\n",
    "\n",
    "print(\"[05B-06] Saved source vocab:\", source_vocab_path)\n",
    "print(\"[05B-06] Source vocab size:\", len(item2id_source))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd0631",
   "metadata": {},
   "source": [
    "Build and save target vocabulary (MARS TRAIN only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aaea285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05B-07] Mars train file: ..\\data\\processed\\mars_splits\\mars_prefix_target_train.parquet\n",
      "[05B-05][VOCAB] 1/1 files | unique_items_so_far=700\n",
      "[05B-07] Saved target vocab: ..\\data\\processed\\vocab_topn\\item2id_target.json\n",
      "[05B-07] Target vocab size: 702\n"
     ]
    }
   ],
   "source": [
    "# CELL [05B-07] — Build TARGET vocab (mars TRAIN)\n",
    "\n",
    "mars_train_file = splits[\"mars\"][\"train\"]\n",
    "mars_train_files = as_file_list(mars_train_file)\n",
    "\n",
    "assert_files_exist(mars_train_files, \"mars/train\")\n",
    "\n",
    "print(\"[05B-07] Mars train file:\", mars_train_files[0])\n",
    "\n",
    "item2id_target = build_vocab_from_prefix_parts(\n",
    "    mars_train_files,\n",
    "    top_k=None,\n",
    "    log_every=1\n",
    ")\n",
    "\n",
    "target_vocab_path = VOCAB_DIR / \"item2id_target.json\"\n",
    "with open(target_vocab_path, \"w\") as f:\n",
    "    json.dump(item2id_target, f)\n",
    "\n",
    "print(\"[05B-07] Saved target vocab:\", target_vocab_path)\n",
    "print(\"[05B-07] Target vocab size:\", len(item2id_target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565495c5",
   "metadata": {},
   "source": [
    "Tensorization core (writes shards with logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3a6ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL [05B-08] — Tensorization core\n",
    "\n",
    "def tensorize_split(domain, split_name, files, item2id, max_len=MAX_PREFIX_LEN):\n",
    "    \"\"\"\n",
    "    Convert prefix->target pairs in parquet files into tensor shards.\n",
    "    Writes: input_ids, attention_mask, pos_ids, labels, lengths\n",
    "    \"\"\"\n",
    "    files = list(files)\n",
    "    assert_files_exist(files, f\"{domain}/{split_name}\")\n",
    "\n",
    "    shard_id = 0\n",
    "    n_rows_total = 0\n",
    "\n",
    "    buffer = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"pos_ids\": [],\n",
    "        \"labels\": [],\n",
    "        \"lengths\": []\n",
    "    }\n",
    "\n",
    "    def flush():\n",
    "        nonlocal shard_id\n",
    "        if not buffer[\"input_ids\"]:\n",
    "            return\n",
    "\n",
    "        pt = {k: torch.LongTensor(v) for k, v in buffer.items()}\n",
    "        out_path = OUT_DIR / f\"{domain}_{split_name}_shard_{shard_id:03d}.pt\"\n",
    "        torch.save(pt, out_path)\n",
    "\n",
    "        print(f\"[05B-08][SAVE] {domain}/{split_name} shard={shard_id:03d} \"\n",
    "              f\"rows={len(buffer['input_ids'])} -> {out_path.name}\")\n",
    "\n",
    "        shard_id += 1\n",
    "        for k in buffer:\n",
    "            buffer[k].clear()\n",
    "\n",
    "    for fi, p in enumerate(files, 1):\n",
    "        df = pd.read_parquet(p, columns=[\"prefix\", \"target\"])\n",
    "        n_rows_total += len(df)\n",
    "\n",
    "        for row in df.itertuples(index=False):\n",
    "            pref_tokens = row.prefix.split() if isinstance(row.prefix, str) and row.prefix else []\n",
    "            ids = [item2id.get(t, UNK_ID) for t in pref_tokens]\n",
    "\n",
    "            # right-truncate\n",
    "            if len(ids) > max_len:\n",
    "                ids = ids[-max_len:]\n",
    "\n",
    "            length = len(ids)\n",
    "            pad_len = max_len - length\n",
    "\n",
    "            input_ids = [PAD_ID] * pad_len + ids\n",
    "            attention_mask = [0] * pad_len + [1] * length\n",
    "            pos_ids = list(range(max_len))\n",
    "            label = item2id.get(str(row.target), UNK_ID)\n",
    "\n",
    "            buffer[\"input_ids\"].append(input_ids)\n",
    "            buffer[\"attention_mask\"].append(attention_mask)\n",
    "            buffer[\"pos_ids\"].append(pos_ids)\n",
    "            buffer[\"labels\"].append(label)\n",
    "            buffer[\"lengths\"].append(length)\n",
    "\n",
    "            if len(buffer[\"input_ids\"]) >= SHARD_SIZE:\n",
    "                flush()\n",
    "\n",
    "        print(f\"[05B-08][READ] {domain}/{split_name} file {fi}/{len(files)} \"\n",
    "              f\"rows={len(df)} | total_rows_seen={n_rows_total}\")\n",
    "\n",
    "    flush()\n",
    "    print(f\"[05B-08][DONE] {domain}/{split_name} total_rows={n_rows_total} shards={shard_id}\")\n",
    "    return {\"total_rows\": n_rows_total, \"shards\": shard_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb48f78c",
   "metadata": {},
   "source": [
    "Run tensorization for ALL domains/splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae27c63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05B-09] Tensorizing amazon/train | files=14\n",
      "[05B-08][SAVE] amazon/train shard=000 rows=250000 -> amazon_train_shard_000.pt\n",
      "[05B-08][SAVE] amazon/train shard=001 rows=250000 -> amazon_train_shard_001.pt\n",
      "[05B-08][SAVE] amazon/train shard=002 rows=250000 -> amazon_train_shard_002.pt\n",
      "[05B-08][SAVE] amazon/train shard=003 rows=250000 -> amazon_train_shard_003.pt\n",
      "[05B-08][READ] amazon/train file 1/14 rows=1000000 | total_rows_seen=1000000\n",
      "[05B-08][SAVE] amazon/train shard=004 rows=250000 -> amazon_train_shard_004.pt\n",
      "[05B-08][SAVE] amazon/train shard=005 rows=250000 -> amazon_train_shard_005.pt\n",
      "[05B-08][SAVE] amazon/train shard=006 rows=250000 -> amazon_train_shard_006.pt\n",
      "[05B-08][SAVE] amazon/train shard=007 rows=250000 -> amazon_train_shard_007.pt\n",
      "[05B-08][READ] amazon/train file 2/14 rows=1000000 | total_rows_seen=2000000\n",
      "[05B-08][SAVE] amazon/train shard=008 rows=250000 -> amazon_train_shard_008.pt\n",
      "[05B-08][SAVE] amazon/train shard=009 rows=250000 -> amazon_train_shard_009.pt\n",
      "[05B-08][SAVE] amazon/train shard=010 rows=250000 -> amazon_train_shard_010.pt\n",
      "[05B-08][SAVE] amazon/train shard=011 rows=250000 -> amazon_train_shard_011.pt\n",
      "[05B-08][READ] amazon/train file 3/14 rows=1000000 | total_rows_seen=3000000\n",
      "[05B-08][SAVE] amazon/train shard=012 rows=250000 -> amazon_train_shard_012.pt\n",
      "[05B-08][SAVE] amazon/train shard=013 rows=250000 -> amazon_train_shard_013.pt\n",
      "[05B-08][SAVE] amazon/train shard=014 rows=250000 -> amazon_train_shard_014.pt\n",
      "[05B-08][SAVE] amazon/train shard=015 rows=250000 -> amazon_train_shard_015.pt\n",
      "[05B-08][READ] amazon/train file 4/14 rows=1000000 | total_rows_seen=4000000\n",
      "[05B-08][SAVE] amazon/train shard=016 rows=250000 -> amazon_train_shard_016.pt\n",
      "[05B-08][SAVE] amazon/train shard=017 rows=250000 -> amazon_train_shard_017.pt\n",
      "[05B-08][SAVE] amazon/train shard=018 rows=250000 -> amazon_train_shard_018.pt\n",
      "[05B-08][SAVE] amazon/train shard=019 rows=250000 -> amazon_train_shard_019.pt\n",
      "[05B-08][READ] amazon/train file 5/14 rows=1000000 | total_rows_seen=5000000\n",
      "[05B-08][SAVE] amazon/train shard=020 rows=250000 -> amazon_train_shard_020.pt\n",
      "[05B-08][SAVE] amazon/train shard=021 rows=250000 -> amazon_train_shard_021.pt\n",
      "[05B-08][SAVE] amazon/train shard=022 rows=250000 -> amazon_train_shard_022.pt\n",
      "[05B-08][SAVE] amazon/train shard=023 rows=250000 -> amazon_train_shard_023.pt\n",
      "[05B-08][READ] amazon/train file 6/14 rows=1000000 | total_rows_seen=6000000\n",
      "[05B-08][SAVE] amazon/train shard=024 rows=250000 -> amazon_train_shard_024.pt\n",
      "[05B-08][SAVE] amazon/train shard=025 rows=250000 -> amazon_train_shard_025.pt\n",
      "[05B-08][SAVE] amazon/train shard=026 rows=250000 -> amazon_train_shard_026.pt\n",
      "[05B-08][SAVE] amazon/train shard=027 rows=250000 -> amazon_train_shard_027.pt\n",
      "[05B-08][READ] amazon/train file 7/14 rows=1000000 | total_rows_seen=7000000\n",
      "[05B-08][SAVE] amazon/train shard=028 rows=250000 -> amazon_train_shard_028.pt\n",
      "[05B-08][SAVE] amazon/train shard=029 rows=250000 -> amazon_train_shard_029.pt\n",
      "[05B-08][SAVE] amazon/train shard=030 rows=250000 -> amazon_train_shard_030.pt\n",
      "[05B-08][SAVE] amazon/train shard=031 rows=250000 -> amazon_train_shard_031.pt\n",
      "[05B-08][READ] amazon/train file 8/14 rows=1000000 | total_rows_seen=8000000\n",
      "[05B-08][SAVE] amazon/train shard=032 rows=250000 -> amazon_train_shard_032.pt\n",
      "[05B-08][SAVE] amazon/train shard=033 rows=250000 -> amazon_train_shard_033.pt\n",
      "[05B-08][SAVE] amazon/train shard=034 rows=250000 -> amazon_train_shard_034.pt\n",
      "[05B-08][SAVE] amazon/train shard=035 rows=250000 -> amazon_train_shard_035.pt\n",
      "[05B-08][READ] amazon/train file 9/14 rows=1000000 | total_rows_seen=9000000\n",
      "[05B-08][SAVE] amazon/train shard=036 rows=250000 -> amazon_train_shard_036.pt\n",
      "[05B-08][SAVE] amazon/train shard=037 rows=250000 -> amazon_train_shard_037.pt\n",
      "[05B-08][SAVE] amazon/train shard=038 rows=250000 -> amazon_train_shard_038.pt\n",
      "[05B-08][SAVE] amazon/train shard=039 rows=250000 -> amazon_train_shard_039.pt\n",
      "[05B-08][READ] amazon/train file 10/14 rows=1000000 | total_rows_seen=10000000\n",
      "[05B-08][SAVE] amazon/train shard=040 rows=250000 -> amazon_train_shard_040.pt\n",
      "[05B-08][SAVE] amazon/train shard=041 rows=250000 -> amazon_train_shard_041.pt\n",
      "[05B-08][SAVE] amazon/train shard=042 rows=250000 -> amazon_train_shard_042.pt\n",
      "[05B-08][SAVE] amazon/train shard=043 rows=250000 -> amazon_train_shard_043.pt\n",
      "[05B-08][READ] amazon/train file 11/14 rows=1000000 | total_rows_seen=11000000\n",
      "[05B-08][SAVE] amazon/train shard=044 rows=250000 -> amazon_train_shard_044.pt\n",
      "[05B-08][SAVE] amazon/train shard=045 rows=250000 -> amazon_train_shard_045.pt\n",
      "[05B-08][SAVE] amazon/train shard=046 rows=250000 -> amazon_train_shard_046.pt\n",
      "[05B-08][SAVE] amazon/train shard=047 rows=250000 -> amazon_train_shard_047.pt\n",
      "[05B-08][READ] amazon/train file 12/14 rows=1000000 | total_rows_seen=12000000\n",
      "[05B-08][SAVE] amazon/train shard=048 rows=250000 -> amazon_train_shard_048.pt\n",
      "[05B-08][SAVE] amazon/train shard=049 rows=250000 -> amazon_train_shard_049.pt\n",
      "[05B-08][SAVE] amazon/train shard=050 rows=250000 -> amazon_train_shard_050.pt\n",
      "[05B-08][SAVE] amazon/train shard=051 rows=250000 -> amazon_train_shard_051.pt\n",
      "[05B-08][READ] amazon/train file 13/14 rows=1000000 | total_rows_seen=13000000\n",
      "[05B-08][SAVE] amazon/train shard=052 rows=250000 -> amazon_train_shard_052.pt\n",
      "[05B-08][SAVE] amazon/train shard=053 rows=250000 -> amazon_train_shard_053.pt\n",
      "[05B-08][SAVE] amazon/train shard=054 rows=250000 -> amazon_train_shard_054.pt\n",
      "[05B-08][SAVE] amazon/train shard=055 rows=250000 -> amazon_train_shard_055.pt\n",
      "[05B-08][READ] amazon/train file 14/14 rows=1000000 | total_rows_seen=14000000\n",
      "[05B-08][DONE] amazon/train total_rows=14000000 shards=56\n",
      "[05B-09] Tensorizing amazon/val | files=2\n",
      "[05B-08][SAVE] amazon/val shard=000 rows=250000 -> amazon_val_shard_000.pt\n",
      "[05B-08][SAVE] amazon/val shard=001 rows=250000 -> amazon_val_shard_001.pt\n",
      "[05B-08][SAVE] amazon/val shard=002 rows=250000 -> amazon_val_shard_002.pt\n",
      "[05B-08][SAVE] amazon/val shard=003 rows=250000 -> amazon_val_shard_003.pt\n",
      "[05B-08][READ] amazon/val file 1/2 rows=1000000 | total_rows_seen=1000000\n",
      "[05B-08][SAVE] amazon/val shard=004 rows=250000 -> amazon_val_shard_004.pt\n",
      "[05B-08][SAVE] amazon/val shard=005 rows=250000 -> amazon_val_shard_005.pt\n",
      "[05B-08][SAVE] amazon/val shard=006 rows=250000 -> amazon_val_shard_006.pt\n",
      "[05B-08][SAVE] amazon/val shard=007 rows=250000 -> amazon_val_shard_007.pt\n",
      "[05B-08][READ] amazon/val file 2/2 rows=1000000 | total_rows_seen=2000000\n",
      "[05B-08][DONE] amazon/val total_rows=2000000 shards=8\n",
      "[05B-09] Tensorizing amazon/test | files=2\n",
      "[05B-08][SAVE] amazon/test shard=000 rows=250000 -> amazon_test_shard_000.pt\n",
      "[05B-08][SAVE] amazon/test shard=001 rows=250000 -> amazon_test_shard_001.pt\n",
      "[05B-08][SAVE] amazon/test shard=002 rows=250000 -> amazon_test_shard_002.pt\n",
      "[05B-08][SAVE] amazon/test shard=003 rows=250000 -> amazon_test_shard_003.pt\n",
      "[05B-08][READ] amazon/test file 1/2 rows=1000000 | total_rows_seen=1000000\n",
      "[05B-08][SAVE] amazon/test shard=004 rows=250000 -> amazon_test_shard_004.pt\n",
      "[05B-08][READ] amazon/test file 2/2 rows=464455 | total_rows_seen=1464455\n",
      "[05B-08][SAVE] amazon/test shard=005 rows=214455 -> amazon_test_shard_005.pt\n",
      "[05B-08][DONE] amazon/test total_rows=1464455 shards=6\n",
      "[05B-09] Tensorizing yoochoose/train | files=19\n",
      "[05B-08][SAVE] yoochoose/train shard=000 rows=250000 -> yoochoose_train_shard_000.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=001 rows=250000 -> yoochoose_train_shard_001.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=002 rows=250000 -> yoochoose_train_shard_002.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=003 rows=250000 -> yoochoose_train_shard_003.pt\n",
      "[05B-08][READ] yoochoose/train file 1/19 rows=1000000 | total_rows_seen=1000000\n",
      "[05B-08][SAVE] yoochoose/train shard=004 rows=250000 -> yoochoose_train_shard_004.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=005 rows=250000 -> yoochoose_train_shard_005.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=006 rows=250000 -> yoochoose_train_shard_006.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=007 rows=250000 -> yoochoose_train_shard_007.pt\n",
      "[05B-08][READ] yoochoose/train file 2/19 rows=1000000 | total_rows_seen=2000000\n",
      "[05B-08][SAVE] yoochoose/train shard=008 rows=250000 -> yoochoose_train_shard_008.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=009 rows=250000 -> yoochoose_train_shard_009.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=010 rows=250000 -> yoochoose_train_shard_010.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=011 rows=250000 -> yoochoose_train_shard_011.pt\n",
      "[05B-08][READ] yoochoose/train file 3/19 rows=1000000 | total_rows_seen=3000000\n",
      "[05B-08][SAVE] yoochoose/train shard=012 rows=250000 -> yoochoose_train_shard_012.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=013 rows=250000 -> yoochoose_train_shard_013.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=014 rows=250000 -> yoochoose_train_shard_014.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=015 rows=250000 -> yoochoose_train_shard_015.pt\n",
      "[05B-08][READ] yoochoose/train file 4/19 rows=1000000 | total_rows_seen=4000000\n",
      "[05B-08][SAVE] yoochoose/train shard=016 rows=250000 -> yoochoose_train_shard_016.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=017 rows=250000 -> yoochoose_train_shard_017.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=018 rows=250000 -> yoochoose_train_shard_018.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=019 rows=250000 -> yoochoose_train_shard_019.pt\n",
      "[05B-08][READ] yoochoose/train file 5/19 rows=1000000 | total_rows_seen=5000000\n",
      "[05B-08][SAVE] yoochoose/train shard=020 rows=250000 -> yoochoose_train_shard_020.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=021 rows=250000 -> yoochoose_train_shard_021.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=022 rows=250000 -> yoochoose_train_shard_022.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=023 rows=250000 -> yoochoose_train_shard_023.pt\n",
      "[05B-08][READ] yoochoose/train file 6/19 rows=1000000 | total_rows_seen=6000000\n",
      "[05B-08][SAVE] yoochoose/train shard=024 rows=250000 -> yoochoose_train_shard_024.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=025 rows=250000 -> yoochoose_train_shard_025.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=026 rows=250000 -> yoochoose_train_shard_026.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=027 rows=250000 -> yoochoose_train_shard_027.pt\n",
      "[05B-08][READ] yoochoose/train file 7/19 rows=1000000 | total_rows_seen=7000000\n",
      "[05B-08][SAVE] yoochoose/train shard=028 rows=250000 -> yoochoose_train_shard_028.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=029 rows=250000 -> yoochoose_train_shard_029.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=030 rows=250000 -> yoochoose_train_shard_030.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=031 rows=250000 -> yoochoose_train_shard_031.pt\n",
      "[05B-08][READ] yoochoose/train file 8/19 rows=1000000 | total_rows_seen=8000000\n",
      "[05B-08][SAVE] yoochoose/train shard=032 rows=250000 -> yoochoose_train_shard_032.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=033 rows=250000 -> yoochoose_train_shard_033.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=034 rows=250000 -> yoochoose_train_shard_034.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=035 rows=250000 -> yoochoose_train_shard_035.pt\n",
      "[05B-08][READ] yoochoose/train file 9/19 rows=1000000 | total_rows_seen=9000000\n",
      "[05B-08][SAVE] yoochoose/train shard=036 rows=250000 -> yoochoose_train_shard_036.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=037 rows=250000 -> yoochoose_train_shard_037.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=038 rows=250000 -> yoochoose_train_shard_038.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=039 rows=250000 -> yoochoose_train_shard_039.pt\n",
      "[05B-08][READ] yoochoose/train file 10/19 rows=1000000 | total_rows_seen=10000000\n",
      "[05B-08][SAVE] yoochoose/train shard=040 rows=250000 -> yoochoose_train_shard_040.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=041 rows=250000 -> yoochoose_train_shard_041.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=042 rows=250000 -> yoochoose_train_shard_042.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=043 rows=250000 -> yoochoose_train_shard_043.pt\n",
      "[05B-08][READ] yoochoose/train file 11/19 rows=1000000 | total_rows_seen=11000000\n",
      "[05B-08][SAVE] yoochoose/train shard=044 rows=250000 -> yoochoose_train_shard_044.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=045 rows=250000 -> yoochoose_train_shard_045.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=046 rows=250000 -> yoochoose_train_shard_046.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=047 rows=250000 -> yoochoose_train_shard_047.pt\n",
      "[05B-08][READ] yoochoose/train file 12/19 rows=1000000 | total_rows_seen=12000000\n",
      "[05B-08][SAVE] yoochoose/train shard=048 rows=250000 -> yoochoose_train_shard_048.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=049 rows=250000 -> yoochoose_train_shard_049.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=050 rows=250000 -> yoochoose_train_shard_050.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=051 rows=250000 -> yoochoose_train_shard_051.pt\n",
      "[05B-08][READ] yoochoose/train file 13/19 rows=1000000 | total_rows_seen=13000000\n",
      "[05B-08][SAVE] yoochoose/train shard=052 rows=250000 -> yoochoose_train_shard_052.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=053 rows=250000 -> yoochoose_train_shard_053.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=054 rows=250000 -> yoochoose_train_shard_054.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=055 rows=250000 -> yoochoose_train_shard_055.pt\n",
      "[05B-08][READ] yoochoose/train file 14/19 rows=1000000 | total_rows_seen=14000000\n",
      "[05B-08][SAVE] yoochoose/train shard=056 rows=250000 -> yoochoose_train_shard_056.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=057 rows=250000 -> yoochoose_train_shard_057.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=058 rows=250000 -> yoochoose_train_shard_058.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=059 rows=250000 -> yoochoose_train_shard_059.pt\n",
      "[05B-08][READ] yoochoose/train file 15/19 rows=1000000 | total_rows_seen=15000000\n",
      "[05B-08][SAVE] yoochoose/train shard=060 rows=250000 -> yoochoose_train_shard_060.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=061 rows=250000 -> yoochoose_train_shard_061.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=062 rows=250000 -> yoochoose_train_shard_062.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=063 rows=250000 -> yoochoose_train_shard_063.pt\n",
      "[05B-08][READ] yoochoose/train file 16/19 rows=1000000 | total_rows_seen=16000000\n",
      "[05B-08][SAVE] yoochoose/train shard=064 rows=250000 -> yoochoose_train_shard_064.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=065 rows=250000 -> yoochoose_train_shard_065.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=066 rows=250000 -> yoochoose_train_shard_066.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=067 rows=250000 -> yoochoose_train_shard_067.pt\n",
      "[05B-08][READ] yoochoose/train file 17/19 rows=1000000 | total_rows_seen=17000000\n",
      "[05B-08][SAVE] yoochoose/train shard=068 rows=250000 -> yoochoose_train_shard_068.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=069 rows=250000 -> yoochoose_train_shard_069.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=070 rows=250000 -> yoochoose_train_shard_070.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=071 rows=250000 -> yoochoose_train_shard_071.pt\n",
      "[05B-08][READ] yoochoose/train file 18/19 rows=1000000 | total_rows_seen=18000000\n",
      "[05B-08][SAVE] yoochoose/train shard=072 rows=250000 -> yoochoose_train_shard_072.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=073 rows=250000 -> yoochoose_train_shard_073.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=074 rows=250000 -> yoochoose_train_shard_074.pt\n",
      "[05B-08][SAVE] yoochoose/train shard=075 rows=250000 -> yoochoose_train_shard_075.pt\n",
      "[05B-08][READ] yoochoose/train file 19/19 rows=1000000 | total_rows_seen=19000000\n",
      "[05B-08][DONE] yoochoose/train total_rows=19000000 shards=76\n",
      "[05B-09] Tensorizing yoochoose/val | files=2\n",
      "[05B-08][SAVE] yoochoose/val shard=000 rows=250000 -> yoochoose_val_shard_000.pt\n",
      "[05B-08][SAVE] yoochoose/val shard=001 rows=250000 -> yoochoose_val_shard_001.pt\n",
      "[05B-08][SAVE] yoochoose/val shard=002 rows=250000 -> yoochoose_val_shard_002.pt\n",
      "[05B-08][SAVE] yoochoose/val shard=003 rows=250000 -> yoochoose_val_shard_003.pt\n",
      "[05B-08][READ] yoochoose/val file 1/2 rows=1000000 | total_rows_seen=1000000\n",
      "[05B-08][SAVE] yoochoose/val shard=004 rows=250000 -> yoochoose_val_shard_004.pt\n",
      "[05B-08][SAVE] yoochoose/val shard=005 rows=250000 -> yoochoose_val_shard_005.pt\n",
      "[05B-08][SAVE] yoochoose/val shard=006 rows=250000 -> yoochoose_val_shard_006.pt\n",
      "[05B-08][SAVE] yoochoose/val shard=007 rows=250000 -> yoochoose_val_shard_007.pt\n",
      "[05B-08][READ] yoochoose/val file 2/2 rows=1000000 | total_rows_seen=2000000\n",
      "[05B-08][DONE] yoochoose/val total_rows=2000000 shards=8\n",
      "[05B-09] Tensorizing yoochoose/test | files=3\n",
      "[05B-08][SAVE] yoochoose/test shard=000 rows=250000 -> yoochoose_test_shard_000.pt\n",
      "[05B-08][SAVE] yoochoose/test shard=001 rows=250000 -> yoochoose_test_shard_001.pt\n",
      "[05B-08][SAVE] yoochoose/test shard=002 rows=250000 -> yoochoose_test_shard_002.pt\n",
      "[05B-08][SAVE] yoochoose/test shard=003 rows=250000 -> yoochoose_test_shard_003.pt\n",
      "[05B-08][READ] yoochoose/test file 1/3 rows=1000000 | total_rows_seen=1000000\n",
      "[05B-08][SAVE] yoochoose/test shard=004 rows=250000 -> yoochoose_test_shard_004.pt\n",
      "[05B-08][SAVE] yoochoose/test shard=005 rows=250000 -> yoochoose_test_shard_005.pt\n",
      "[05B-08][SAVE] yoochoose/test shard=006 rows=250000 -> yoochoose_test_shard_006.pt\n",
      "[05B-08][SAVE] yoochoose/test shard=007 rows=250000 -> yoochoose_test_shard_007.pt\n",
      "[05B-08][READ] yoochoose/test file 2/3 rows=1000000 | total_rows_seen=2000000\n",
      "[05B-08][SAVE] yoochoose/test shard=008 rows=250000 -> yoochoose_test_shard_008.pt\n",
      "[05B-08][SAVE] yoochoose/test shard=009 rows=250000 -> yoochoose_test_shard_009.pt\n",
      "[05B-08][SAVE] yoochoose/test shard=010 rows=250000 -> yoochoose_test_shard_010.pt\n",
      "[05B-08][READ] yoochoose/test file 3/3 rows=754215 | total_rows_seen=2754215\n",
      "[05B-08][SAVE] yoochoose/test shard=011 rows=4215 -> yoochoose_test_shard_011.pt\n",
      "[05B-08][DONE] yoochoose/test total_rows=2754215 shards=12\n",
      "[05B-09] Tensorizing mars/train | files=1\n",
      "[05B-08][READ] mars/train file 1/1 rows=1744 | total_rows_seen=1744\n",
      "[05B-08][SAVE] mars/train shard=000 rows=1744 -> mars_train_shard_000.pt\n",
      "[05B-08][DONE] mars/train total_rows=1744 shards=1\n",
      "[05B-09] Tensorizing mars/val | files=1\n",
      "[05B-08][READ] mars/val file 1/1 rows=282 | total_rows_seen=282\n",
      "[05B-08][SAVE] mars/val shard=000 rows=282 -> mars_val_shard_000.pt\n",
      "[05B-08][DONE] mars/val total_rows=282 shards=1\n",
      "[05B-09] Tensorizing mars/test | files=1\n",
      "[05B-08][READ] mars/test file 1/1 rows=358 | total_rows_seen=358\n",
      "[05B-08][SAVE] mars/test shard=000 rows=358 -> mars_test_shard_000.pt\n",
      "[05B-08][DONE] mars/test total_rows=358 shards=1\n"
     ]
    }
   ],
   "source": [
    "# CELL [05B-09] — Run tensorization (ALL domains & splits)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# SOURCE domains\n",
    "for domain in [\"amazon\", \"yoochoose\"]:\n",
    "    results[domain] = {}\n",
    "    for split_name in [\"train\", \"val\", \"test\"]:\n",
    "        files = as_file_list(splits[domain][split_name])\n",
    "        print(f\"[05B-09] Tensorizing {domain}/{split_name} | files={len(files)}\")\n",
    "        results[domain][split_name] = tensorize_split(domain, split_name, files, item2id_source)\n",
    "\n",
    "# TARGET domain\n",
    "domain = \"mars\"\n",
    "results[domain] = {}\n",
    "for split_name in [\"train\", \"val\", \"test\"]:\n",
    "    files = as_file_list(splits[domain][split_name])  # each is a single parquet path\n",
    "    print(f\"[05B-09] Tensorizing {domain}/{split_name} | files={len(files)}\")\n",
    "    results[domain][split_name] = tensorize_split(domain, split_name, files, item2id_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadd699",
   "metadata": {},
   "source": [
    "Save metadata (single source of truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea88b1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05B-10] Saved metadata: ..\\data\\processed\\tensor_shards_v2\\metadata.json\n",
      "[05B-10] Summary: {'amazon': {'train': 56, 'val': 8, 'test': 6}, 'yoochoose': {'train': 76, 'val': 8, 'test': 12}, 'mars': {'train': 1, 'val': 1, 'test': 1}}\n"
     ]
    }
   ],
   "source": [
    "# CELL [05B-10] — Save metadata.json\n",
    "\n",
    "metadata = {\n",
    "    \"max_prefix_len\": MAX_PREFIX_LEN,\n",
    "    \"pad_id\": PAD_ID,\n",
    "    \"unk_id\": UNK_ID,\n",
    "    \"shard_size\": SHARD_SIZE,\n",
    "    \"vocab\": {\n",
    "        \"source\": {\n",
    "            \"path\": str(source_vocab_path),\n",
    "            \"size\": len(item2id_source)\n",
    "        },\n",
    "        \"target\": {\n",
    "            \"path\": str(target_vocab_path),\n",
    "            \"size\": len(item2id_target)\n",
    "        }\n",
    "    },\n",
    "    \"tensor_output_dir\": str(OUT_DIR),\n",
    "    \"tensor_fields\": [\"input_ids\", \"attention_mask\", \"pos_ids\", \"labels\", \"lengths\"],\n",
    "    \"results\": results\n",
    "}\n",
    "\n",
    "meta_path = OUT_DIR / \"metadata.json\"\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"[05B-10] Saved metadata:\", meta_path)\n",
    "print(\"[05B-10] Summary:\", {d: {s: results[d][s][\"shards\"] for s in results[d]} for d in results})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04651124",
   "metadata": {},
   "source": [
    "Quick sanity check (loads one shard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ef1866e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05B-11] Loaded: mars_train_shard_000.pt\n",
      "    input_ids torch.Size([1744, 20]) torch.int64\n",
      "    attention_mask torch.Size([1744, 20]) torch.int64\n",
      "    pos_ids torch.Size([1744, 20]) torch.int64\n",
      "    labels torch.Size([1744]) torch.int64\n",
      "    lengths torch.Size([1744]) torch.int64\n",
      "[05B-11] First example input_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 124]\n",
      "[05B-11] First example attention_mask: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[05B-11] First example label: 226\n"
     ]
    }
   ],
   "source": [
    "# CELL [05B-11] — Sanity check: load one shard\n",
    "\n",
    "example = OUT_DIR / \"mars_train_shard_000.pt\"\n",
    "if example.exists():\n",
    "    batch = torch.load(example)\n",
    "    print(\"[05B-11] Loaded:\", example.name)\n",
    "    for k, v in batch.items():\n",
    "        print(\"   \", k, v.shape, v.dtype)\n",
    "    print(\"[05B-11] First example input_ids:\", batch[\"input_ids\"][0].tolist())\n",
    "    print(\"[05B-11] First example attention_mask:\", batch[\"attention_mask\"][0].tolist())\n",
    "    print(\"[05B-11] First example label:\", int(batch[\"labels\"][0]))\n",
    "else:\n",
    "    print(\"[05B-11] Example shard not found yet:\", example)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
