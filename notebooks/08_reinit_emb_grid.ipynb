{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e129a0b",
   "metadata": {},
   "source": [
    "### Title & notes \n",
    "\n",
    "Purpose: Grid search over learning rates for the strategy:\n",
    "- copy pretrained encoder weights\n",
    "- re-initialize item embedding & out head\n",
    "- freeze encoder + pos_emb\n",
    "- train item_emb + out only\n",
    "\n",
    "Outputs:\n",
    "- checkpoint per lr/seed: ../models/reinit_emb_lr{lr}_s{seed}.pt\n",
    "- CSV summary: ../models/reinit_emb_grid_results.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d52fc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\n"
     ]
    }
   ],
   "source": [
    "# Quick (unsafe) workaround to avoid the libiomp5md.dll crash.\n",
    "# Use this only to continue working in the notebook quickly.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "print(\"Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8a234",
   "metadata": {},
   "source": [
    "### Imports & global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8be39e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "PRETRAIN: ..\\models\\sasrec_full_top200000_epoch0.pt_epoch0.pt\n"
     ]
    }
   ],
   "source": [
    "import json, time, random, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ROOT = Path(\"..\")\n",
    "DATA_DIR = ROOT/\"data\"/\"processed\"\n",
    "CKPT_DIR = ROOT/\"models\"\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "MARS_SHARD = DATA_DIR/\"mars_shards\"/\"mars_shard_full.pt\"\n",
    "VOCAB_FILE = DATA_DIR/\"vocab_mars\"/\"item2id_mars.json\"\n",
    "PRETRAIN = sorted(CKPT_DIR.glob(\"*full*.pt\"), key=lambda p:p.stat().st_mtime, reverse=True)\n",
    "PRETRAIN = PRETRAIN[0] if PRETRAIN else None\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", DEVICE)\n",
    "print(\"PRETRAIN:\", PRETRAIN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecdb92d",
   "metadata": {},
   "source": [
    "### Model definition & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a34d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SASRecSmall(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, max_len=20):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=2048, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=2)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        B,L = x.size()\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0).expand(B,L)\n",
    "        seq = self.item_emb(x) + self.pos_emb(pos)\n",
    "        seq = self.encoder(seq)\n",
    "        last = seq[:,-1,:]\n",
    "        logits = self.out(last)\n",
    "        return logits, last\n",
    "\n",
    "def sampled_loss(final, y, emb, neg=32):\n",
    "    pos = (final * emb[y]).sum(dim=1)\n",
    "    V = emb.size(0); B = final.size(0)\n",
    "    neg_idx = torch.randint(0, V, (B, neg), device=final.device)\n",
    "    negW = emb[neg_idx]\n",
    "    neg_scores = (negW * final.unsqueeze(1)).sum(dim=2)\n",
    "    logits = torch.cat([pos.unsqueeze(1), neg_scores], dim=1)\n",
    "    labels = torch.zeros(B, dtype=torch.long, device=final.device)\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18ae92",
   "metadata": {},
   "source": [
    "### Load data shard & splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5fc26ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs: 2380 train: 1904 val: 476 vocab: 777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43824\\4048124871.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mp = torch.load(MARS_SHARD)\n"
     ]
    }
   ],
   "source": [
    "mp = torch.load(MARS_SHARD)\n",
    "P_all = mp['prefix']; T_all = mp['target']\n",
    "N = P_all.size(0)\n",
    "VAL_FRAC = 0.2\n",
    "val_n = max(1, int(N * VAL_FRAC)); train_n = N - val_n\n",
    "train_P, train_T = P_all[:train_n], T_all[:train_n]\n",
    "val_P, val_T = P_all[train_n:], T_all[train_n:]\n",
    "vocab = len(json.load(open(VOCAB_FILE)))\n",
    "print(\"pairs:\", N, \"train:\", train_n, \"val:\", val_n, \"vocab:\", vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ae9d4",
   "metadata": {},
   "source": [
    "### Training loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db03398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PairsDataset(Dataset):\n",
    "    def __init__(self,P,T): self.P=P; self.T=T\n",
    "    def __len__(self): return self.P.size(0)\n",
    "    def __getitem__(self,i): return self.P[i], int(self.T[i].item())\n",
    "\n",
    "def validate_model(model, val_P, val_T, K=20):\n",
    "    model.eval()\n",
    "    hits=0; rr=0.0; tot=val_P.size(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(tot):\n",
    "            X = val_P[i].unsqueeze(0).to(DEVICE)\n",
    "            tgt = int(val_T[i].item())\n",
    "            _, final = model(X)\n",
    "            scores = final @ model.item_emb.weight.t()\n",
    "            topk = scores.topk(K, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            if tgt in topk:\n",
    "                hits += 1\n",
    "                rank = int((topk==tgt).nonzero()[0]) + 1\n",
    "                rr += 1.0 / rank\n",
    "    return hits / tot, rr / tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af477e6",
   "metadata": {},
   "source": [
    "### Grid run (LRs & seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "479c9369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43824\\4070722550.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(PRETRAIN, map_location=DEVICE)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43824\\1937983581.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rank = int((topk==tgt).nonzero()[0]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.01 seed=42 ep=0 loss=3.2958 val_rec=0.0735 val_mrr=0.0153\n",
      "lr=0.01 seed=42 ep=1 loss=2.8451 val_rec=0.0798 val_mrr=0.0298\n",
      "lr=0.01 seed=42 ep=2 loss=2.6776 val_rec=0.0861 val_mrr=0.0275\n",
      "lr=0.01 seed=42 ep=3 loss=2.5633 val_rec=0.1008 val_mrr=0.0386\n",
      "lr=0.01 seed=42 ep=4 loss=2.4984 val_rec=0.1408 val_mrr=0.0362\n",
      "lr=0.01 seed=42 ep=5 loss=2.4146 val_rec=0.1282 val_mrr=0.0409\n",
      "lr=0.01 seed=42 ep=6 loss=2.3329 val_rec=0.1534 val_mrr=0.0398\n",
      "lr=0.01 seed=42 ep=7 loss=2.2555 val_rec=0.1723 val_mrr=0.0465\n",
      "lr=0.01 seed=42 ep=8 loss=2.1906 val_rec=0.2017 val_mrr=0.0617\n",
      "lr=0.01 seed=42 ep=9 loss=2.1104 val_rec=0.2164 val_mrr=0.0652\n",
      "lr=0.01 seed=42 ep=10 loss=2.0385 val_rec=0.2185 val_mrr=0.0692\n",
      "lr=0.01 seed=42 ep=11 loss=1.9612 val_rec=0.2269 val_mrr=0.0770\n",
      "lr=0.01 seed=42 ep=12 loss=1.9119 val_rec=0.2542 val_mrr=0.0837\n",
      "lr=0.01 seed=42 ep=13 loss=1.8273 val_rec=0.2857 val_mrr=0.0976\n",
      "lr=0.01 seed=42 ep=14 loss=1.7673 val_rec=0.3046 val_mrr=0.1126\n",
      "lr=0.01 seed=42 ep=15 loss=1.7035 val_rec=0.3256 val_mrr=0.1280\n",
      "lr=0.01 seed=42 ep=16 loss=1.6361 val_rec=0.3466 val_mrr=0.1353\n",
      "lr=0.01 seed=42 ep=17 loss=1.5742 val_rec=0.3634 val_mrr=0.1578\n",
      "lr=0.01 seed=42 ep=18 loss=1.5064 val_rec=0.3971 val_mrr=0.1765\n",
      "lr=0.01 seed=42 ep=19 loss=1.4389 val_rec=0.4181 val_mrr=0.1807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43824\\4070722550.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(PRETRAIN, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.01 seed=100 ep=0 loss=3.2997 val_rec=0.0609 val_mrr=0.0154\n",
      "lr=0.01 seed=100 ep=1 loss=2.8402 val_rec=0.0609 val_mrr=0.0213\n",
      "lr=0.01 seed=100 ep=2 loss=2.6531 val_rec=0.0672 val_mrr=0.0262\n",
      "lr=0.01 seed=100 ep=3 loss=2.5777 val_rec=0.0903 val_mrr=0.0295\n",
      "lr=0.01 seed=100 ep=4 loss=2.4926 val_rec=0.1050 val_mrr=0.0278\n",
      "lr=0.01 seed=100 ep=5 loss=2.4168 val_rec=0.1092 val_mrr=0.0363\n",
      "lr=0.01 seed=100 ep=6 loss=2.3370 val_rec=0.1387 val_mrr=0.0421\n",
      "lr=0.01 seed=100 ep=7 loss=2.2764 val_rec=0.1681 val_mrr=0.0509\n",
      "lr=0.01 seed=100 ep=8 loss=2.2088 val_rec=0.1786 val_mrr=0.0544\n",
      "lr=0.01 seed=100 ep=9 loss=2.1343 val_rec=0.1807 val_mrr=0.0602\n",
      "lr=0.01 seed=100 ep=10 loss=2.0601 val_rec=0.1912 val_mrr=0.0641\n",
      "lr=0.01 seed=100 ep=11 loss=1.9748 val_rec=0.2164 val_mrr=0.0757\n",
      "lr=0.01 seed=100 ep=12 loss=1.9295 val_rec=0.2395 val_mrr=0.0829\n",
      "lr=0.01 seed=100 ep=13 loss=1.8474 val_rec=0.2605 val_mrr=0.0925\n",
      "lr=0.01 seed=100 ep=14 loss=1.7763 val_rec=0.2773 val_mrr=0.0985\n",
      "lr=0.01 seed=100 ep=15 loss=1.7019 val_rec=0.3025 val_mrr=0.1069\n",
      "lr=0.01 seed=100 ep=16 loss=1.6362 val_rec=0.3361 val_mrr=0.1262\n",
      "lr=0.01 seed=100 ep=17 loss=1.5634 val_rec=0.3761 val_mrr=0.1375\n",
      "lr=0.01 seed=100 ep=18 loss=1.5063 val_rec=0.3887 val_mrr=0.1520\n",
      "lr=0.01 seed=100 ep=19 loss=1.4339 val_rec=0.4013 val_mrr=0.1566\n",
      "lr=0.01 seed=2023 ep=0 loss=3.3041 val_rec=0.0546 val_mrr=0.0165\n",
      "lr=0.01 seed=2023 ep=1 loss=2.8384 val_rec=0.0672 val_mrr=0.0249\n",
      "lr=0.01 seed=2023 ep=2 loss=2.6633 val_rec=0.0651 val_mrr=0.0262\n",
      "lr=0.01 seed=2023 ep=3 loss=2.5723 val_rec=0.0798 val_mrr=0.0266\n",
      "lr=0.01 seed=2023 ep=4 loss=2.4905 val_rec=0.0903 val_mrr=0.0323\n",
      "lr=0.01 seed=2023 ep=5 loss=2.4259 val_rec=0.1176 val_mrr=0.0351\n",
      "lr=0.01 seed=2023 ep=6 loss=2.3528 val_rec=0.1239 val_mrr=0.0394\n",
      "lr=0.01 seed=2023 ep=7 loss=2.3051 val_rec=0.1429 val_mrr=0.0418\n",
      "lr=0.01 seed=2023 ep=8 loss=2.2033 val_rec=0.1702 val_mrr=0.0473\n",
      "lr=0.01 seed=2023 ep=9 loss=2.1478 val_rec=0.1849 val_mrr=0.0552\n",
      "lr=0.01 seed=2023 ep=10 loss=2.0731 val_rec=0.1975 val_mrr=0.0588\n",
      "lr=0.01 seed=2023 ep=11 loss=2.0120 val_rec=0.2185 val_mrr=0.0666\n",
      "lr=0.01 seed=2023 ep=12 loss=1.9418 val_rec=0.2227 val_mrr=0.0756\n",
      "lr=0.01 seed=2023 ep=13 loss=1.8739 val_rec=0.2395 val_mrr=0.0862\n",
      "lr=0.01 seed=2023 ep=14 loss=1.7953 val_rec=0.2836 val_mrr=0.0965\n",
      "lr=0.01 seed=2023 ep=15 loss=1.7519 val_rec=0.3004 val_mrr=0.1044\n",
      "lr=0.01 seed=2023 ep=16 loss=1.6491 val_rec=0.3235 val_mrr=0.1097\n",
      "lr=0.01 seed=2023 ep=17 loss=1.6073 val_rec=0.3424 val_mrr=0.1219\n",
      "lr=0.01 seed=2023 ep=18 loss=1.5242 val_rec=0.3718 val_mrr=0.1369\n",
      "lr=0.01 seed=2023 ep=19 loss=1.4580 val_rec=0.3929 val_mrr=0.1519\n",
      "lr=0.005 seed=42 ep=0 loss=3.3810 val_rec=0.0672 val_mrr=0.0178\n",
      "lr=0.005 seed=42 ep=1 loss=3.0626 val_rec=0.0798 val_mrr=0.0180\n",
      "lr=0.005 seed=42 ep=2 loss=2.8484 val_rec=0.0630 val_mrr=0.0235\n",
      "lr=0.005 seed=42 ep=3 loss=2.7212 val_rec=0.0630 val_mrr=0.0244\n",
      "lr=0.005 seed=42 ep=4 loss=2.6662 val_rec=0.0714 val_mrr=0.0261\n",
      "lr=0.005 seed=42 ep=5 loss=2.6073 val_rec=0.0819 val_mrr=0.0287\n",
      "lr=0.005 seed=42 ep=6 loss=2.5518 val_rec=0.0882 val_mrr=0.0281\n",
      "lr=0.005 seed=42 ep=7 loss=2.5100 val_rec=0.0924 val_mrr=0.0295\n",
      "lr=0.005 seed=42 ep=8 loss=2.4823 val_rec=0.1008 val_mrr=0.0349\n",
      "lr=0.005 seed=42 ep=9 loss=2.4358 val_rec=0.1176 val_mrr=0.0329\n",
      "lr=0.005 seed=42 ep=10 loss=2.3954 val_rec=0.1282 val_mrr=0.0347\n",
      "lr=0.005 seed=42 ep=11 loss=2.3568 val_rec=0.1197 val_mrr=0.0369\n",
      "lr=0.005 seed=42 ep=12 loss=2.3311 val_rec=0.1513 val_mrr=0.0375\n",
      "lr=0.005 seed=42 ep=13 loss=2.2831 val_rec=0.1492 val_mrr=0.0446\n",
      "lr=0.005 seed=42 ep=14 loss=2.2505 val_rec=0.1555 val_mrr=0.0471\n",
      "lr=0.005 seed=42 ep=15 loss=2.2225 val_rec=0.1555 val_mrr=0.0473\n",
      "lr=0.005 seed=42 ep=16 loss=2.1916 val_rec=0.1534 val_mrr=0.0528\n",
      "lr=0.005 seed=42 ep=17 loss=2.1568 val_rec=0.1765 val_mrr=0.0570\n",
      "lr=0.005 seed=42 ep=18 loss=2.1192 val_rec=0.1870 val_mrr=0.0611\n",
      "lr=0.005 seed=42 ep=19 loss=2.0773 val_rec=0.1870 val_mrr=0.0656\n",
      "lr=0.005 seed=100 ep=0 loss=3.3800 val_rec=0.0651 val_mrr=0.0166\n",
      "lr=0.005 seed=100 ep=1 loss=3.0586 val_rec=0.0546 val_mrr=0.0178\n",
      "lr=0.005 seed=100 ep=2 loss=2.8266 val_rec=0.0630 val_mrr=0.0206\n",
      "lr=0.005 seed=100 ep=3 loss=2.7246 val_rec=0.0651 val_mrr=0.0252\n",
      "lr=0.005 seed=100 ep=4 loss=2.6568 val_rec=0.0798 val_mrr=0.0247\n",
      "lr=0.005 seed=100 ep=5 loss=2.6062 val_rec=0.0693 val_mrr=0.0264\n",
      "lr=0.005 seed=100 ep=6 loss=2.5476 val_rec=0.0714 val_mrr=0.0327\n",
      "lr=0.005 seed=100 ep=7 loss=2.5159 val_rec=0.0840 val_mrr=0.0324\n",
      "lr=0.005 seed=100 ep=8 loss=2.4828 val_rec=0.0819 val_mrr=0.0325\n",
      "lr=0.005 seed=100 ep=9 loss=2.4474 val_rec=0.0966 val_mrr=0.0364\n",
      "lr=0.005 seed=100 ep=10 loss=2.4085 val_rec=0.1050 val_mrr=0.0369\n",
      "lr=0.005 seed=100 ep=11 loss=2.3555 val_rec=0.1197 val_mrr=0.0380\n",
      "lr=0.005 seed=100 ep=12 loss=2.3451 val_rec=0.1218 val_mrr=0.0402\n",
      "lr=0.005 seed=100 ep=13 loss=2.2938 val_rec=0.1366 val_mrr=0.0431\n",
      "lr=0.005 seed=100 ep=14 loss=2.2648 val_rec=0.1366 val_mrr=0.0409\n",
      "lr=0.005 seed=100 ep=15 loss=2.2229 val_rec=0.1555 val_mrr=0.0476\n",
      "lr=0.005 seed=100 ep=16 loss=2.1857 val_rec=0.1471 val_mrr=0.0499\n",
      "lr=0.005 seed=100 ep=17 loss=2.1592 val_rec=0.1702 val_mrr=0.0493\n",
      "lr=0.005 seed=100 ep=18 loss=2.1214 val_rec=0.1702 val_mrr=0.0536\n",
      "lr=0.005 seed=100 ep=19 loss=2.0798 val_rec=0.1765 val_mrr=0.0574\n",
      "lr=0.005 seed=2023 ep=0 loss=3.3860 val_rec=0.0630 val_mrr=0.0212\n",
      "lr=0.005 seed=2023 ep=1 loss=3.0599 val_rec=0.0525 val_mrr=0.0168\n",
      "lr=0.005 seed=2023 ep=2 loss=2.8353 val_rec=0.0567 val_mrr=0.0236\n",
      "lr=0.005 seed=2023 ep=3 loss=2.7216 val_rec=0.0609 val_mrr=0.0240\n",
      "lr=0.005 seed=2023 ep=4 loss=2.6434 val_rec=0.0714 val_mrr=0.0299\n",
      "lr=0.005 seed=2023 ep=5 loss=2.5915 val_rec=0.0756 val_mrr=0.0271\n",
      "lr=0.005 seed=2023 ep=6 loss=2.5466 val_rec=0.0840 val_mrr=0.0288\n",
      "lr=0.005 seed=2023 ep=7 loss=2.5306 val_rec=0.0798 val_mrr=0.0332\n",
      "lr=0.005 seed=2023 ep=8 loss=2.4643 val_rec=0.0966 val_mrr=0.0301\n",
      "lr=0.005 seed=2023 ep=9 loss=2.4451 val_rec=0.1113 val_mrr=0.0334\n",
      "lr=0.005 seed=2023 ep=10 loss=2.4040 val_rec=0.1134 val_mrr=0.0393\n",
      "lr=0.005 seed=2023 ep=11 loss=2.3693 val_rec=0.1197 val_mrr=0.0406\n",
      "lr=0.005 seed=2023 ep=12 loss=2.3372 val_rec=0.1282 val_mrr=0.0391\n",
      "lr=0.005 seed=2023 ep=13 loss=2.3095 val_rec=0.1408 val_mrr=0.0417\n",
      "lr=0.005 seed=2023 ep=14 loss=2.2654 val_rec=0.1513 val_mrr=0.0436\n",
      "lr=0.005 seed=2023 ep=15 loss=2.2500 val_rec=0.1660 val_mrr=0.0439\n",
      "lr=0.005 seed=2023 ep=16 loss=2.1879 val_rec=0.1660 val_mrr=0.0478\n",
      "lr=0.005 seed=2023 ep=17 loss=2.1690 val_rec=0.1723 val_mrr=0.0507\n",
      "lr=0.005 seed=2023 ep=18 loss=2.1242 val_rec=0.1723 val_mrr=0.0560\n",
      "lr=0.005 seed=2023 ep=19 loss=2.0896 val_rec=0.1828 val_mrr=0.0588\n",
      "lr=0.001 seed=42 ep=0 loss=3.4678 val_rec=0.0693 val_mrr=0.0152\n",
      "lr=0.001 seed=42 ep=1 loss=3.3844 val_rec=0.0693 val_mrr=0.0201\n",
      "lr=0.001 seed=42 ep=2 loss=3.3207 val_rec=0.0693 val_mrr=0.0214\n",
      "lr=0.001 seed=42 ep=3 loss=3.2572 val_rec=0.0693 val_mrr=0.0219\n",
      "lr=0.001 seed=42 ep=4 loss=3.2006 val_rec=0.0567 val_mrr=0.0216\n",
      "lr=0.001 seed=42 ep=5 loss=3.1405 val_rec=0.0777 val_mrr=0.0186\n",
      "lr=0.001 seed=42 ep=6 loss=3.0738 val_rec=0.0693 val_mrr=0.0173\n",
      "lr=0.001 seed=42 ep=7 loss=3.0165 val_rec=0.0693 val_mrr=0.0187\n",
      "lr=0.001 seed=42 ep=8 loss=2.9675 val_rec=0.0651 val_mrr=0.0211\n",
      "lr=0.001 seed=42 ep=9 loss=2.9157 val_rec=0.0630 val_mrr=0.0210\n",
      "lr=0.001 seed=42 ep=10 loss=2.8698 val_rec=0.0525 val_mrr=0.0229\n",
      "lr=0.001 seed=100 ep=0 loss=3.4672 val_rec=0.0567 val_mrr=0.0200\n",
      "lr=0.001 seed=100 ep=1 loss=3.3836 val_rec=0.0546 val_mrr=0.0196\n",
      "lr=0.001 seed=100 ep=2 loss=3.3171 val_rec=0.0588 val_mrr=0.0199\n",
      "lr=0.001 seed=100 ep=3 loss=3.2587 val_rec=0.0588 val_mrr=0.0171\n",
      "lr=0.001 seed=100 ep=4 loss=3.1951 val_rec=0.0630 val_mrr=0.0150\n",
      "lr=0.001 seed=100 ep=5 loss=3.1328 val_rec=0.0651 val_mrr=0.0176\n",
      "lr=0.001 seed=100 ep=6 loss=3.0679 val_rec=0.0714 val_mrr=0.0173\n",
      "lr=0.001 seed=100 ep=7 loss=3.0131 val_rec=0.0693 val_mrr=0.0193\n",
      "lr=0.001 seed=100 ep=8 loss=2.9597 val_rec=0.0630 val_mrr=0.0188\n",
      "lr=0.001 seed=100 ep=9 loss=2.9073 val_rec=0.0630 val_mrr=0.0214\n",
      "lr=0.001 seed=100 ep=10 loss=2.8678 val_rec=0.0546 val_mrr=0.0235\n",
      "lr=0.001 seed=100 ep=11 loss=2.8265 val_rec=0.0567 val_mrr=0.0248\n",
      "lr=0.001 seed=2023 ep=0 loss=3.4693 val_rec=0.0567 val_mrr=0.0195\n",
      "lr=0.001 seed=2023 ep=1 loss=3.3854 val_rec=0.0588 val_mrr=0.0194\n",
      "lr=0.001 seed=2023 ep=2 loss=3.3190 val_rec=0.0609 val_mrr=0.0206\n",
      "lr=0.001 seed=2023 ep=3 loss=3.2588 val_rec=0.0630 val_mrr=0.0215\n",
      "lr=0.001 seed=2023 ep=4 loss=3.1968 val_rec=0.0609 val_mrr=0.0154\n",
      "lr=0.001 seed=2023 ep=5 loss=3.1321 val_rec=0.0567 val_mrr=0.0156\n",
      "lr=0.001 seed=2023 ep=6 loss=3.0730 val_rec=0.0651 val_mrr=0.0166\n",
      "lr=0.001 seed=2023 ep=7 loss=3.0204 val_rec=0.0630 val_mrr=0.0175\n",
      "lr=0.001 seed=2023 ep=8 loss=2.9557 val_rec=0.0630 val_mrr=0.0208\n",
      "lr=0.001 seed=2023 ep=9 loss=2.9127 val_rec=0.0525 val_mrr=0.0205\n",
      "lr=0.001 seed=2023 ep=10 loss=2.8665 val_rec=0.0546 val_mrr=0.0222\n",
      "lr=0.001 seed=2023 ep=11 loss=2.8240 val_rec=0.0567 val_mrr=0.0230\n",
      "lr=0.0005 seed=42 ep=0 loss=3.4816 val_rec=0.0693 val_mrr=0.0150\n",
      "lr=0.0005 seed=42 ep=1 loss=3.4364 val_rec=0.0714 val_mrr=0.0198\n",
      "lr=0.0005 seed=42 ep=2 loss=3.4001 val_rec=0.0693 val_mrr=0.0202\n",
      "lr=0.0005 seed=42 ep=3 loss=3.3648 val_rec=0.0693 val_mrr=0.0202\n",
      "lr=0.0005 seed=42 ep=4 loss=3.3350 val_rec=0.0693 val_mrr=0.0203\n",
      "lr=0.0005 seed=42 ep=5 loss=3.3053 val_rec=0.0651 val_mrr=0.0202\n",
      "lr=0.0005 seed=42 ep=6 loss=3.2723 val_rec=0.0651 val_mrr=0.0204\n",
      "lr=0.0005 seed=100 ep=0 loss=3.4815 val_rec=0.0588 val_mrr=0.0195\n",
      "lr=0.0005 seed=100 ep=1 loss=3.4362 val_rec=0.0546 val_mrr=0.0197\n",
      "lr=0.0005 seed=100 ep=2 loss=3.3986 val_rec=0.0567 val_mrr=0.0195\n",
      "lr=0.0005 seed=100 ep=3 loss=3.3663 val_rec=0.0546 val_mrr=0.0204\n",
      "lr=0.0005 seed=100 ep=4 loss=3.3330 val_rec=0.0546 val_mrr=0.0206\n",
      "lr=0.0005 seed=100 ep=5 loss=3.3021 val_rec=0.0546 val_mrr=0.0207\n",
      "lr=0.0005 seed=2023 ep=0 loss=3.4825 val_rec=0.0546 val_mrr=0.0190\n",
      "lr=0.0005 seed=2023 ep=1 loss=3.4373 val_rec=0.0567 val_mrr=0.0194\n",
      "lr=0.0005 seed=2023 ep=2 loss=3.3997 val_rec=0.0567 val_mrr=0.0201\n",
      "lr=0.0005 seed=2023 ep=3 loss=3.3659 val_rec=0.0588 val_mrr=0.0202\n",
      "lr=0.0005 seed=2023 ep=4 loss=3.3338 val_rec=0.0546 val_mrr=0.0193\n",
      "lr=0.0005 seed=2023 ep=5 loss=3.3015 val_rec=0.0588 val_mrr=0.0202\n",
      "lr=0.0005 seed=2023 ep=6 loss=3.2726 val_rec=0.0567 val_mrr=0.0223\n",
      "lr=0.0005 seed=2023 ep=7 loss=3.2441 val_rec=0.0651 val_mrr=0.0224\n",
      "lr=0.0005 seed=2023 ep=8 loss=3.2102 val_rec=0.0630 val_mrr=0.0214\n",
      "lr=0.0005 seed=2023 ep=9 loss=3.1807 val_rec=0.0630 val_mrr=0.0174\n",
      "lr=0.0005 seed=2023 ep=10 loss=3.1467 val_rec=0.0546 val_mrr=0.0154\n",
      "lr=0.0005 seed=2023 ep=11 loss=3.1128 val_rec=0.0567 val_mrr=0.0157\n",
      "lr=0.0005 seed=2023 ep=12 loss=3.0848 val_rec=0.0567 val_mrr=0.0160\n",
      "lr=0.0001 seed=42 ep=0 loss=3.4934 val_rec=0.0609 val_mrr=0.0127\n",
      "lr=0.0001 seed=42 ep=1 loss=3.4837 val_rec=0.0693 val_mrr=0.0152\n",
      "lr=0.0001 seed=42 ep=2 loss=3.4753 val_rec=0.0588 val_mrr=0.0145\n",
      "lr=0.0001 seed=42 ep=3 loss=3.4665 val_rec=0.0714 val_mrr=0.0195\n",
      "lr=0.0001 seed=42 ep=4 loss=3.4589 val_rec=0.0630 val_mrr=0.0196\n",
      "lr=0.0001 seed=42 ep=5 loss=3.4511 val_rec=0.0693 val_mrr=0.0199\n",
      "lr=0.0001 seed=42 ep=6 loss=3.4428 val_rec=0.0714 val_mrr=0.0200\n",
      "lr=0.0001 seed=42 ep=7 loss=3.4357 val_rec=0.0714 val_mrr=0.0199\n",
      "lr=0.0001 seed=42 ep=8 loss=3.4289 val_rec=0.0714 val_mrr=0.0201\n",
      "lr=0.0001 seed=100 ep=0 loss=3.4937 val_rec=0.0672 val_mrr=0.0133\n",
      "lr=0.0001 seed=100 ep=1 loss=3.4839 val_rec=0.0588 val_mrr=0.0128\n",
      "lr=0.0001 seed=100 ep=2 loss=3.4752 val_rec=0.0630 val_mrr=0.0137\n",
      "lr=0.0001 seed=100 ep=3 loss=3.4673 val_rec=0.0588 val_mrr=0.0158\n",
      "lr=0.0001 seed=100 ep=4 loss=3.4589 val_rec=0.0546 val_mrr=0.0199\n",
      "lr=0.0001 seed=100 ep=5 loss=3.4510 val_rec=0.0546 val_mrr=0.0196\n",
      "lr=0.0001 seed=2023 ep=0 loss=3.4938 val_rec=0.0483 val_mrr=0.0145\n",
      "lr=0.0001 seed=2023 ep=1 loss=3.4842 val_rec=0.0588 val_mrr=0.0125\n",
      "lr=0.0001 seed=2023 ep=2 loss=3.4754 val_rec=0.0672 val_mrr=0.0202\n",
      "lr=0.0001 seed=2023 ep=3 loss=3.4672 val_rec=0.0672 val_mrr=0.0207\n",
      "lr=0.0001 seed=2023 ep=4 loss=3.4591 val_rec=0.0609 val_mrr=0.0197\n",
      "lr=0.0001 seed=2023 ep=5 loss=3.4509 val_rec=0.0609 val_mrr=0.0198\n",
      "lr=0.0001 seed=2023 ep=6 loss=3.4436 val_rec=0.0609 val_mrr=0.0197\n",
      "lr=0.0001 seed=2023 ep=7 loss=3.4363 val_rec=0.0609 val_mrr=0.0196\n",
      "Grid done. Saved results to ..\\models\\reinit_emb_grid_results.csv\n"
     ]
    }
   ],
   "source": [
    "LRs=[1e-2,5e-3,1e-3,5e-4,1e-4]\n",
    "SEEDS=[42,100,2023]\n",
    "EPOCHS=20; BATCH=32\n",
    "results=[]\n",
    "\n",
    "for lr in LRs:\n",
    "    for seed in SEEDS:\n",
    "        torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
    "        model = SASRecSmall(vocab).to(DEVICE)\n",
    "        if PRETRAIN:\n",
    "            ck = torch.load(PRETRAIN, map_location=DEVICE)\n",
    "            st = ck.get('model_state', ck)\n",
    "            ms = model.state_dict()\n",
    "            for k,v in st.items():\n",
    "                if k in ms and ms[k].shape == v.shape:\n",
    "                    ms[k] = v\n",
    "            model.load_state_dict(ms)\n",
    "        # reinit item emb + out\n",
    "        nn.init.normal_(model.item_emb.weight, mean=0.0, std=0.01)\n",
    "        nn.init.normal_(model.out.weight, mean=0.0, std=0.01)\n",
    "        # freeze encoder + pos_emb\n",
    "        for name,p in model.named_parameters():\n",
    "            if name.startswith('encoder') or name.startswith('pos_emb'):\n",
    "                p.requires_grad = False\n",
    "            else:\n",
    "                p.requires_grad = True\n",
    "        opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-6)\n",
    "        loader = DataLoader(PairsDataset(train_P, train_T), batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "        best_val=-1; best_state=None; patience=0\n",
    "        for ep in range(EPOCHS):\n",
    "            model.train(); running=0.0; steps=0\n",
    "            for Xidx, yidx in loader:\n",
    "                X = Xidx.to(DEVICE); y = yidx.to(DEVICE)\n",
    "                _, final = model(X)\n",
    "                loss = sampled_loss(final, y, model.item_emb.weight, neg=32)\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "                running += float(loss.item()); steps += 1\n",
    "            val_rec, val_mrr = validate_model(model, val_P, val_T, K=20)\n",
    "            print(f\"lr={lr} seed={seed} ep={ep} loss={running/max(1,steps):.4f} val_rec={val_rec:.4f} val_mrr={val_mrr:.4f}\")\n",
    "            if val_rec > best_val:\n",
    "                best_val = val_rec; best_state = deepcopy(model.state_dict()); patience=0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= 5:\n",
    "                    break\n",
    "        out_path = CKPT_DIR/f\"reinit_emb_lr{lr}_s{seed}.pt\"\n",
    "        torch.save({'model_state': best_state}, out_path)\n",
    "        results.append({'lr':lr,'seed':seed,'val_rec':best_val})\n",
    "import pandas as pd\n",
    "pd.DataFrame(results).to_csv(CKPT_DIR/'reinit_emb_grid_results.csv', index=False)\n",
    "print(\"Grid done. Saved results to\", CKPT_DIR/'reinit_emb_grid_results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
