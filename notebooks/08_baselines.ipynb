{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b411e6ad",
   "metadata": {},
   "source": [
    "### Baseline 1 \u2014 Popularity recommender (quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882faed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set KMP_DUPLICATE_LIB_OK=TRUE \u2014 restart kernel and re-run cells now.\n"
     ]
    }
   ],
   "source": [
    "# Quick (unsafe) workaround to avoid the libiomp5md.dll crash.\n",
    "# Use this only to continue working in the notebook quickly.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "print(\"Set KMP_DUPLICATE_LIB_OK=TRUE \u2014 restart kernel and re-run cells now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaaa1f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "SASRecSmall loaded for baselines.\n"
     ]
    }
   ],
   "source": [
    "# Baselines Notebook \u2013 Required imports + SASRecSmall\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---- SASRecSmall definition (same architecture as your pretraining) -----\n",
    "class SASRecSmall(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, max_len=20,\n",
    "                 num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.item_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # output head: maps embedding -> embedding\n",
    "        self.out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.size()\n",
    "        pos_ids = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        seq = self.item_emb(x) + self.pos_emb(pos_ids)\n",
    "        seq = self.encoder(seq)\n",
    "        last = seq[:, -1, :]            # (B, D)\n",
    "        logits = self.out(last)         # (B, D)\n",
    "        return logits, last\n",
    "\n",
    "print(\"SASRecSmall loaded for baselines.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f339eb0",
   "metadata": {},
   "source": [
    "### Baseline 1 \u2014 popularity top-K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff773321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: popularity top-K\n",
    "from collections import Counter\n",
    "\n",
    "# build popularity from training pairs (mp is your MARS shard)\n",
    "mp = torch.load(\"../data/processed/mars_shards/mars_shard_full.pt\")\n",
    "# compute item counts from train split (we used last 20% as val earlier); adjust if different\n",
    "num_pairs = mp['prefix'].size(0)\n",
    "train_n = int(num_pairs * 0.8)\n",
    "train_targets = mp['target'][:train_n].tolist()\n",
    "pop_counts = Counter(train_targets)\n",
    "topk = [item for item,_ in pop_counts.most_common(100)]\n",
    "\n",
    "def eval_pop(K=20, df_test=None):\n",
    "    # if df_test not given create from shard last 10% or you can pass test df\n",
    "    if df_test is None:\n",
    "        df_test = ... # load ../data/processed/mars_test_pairs.parquet\n",
    "    hits = 0; rr = 0.0; total = 0\n",
    "    for _, r in df_test.iterrows():\n",
    "        target = int(r['target'])\n",
    "        total += 1\n",
    "        if target in topk[:K]:\n",
    "            hits += 1\n",
    "            rank = topk.index(target) + 1\n",
    "            rr += 1.0 / rank\n",
    "    return hits/total, rr/total\n",
    "\n",
    "# usage\n",
    "df_test = pd.read_parquet(\"../data/processed/mars_test_pairs.parquet\")\n",
    "print(\"Popularity Recall@20, MRR:\", eval_pop(20, df_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac7a24",
   "metadata": {},
   "source": [
    "### Baseline 2 \u2014 SASRec trained from scratch on MARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b470eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\1747533834.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mp = torch.load(MARS_SHARD)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SASRec FROM SCRATCH on MARS...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\1747533834.py:145: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rank = int((topk == y).nonzero()[0]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_loss=15.3335, val_recall@20=0.0273, mrr=0.0062, time=1.5s\n",
      "[Epoch 1] train_loss=11.6927, val_recall@20=0.0525, mrr=0.0065, time=1.3s\n",
      "[Epoch 2] train_loss=10.4094, val_recall@20=0.0588, mrr=0.0083, time=1.3s\n",
      "[Epoch 3] train_loss=9.6660, val_recall@20=0.0693, mrr=0.0106, time=1.2s\n",
      "[Epoch 4] train_loss=8.9242, val_recall@20=0.0966, mrr=0.0185, time=1.2s\n",
      "[Epoch 5] train_loss=8.4433, val_recall@20=0.1239, mrr=0.0206, time=1.2s\n",
      "[Epoch 6] train_loss=7.8064, val_recall@20=0.1660, mrr=0.0359, time=1.2s\n",
      "[Epoch 7] train_loss=7.2916, val_recall@20=0.1849, mrr=0.0495, time=1.2s\n",
      "[Epoch 8] train_loss=6.7840, val_recall@20=0.2143, mrr=0.0650, time=1.4s\n",
      "[Epoch 9] train_loss=6.3793, val_recall@20=0.2311, mrr=0.0663, time=1.3s\n",
      "\n",
      "Baseline 2 training complete.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Baseline 2 \u2014 SASRec From Scratch on MARS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import json\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =========================\n",
    "# 1) Model Definition\n",
    "# =========================\n",
    "class SASRecSmall(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, max_len=20,\n",
    "                 num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.item_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.size()\n",
    "        pos_ids = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "\n",
    "        seq = self.item_emb(x) + self.pos_emb(pos_ids)\n",
    "        seq = self.encoder(seq)\n",
    "\n",
    "        last = seq[:, -1, :]                # (B, D)\n",
    "        logits = self.out(last)             # (B, D)\n",
    "        return logits, last\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Load vocab + prefix-target pairs\n",
    "# =========================\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "VOCAB_FILE = DATA_DIR / \"vocab_mars\" / \"item2id_mars.json\"\n",
    "MARS_SHARD = DATA_DIR / \"mars_shards\" / \"mars_shard_full.pt\"\n",
    "\n",
    "item2id = json.load(open(VOCAB_FILE))\n",
    "vocab_size = len(item2id)\n",
    "\n",
    "mp = torch.load(MARS_SHARD)\n",
    "prefix_all = mp[\"prefix\"]\n",
    "target_all = mp[\"target\"]\n",
    "length_all = mp[\"length\"]\n",
    "\n",
    "total = prefix_all.size(0)\n",
    "val_n = max(1, int(0.2 * total))\n",
    "train_n = total - val_n\n",
    "\n",
    "train_prefix = prefix_all[:train_n]\n",
    "train_target = target_all[:train_n]\n",
    "\n",
    "val_prefix = prefix_all[train_n:]\n",
    "val_target = target_all[train_n:]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Dataset + Dataloader\n",
    "# =========================\n",
    "class MarsDataset(IterableDataset):\n",
    "    def __init__(self, P, T):\n",
    "        self.P = P\n",
    "        self.T = T\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.P.size(0)):\n",
    "            yield self.P[i], self.T[i]\n",
    "\n",
    "\n",
    "BATCH = 32\n",
    "train_loader = DataLoader(\n",
    "    MarsDataset(train_prefix, train_target),\n",
    "    batch_size=BATCH,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda b: (\n",
    "        torch.stack([x[0] for x in b]).to(device),\n",
    "        torch.tensor([int(x[1]) for x in b]).to(device),\n",
    "    )\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 4) Loss function (sampled softmax)\n",
    "# =========================\n",
    "def sampled_softmax_loss(final, y, emb_weights, num_neg=32):\n",
    "    pos_scores = (final * emb_weights[y]).sum(dim=1)\n",
    "\n",
    "    V = emb_weights.size(0)\n",
    "    neg_idx = torch.randint(0, V, (final.size(0), num_neg), device=final.device)\n",
    "    neg_w = emb_weights[neg_idx]\n",
    "\n",
    "    neg_scores = (neg_w * final.unsqueeze(1)).sum(dim=2)\n",
    "\n",
    "    logits = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)\n",
    "    labels = torch.zeros(final.size(0), dtype=torch.long, device=final.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Init model from scratch\n",
    "# =========================\n",
    "model = SASRecSmall(vocab_size=vocab_size).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Validation function\n",
    "# =========================\n",
    "def eval_recall_mrr(model, k=20):\n",
    "    model.eval()\n",
    "    hits, rr_sum, total_eval = 0, 0.0, val_prefix.size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(total_eval):\n",
    "            X = val_prefix[i].unsqueeze(0).to(device)\n",
    "            y = int(val_target[i].item())\n",
    "\n",
    "            logits, final = model(X)\n",
    "            scores = torch.matmul(final, model.item_emb.weight.t())\n",
    "\n",
    "            topk = scores.topk(k, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "\n",
    "            if y in topk:\n",
    "                hits += 1\n",
    "                rank = int((topk == y).nonzero()[0]) + 1\n",
    "                rr_sum += 1.0 / rank\n",
    "\n",
    "    return hits / total_eval, rr_sum / total_eval\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) Train Scratch Model\n",
    "# =========================\n",
    "EPOCHS = 10\n",
    "print(\"\\nTraining SASRec FROM SCRATCH on MARS...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for X, y in train_loader:\n",
    "        logits, final = model(X)\n",
    "        loss = sampled_softmax_loss(final, y, model.item_emb.weight, num_neg=32)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running += loss.item()\n",
    "        steps += 1\n",
    "\n",
    "    train_loss = running / max(1, steps)\n",
    "    recall, mrr = eval_recall_mrr(model)\n",
    "\n",
    "    print(f\"[Epoch {epoch}] train_loss={train_loss:.4f}, \"\n",
    "          f\"val_recall@20={recall:.4f}, mrr={mrr:.4f}, \"\n",
    "          f\"time={time.time()-t0:.1f}s\")\n",
    "\n",
    "print(\"\\nBaseline 2 training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536f111",
   "metadata": {},
   "source": [
    "### Baseline Comparison Table (for thesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc037b95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pop_recall20' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m results = []\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Example entries \u2014 replace with your measured results\u001b[39;00m\n\u001b[32m      5\u001b[39m results.append({\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mPopularity (Top-K)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRecall@20\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mpop_recall20\u001b[49m,\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMRR\u001b[39m\u001b[33m'\u001b[39m: pop_mrr\n\u001b[32m      9\u001b[39m })\n\u001b[32m     10\u001b[39m results.append({\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mSASRec-Scratch (MARS only)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRecall@20\u001b[39m\u001b[33m'\u001b[39m: scratch_recall20,\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMRR\u001b[39m\u001b[33m'\u001b[39m: scratch_mrr\n\u001b[32m     14\u001b[39m })\n\u001b[32m     15\u001b[39m results.append({\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mSASRec-Transfer (Pretrained \u2192 MARS)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRecall@20\u001b[39m\u001b[33m'\u001b[39m: recall_at_k,\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMRR\u001b[39m\u001b[33m'\u001b[39m: mrr\n\u001b[32m     19\u001b[39m })\n",
      "\u001b[31mNameError\u001b[39m: name 'pop_recall20' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "# Example entries \u2014 replace with your measured results\n",
    "results.append({\n",
    "    'Model': 'Popularity (Top-K)',\n",
    "    'Recall@20': pop_recall20,\n",
    "    'MRR': pop_mrr\n",
    "})\n",
    "results.append({\n",
    "    'Model': 'SASRec-Scratch (MARS only)',\n",
    "    'Recall@20': scratch_recall20,\n",
    "    'MRR': scratch_mrr\n",
    "})\n",
    "results.append({\n",
    "    'Model': 'SASRec-Transfer (Pretrained \u2192 MARS)',\n",
    "    'Recall@20': recall_at_k,\n",
    "    'MRR': mrr\n",
    "})\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71057558",
   "metadata": {},
   "source": [
    "### Plot Training Curves (Loss, Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86809d20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m,\u001b[32m4\u001b[39m))\n\u001b[32m      9\u001b[39m plt.subplot(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m plt.plot(\u001b[43mtrain_losses\u001b[49m)\n\u001b[32m     11\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mTrain Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mEpoch\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppose you collected during fine-tuning:\n",
    "# train_losses = [...]\n",
    "# val_recalls = [...]\n",
    "# val_mrrs = [...]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(val_recalls)\n",
    "plt.title('Validation Recall@20')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall@20')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(val_mrrs)\n",
    "plt.title('Validation MRR')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MRR')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eeb99e",
   "metadata": {},
   "source": [
    "### Meta-Learning Framework (Conceptual & Experimental Setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c86e409",
   "metadata": {},
   "source": [
    "\n",
    "We will design a meta-learning pipeline where each **course** or **domain** is treated as a *task*. The model learns an initialization \u03b8 that can quickly adapt to a new MOOC domain with very few interactions.\n",
    "\n",
    "#### Step 1 \u2014 Task Construction\n",
    "- Amazon Books \u2192 one task per genre.\n",
    "- Yoochoose \u2192 one task per category.\n",
    "- MARS \u2192 target task.\n",
    "\n",
    "Each task T\u1d62 consists of:\n",
    "- Support set S\u1d62 (small number of sessions)\n",
    "- Query set Q\u1d62 (evaluation target)\n",
    "\n",
    "#### Step 2 \u2014 Base Model\n",
    "Use the SAME SASRecSmall architecture. Extend it with task-conditioning if needed.\n",
    "\n",
    "#### Step 3 \u2014 Meta-Learning Algorithm Options\n",
    "- **MAML-style**: \n",
    "  - Compute task-specific gradients on S\u1d62\n",
    "  - Update \u03b8\u1d62\n",
    "  - Evaluate on Q\u1d62\n",
    "  - Backprop to update meta-parameters \u03b8\n",
    "\n",
    "- **Reptile-style** (simpler, faster, works well for recommendation):\n",
    "  - Copy \u03b8\n",
    "  - Train \u03b8\u1d62 on support set for few steps\n",
    "  - Move \u03b8 toward \u03b8\u1d62\n",
    "\n",
    "#### Step 4 \u2014 Transfer to MARS (Cold-Start Evaluation)\n",
    "- Fine-tune only item embeddings + output layer\n",
    "- Measure improvements vs scratch and vs transfer learning\n",
    "\n",
    "#### Step 5 \u2014 Metrics\n",
    "- Recall@20\n",
    "- MRR\n",
    "- Few-shot adaptation speed (how many steps required?)\n",
    "\n",
    "This section will be expanded in a separate notebook `08_meta_learning_framework.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480324ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1512fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a46f6081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs total: 2380 train: 1904 val: 476\n",
      "sample train prefix (first): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 434] target: 441\n",
      "sample val prefix (first): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 134, 129, 322, 426, 126] target: 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\2564913884.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mp = torch.load(DATA_DIR/\"mars_shards\"/\"mars_shard_full.pt\")\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics A: verify split counts and sample rows\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "mp = torch.load(DATA_DIR/\"mars_shards\"/\"mars_shard_full.pt\")\n",
    "total = mp['prefix'].size(0)\n",
    "val_n = max(1, int(0.2 * total))\n",
    "train_n = total - val_n\n",
    "print(\"pairs total:\", total, \"train:\", train_n, \"val:\", val_n)\n",
    "print(\"sample train prefix (first):\", mp['prefix'][0].tolist(), \"target:\", int(mp['target'][0].item()))\n",
    "print(\"sample val prefix (first):\", mp['prefix'][train_n].tolist(), \"target:\", int(mp['target'][train_n].item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7221a029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popularity val Recall@20: 0.05042016806722689 MRR: 0.019071525669091163\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics B: popularity baseline recall@20 + mrr on validation used above\n",
    "P = mp['prefix']; T = mp['target']\n",
    "train_targets = [int(x.item()) for x in T[:train_n]]\n",
    "pop = [x for x,_ in Counter(train_targets).most_common(200)]\n",
    "def eval_list_on_slice(indices, topk_list, K=20):\n",
    "    hits=0; rr=0.0; total=len(indices)\n",
    "    for idx in indices:\n",
    "        target = int(T[idx].item())\n",
    "        if target in topk_list[:K]:\n",
    "            hits += 1\n",
    "            rank = topk_list.index(target)+1\n",
    "            rr += 1.0/rank\n",
    "    return hits/total, rr/total\n",
    "val_indices = list(range(train_n, total))\n",
    "pop_recall, pop_mrr = eval_list_on_slice(val_indices, pop, K=20)\n",
    "print(\"Popularity val Recall@20:\", pop_recall, \"MRR:\", pop_mrr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26bd9a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\832510079.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(CKPT, map_location=\"cpu\")\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\832510079.py:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rank = int((topk == target).nonzero()[0]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer model eval on SAME val -> Recall@20: 0.025210084033613446 MRR: 0.0021053903079197717\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics C: evaluate the transfer checkpoint (best) on same validation\n",
    "CKPT = Path(\"../models/mars_finetune_best.pt\")  # update path if needed\n",
    "assert CKPT.exists(), \"checkpoint not found: \" + str(CKPT)\n",
    "ck = torch.load(CKPT, map_location=\"cpu\")\n",
    "# create model instance (must match architecture used for transfer)\n",
    "model_transfer = SASRecSmall(vocab_size=len(json.load(open(\"../data/processed/vocab_mars/item2id_mars.json\")))).to(\"cpu\")\n",
    "model_transfer.load_state_dict(ck['model_state'] if 'model_state' in ck else ck)\n",
    "model_transfer.eval()\n",
    "# evaluate on val slice\n",
    "def eval_model_on_indices(model, indices, K=20):\n",
    "    hits=0; rr=0.0; total=len(indices)\n",
    "    with torch.no_grad():\n",
    "        for idx in indices:\n",
    "            X = mp['prefix'][idx].unsqueeze(0).to(\"cpu\")\n",
    "            target = int(mp['target'][idx].item())\n",
    "            _, final = model(X)\n",
    "            scores = final @ model.item_emb.weight.t()\n",
    "            topk = scores.topk(K, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            if target in topk:\n",
    "                hits += 1\n",
    "                rank = int((topk == target).nonzero()[0]) + 1\n",
    "                rr += 1.0/rank\n",
    "    return hits/total, rr/total\n",
    "val_rec, val_mrr = eval_model_on_indices(model_transfer, val_indices, K=20)\n",
    "print(\"Transfer model eval on SAME val -> Recall@20:\", val_rec, \"MRR:\", val_mrr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39839204",
   "metadata": {},
   "source": [
    "The diagnostics show:\n",
    "\n",
    "MARS pairs: 2380 (train 1904 / val 476) \u2014 confirmed.\n",
    "\n",
    "Popularity baseline on this val: Recall@20 = 0.0504 (so popularity already beats your earlier transfer).\n",
    "\n",
    "Scratch SASRec (trained on MARS) reached val Recall@20 = 0.2311 (great).\n",
    "\n",
    "Transfer model (your fine-tuned-from-pretrain run) gives val Recall@20 = 0.0252 \u2014 worse than popularity and much worse than scratch.\n",
    "\n",
    "This means transfer as you did it is not helping on MARS. Two pragmatic next experiments (fast, ordered) will determine why and hopefully fix it:\n",
    "\n",
    "(1) Re-run transfer but unfreeze all parameters and fine-tune on MARS (short run).\n",
    "Rationale: freezing the encoder + very small LR can under-adapt pretrained weights to the small but different MARS distribution. Unfreezing will let the encoder adapt.\n",
    "\n",
    "(2) Run quick Adapter experiment (train only small adapter modules).\n",
    "Rationale: adapters are low-parameter and ideal for few-shot\u2014should adapt without catastrophic forgetting.\n",
    "\n",
    "I give you both ready-to-run code blocks below. Run (1) first (fast). If (1) doesn't reach scratch performance, run (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d708d326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "MARS vocab: 777\n",
      "Train pairs: 1904 Val pairs: 476\n",
      "Using checkpoint: ..\\models\\sasrec_full_top200000_epoch0.pt_epoch0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\3063418740.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mp = torch.load(MARS_SHARD_FILE)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\3063418740.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(PRETRAIN, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded.\n",
      "\n",
      "=== QUICK UNFROZEN FINE-TUNE (5 epochs) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\3063418740.py:96: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\3063418740.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\3063418740.py:115: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rank = int((topk == target).nonzero()[0]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] loss=3.6956  val_rec@20=0.0378  mrr=0.0055  time=21.3s\n",
      "[Epoch 1] loss=3.5525  val_rec@20=0.0420  mrr=0.0072  time=20.9s\n",
      "[Epoch 2] loss=3.5102  val_rec@20=0.0546  mrr=0.0089  time=21.4s\n",
      "[Epoch 3] loss=3.4773  val_rec@20=0.0672  mrr=0.0091  time=21.0s\n",
      "[Epoch 4] loss=3.4603  val_rec@20=0.0735  mrr=0.0110  time=20.8s\n",
      "\n",
      "Saved: ..\\models\\mars_finetune_unfrozen_5epoch.pt\n",
      "Best val Recall@20 = 0.07352941176470588\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Reload vocabulary\n",
    "# ------------------------------\n",
    "item2id_mars = json.load(open(DATA_DIR/\"vocab_mars\"/\"item2id_mars.json\"))\n",
    "vocab_size_mars = len(item2id_mars)\n",
    "print(\"MARS vocab:\", vocab_size_mars)\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Load full shard (prefix-target pairs)\n",
    "# ------------------------------\n",
    "mp = torch.load(MARS_SHARD_FILE)\n",
    "P = mp['prefix']\n",
    "T = mp['target']\n",
    "N = P.size(0)\n",
    "\n",
    "val_n = max(1, int(N * VAL_FRAC))\n",
    "train_n = N - val_n\n",
    "\n",
    "train_P = P[:train_n]\n",
    "train_T = T[:train_n]\n",
    "val_P = P[train_n:]\n",
    "val_T = T[train_n:]\n",
    "\n",
    "print(\"Train pairs:\", train_n, \"Val pairs:\", val_n)\n",
    "\n",
    "# ------------------------------\n",
    "# 3) SASRecSmall (same as pretraining)\n",
    "# ------------------------------\n",
    "class SASRecSmall(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=EMBED_DIM, max_len=MAX_PREFIX_LEN):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=4, dim_feedforward=2048, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=2)\n",
    "\n",
    "        self.out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.size()\n",
    "        pos_ids = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        seq = self.item_emb(x) + self.pos_emb(pos_ids)\n",
    "        seq = self.encoder(seq)\n",
    "        last = seq[:, -1, :]\n",
    "        logits = self.out(last)\n",
    "        return logits, last\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Load best pretrained checkpoint\n",
    "# ------------------------------\n",
    "models = sorted(CKPT_DIR.glob(\"*full*.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "assert len(models) > 0, \"No pretrained checkpoint found!\"\n",
    "PRETRAIN = models[0]\n",
    "print(\"Using checkpoint:\", PRETRAIN)\n",
    "\n",
    "ck = torch.load(PRETRAIN, map_location=device)\n",
    "state = ck[\"model_state\"] if \"model_state\" in ck else ck\n",
    "\n",
    "model = SASRecSmall(vocab_size_mars).to(device)\n",
    "\n",
    "# copy weights safely\n",
    "ms = model.state_dict()\n",
    "for k,v in state.items():\n",
    "    if k in ms and ms[k].shape == v.shape:\n",
    "        ms[k] = v\n",
    "model.load_state_dict(ms)\n",
    "print(\"Weights loaded.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5) LOSS + OPTIMIZER\n",
    "# ------------------------------\n",
    "def sampled_softmax_loss(final, y, emb_W, neg=FT_NEG):\n",
    "    pos = (final * emb_W[y]).sum(dim=1)\n",
    "    V = emb_W.size(0)\n",
    "    B = final.size(0)\n",
    "    neg_ids = torch.randint(0, V, (B, neg), device=final.device)\n",
    "    negW = emb_W[neg_ids]\n",
    "    neg_scores = (negW * final.unsqueeze(1)).sum(dim=2)\n",
    "    logits = torch.cat([pos.unsqueeze(1), neg_scores], dim=1)\n",
    "    labels = torch.zeros(B, dtype=torch.long, device=final.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Validation function\n",
    "# ------------------------------\n",
    "def validate(model, k=20):\n",
    "    model.eval()\n",
    "    hits, rr_sum = 0, 0.0\n",
    "    total = val_P.size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(total):\n",
    "            x = val_P[i].unsqueeze(0).to(device)\n",
    "            target = val_T[i].item()\n",
    "            _, final = model(x)\n",
    "            scores = torch.matmul(final, model.item_emb.weight.t())\n",
    "            topk = scores.topk(k, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            if target in topk:\n",
    "                hits += 1\n",
    "                rank = int((topk == target).nonzero()[0]) + 1\n",
    "                rr_sum += 1.0 / rank\n",
    "\n",
    "    return hits/total, rr_sum/total\n",
    "\n",
    "# ------------------------------\n",
    "# 7) QUICK 5-EPOCH FULL FINE-TUNE\n",
    "# ------------------------------\n",
    "EPOCHS = 5\n",
    "print(\"\\n=== QUICK UNFROZEN FINE-TUNE (5 epochs) ===\")\n",
    "\n",
    "best_val = -1\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    running = 0\n",
    "\n",
    "    for i in range(train_P.size(0)):\n",
    "        x = train_P[i].unsqueeze(0).to(device)\n",
    "        y = train_T[i].unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            _, final = model(x)\n",
    "            loss = sampled_softmax_loss(final, y, model.item_emb.weight)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "        running += loss.item()\n",
    "\n",
    "    train_loss = running / train_P.size(0)\n",
    "    val_rec, val_mrr = validate(model)\n",
    "\n",
    "    print(f\"[Epoch {epoch}] loss={train_loss:.4f}  val_rec@20={val_rec:.4f}  mrr={val_mrr:.4f}  time={time.time()-t0:.1f}s\")\n",
    "\n",
    "    if val_rec > best_val:\n",
    "        best_val = val_rec\n",
    "        best_state = deepcopy(model.state_dict())\n",
    "\n",
    "# save best\n",
    "torch.save({\"model_state\": best_state}, CKPT_DIR/\"mars_finetune_unfrozen_5epoch.pt\")\n",
    "print(\"\\nSaved:\", CKPT_DIR/\"mars_finetune_unfrozen_5epoch.pt\")\n",
    "print(\"Best val Recall@20 =\", best_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb0227",
   "metadata": {},
   "source": [
    "##### unfrozen fine-tune improved things (val Recall@20 from 0.025 \u2192 0.0735), but it\u2019s still below the scratch SASRec (~0.23). That\u2019s an important signal and gives us a clear next plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20fe247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\794388907.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(PRETRAIN, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: ['item_emb.weight', 'out.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\3063418740.py:115: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rank = int((topk == target).nonzero()[0]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Reinit-Emb] epoch 0 loss=3.4679 val_rec=0.0588 mrr=0.0154\n",
      "[Reinit-Emb] epoch 1 loss=3.3844 val_rec=0.0672 mrr=0.0159\n",
      "[Reinit-Emb] epoch 2 loss=3.3207 val_rec=0.0588 mrr=0.0160\n",
      "[Reinit-Emb] epoch 3 loss=3.2583 val_rec=0.0567 mrr=0.0151\n",
      "[Reinit-Emb] epoch 4 loss=3.1967 val_rec=0.0483 mrr=0.0151\n",
      "[Reinit-Emb] epoch 5 loss=3.1358 val_rec=0.0525 mrr=0.0158\n",
      "[Reinit-Emb] epoch 6 loss=3.0688 val_rec=0.0609 mrr=0.0162\n",
      "[Reinit-Emb] epoch 7 loss=3.0072 val_rec=0.0672 mrr=0.0184\n",
      "[Reinit-Emb] epoch 8 loss=2.9534 val_rec=0.0567 mrr=0.0202\n",
      "[Reinit-Emb] epoch 9 loss=2.9006 val_rec=0.0609 mrr=0.0210\n",
      "[Reinit-Emb] epoch 10 loss=2.8656 val_rec=0.0567 mrr=0.0219\n",
      "[Reinit-Emb] epoch 11 loss=2.8278 val_rec=0.0672 mrr=0.0247\n",
      "[Reinit-Emb] epoch 12 loss=2.7967 val_rec=0.0630 mrr=0.0251\n",
      "[Reinit-Emb] epoch 13 loss=2.7692 val_rec=0.0609 mrr=0.0232\n",
      "[Reinit-Emb] epoch 14 loss=2.7403 val_rec=0.0693 mrr=0.0237\n",
      "[Reinit-Emb] epoch 15 loss=2.7238 val_rec=0.0651 mrr=0.0237\n",
      "[Reinit-Emb] epoch 16 loss=2.7169 val_rec=0.0651 mrr=0.0244\n",
      "[Reinit-Emb] epoch 17 loss=2.6897 val_rec=0.0714 mrr=0.0244\n",
      "[Reinit-Emb] epoch 18 loss=2.6869 val_rec=0.0714 mrr=0.0257\n",
      "[Reinit-Emb] epoch 19 loss=2.6615 val_rec=0.0651 mrr=0.0267\n",
      "Saved mars_reinit_emb_best.pt best_val: 0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "# === Experiment A: reinit item_emb + out, freeze encoder, train embeddings + out ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1) instantiate model and load pretrained encoder weights (same as earlier)\n",
    "model = SASRecSmall(vocab_size=vocab_size_mars, embed_dim=EMBED_DIM, max_len=MAX_PREFIX_LEN).to(device)\n",
    "ck = torch.load(PRETRAIN, map_location=device)\n",
    "state = ck['model_state'] if 'model_state' in ck else ck\n",
    "ms = model.state_dict()\n",
    "for k,v in state.items():\n",
    "    if k in ms and ms[k].shape == v.shape:\n",
    "        ms[k] = v\n",
    "# NOTE: this will overwrite item_emb if shapes match \u2014 we will reinit after load\n",
    "model.load_state_dict(ms)\n",
    "\n",
    "# 2) reinitialize item embedding and out head (random init)\n",
    "nn.init.normal_(model.item_emb.weight, mean=0.0, std=0.01)\n",
    "nn.init.normal_(model.out.weight, mean=0.0, std=0.01)\n",
    "\n",
    "# 3) freeze encoder and pos_emb; train item_emb + out only\n",
    "for name,p in model.named_parameters():\n",
    "    if name.startswith('encoder') or name.startswith('pos_emb'):\n",
    "        p.requires_grad = False\n",
    "    else:\n",
    "        p.requires_grad = True\n",
    "print(\"Trainable params:\", [n for n,p in model.named_parameters() if p.requires_grad])\n",
    "\n",
    "# 4) dataloaders (use train_P, train_T created earlier)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "class InMemPairs(Dataset):\n",
    "    def __init__(self,P,T): self.P=P; self.T=T\n",
    "    def __len__(self): return self.P.size(0)\n",
    "    def __getitem__(self,i): return self.P[i], int(self.T[i].item())\n",
    "\n",
    "train_loader = DataLoader(InMemPairs(train_P, train_T), batch_size=32, shuffle=True, num_workers=0,\n",
    "                          collate_fn=lambda b: (torch.stack([x[0] for x in b]).to(device),\n",
    "                                               torch.tensor([x[1] for x in b], device=device)))\n",
    "opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "# 5) train a few epochs\n",
    "def sampled_softmax_loss(final,y,emb_W,neg=32):\n",
    "    pos=(final * emb_W[y]).sum(dim=1)\n",
    "    V=emb_W.size(0); B=final.size(0)\n",
    "    neg_idx=torch.randint(0,V,(B,neg),device=final.device)\n",
    "    negW=emb_W[neg_idx]; neg_scores=(negW*final.unsqueeze(1)).sum(dim=2)\n",
    "    logits=torch.cat([pos.unsqueeze(1), neg_scores], dim=1)\n",
    "    labels=torch.zeros(B,dtype=torch.long,device=final.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "best_val=-1; best_state=None\n",
    "for epoch in range(20):\n",
    "    model.train(); running=0.0; steps=0\n",
    "    for X,y in train_loader:\n",
    "        logits, final = model(X)\n",
    "        loss = sampled_softmax_loss(final, y, model.item_emb.weight, neg=32)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        running += float(loss.item()); steps += 1\n",
    "    val_rec, val_mrr = validate(model)\n",
    "    print(f\"[Reinit-Emb] epoch {epoch} loss={running/max(1,steps):.4f} val_rec={val_rec:.4f} mrr={val_mrr:.4f}\")\n",
    "    if val_rec > best_val:\n",
    "        best_val=val_rec; best_state=deepcopy(model.state_dict())\n",
    "torch.save({'model_state':best_state}, CKPT_DIR/\"mars_reinit_emb_best.pt\")\n",
    "print(\"Saved mars_reinit_emb_best.pt best_val:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff01bcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "MARS vocab: 777\n",
      "Train pairs: 1904 Val pairs: 476\n",
      "Using checkpoint: ..\\models\\sasrec_full_top200000_epoch0.pt_epoch0.pt\n",
      "Weights loaded.\n",
      "\n",
      "=== QUICK UNFROZEN FINE-TUNE (5 epochs) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\3063418740.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mp = torch.load(MARS_SHARD_FILE)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\3063418740.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(PRETRAIN, map_location=device)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\3063418740.py:96: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\3063418740.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\3063418740.py:115: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rank = int((topk == target).nonzero()[0]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] loss=3.6753  val_rec@20=0.0231  mrr=0.0030  time=21.5s\n",
      "[Epoch 1] loss=3.5572  val_rec@20=0.0189  mrr=0.0029  time=21.4s\n",
      "[Epoch 2] loss=3.5070  val_rec@20=0.0231  mrr=0.0039  time=21.1s\n",
      "[Epoch 3] loss=3.4751  val_rec@20=0.0294  mrr=0.0047  time=20.9s\n",
      "[Epoch 4] loss=3.4550  val_rec@20=0.0420  mrr=0.0068  time=21.1s\n",
      "\n",
      "Saved: ..\\models\\mars_finetune_unfrozen_5epoch.pt\n",
      "Best val Recall@20 = 0.04201680672268908\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Reload vocabulary\n",
    "# ------------------------------\n",
    "item2id_mars = json.load(open(DATA_DIR/\"vocab_mars\"/\"item2id_mars.json\"))\n",
    "vocab_size_mars = len(item2id_mars)\n",
    "print(\"MARS vocab:\", vocab_size_mars)\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Load full shard (prefix-target pairs)\n",
    "# ------------------------------\n",
    "mp = torch.load(MARS_SHARD_FILE)\n",
    "P = mp['prefix']\n",
    "T = mp['target']\n",
    "N = P.size(0)\n",
    "\n",
    "val_n = max(1, int(N * VAL_FRAC))\n",
    "train_n = N - val_n\n",
    "\n",
    "train_P = P[:train_n]\n",
    "train_T = T[:train_n]\n",
    "val_P = P[train_n:]\n",
    "val_T = T[train_n:]\n",
    "\n",
    "print(\"Train pairs:\", train_n, \"Val pairs:\", val_n)\n",
    "\n",
    "# ------------------------------\n",
    "# 3) SASRecSmall (same as pretraining)\n",
    "# ------------------------------\n",
    "class SASRecSmall(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=EMBED_DIM, max_len=MAX_PREFIX_LEN):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=4, dim_feedforward=2048, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=2)\n",
    "\n",
    "        self.out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.size()\n",
    "        pos_ids = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        seq = self.item_emb(x) + self.pos_emb(pos_ids)\n",
    "        seq = self.encoder(seq)\n",
    "        last = seq[:, -1, :]\n",
    "        logits = self.out(last)\n",
    "        return logits, last\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Load best pretrained checkpoint\n",
    "# ------------------------------\n",
    "models = sorted(CKPT_DIR.glob(\"*full*.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "assert len(models) > 0, \"No pretrained checkpoint found!\"\n",
    "PRETRAIN = models[0]\n",
    "print(\"Using checkpoint:\", PRETRAIN)\n",
    "\n",
    "ck = torch.load(PRETRAIN, map_location=device)\n",
    "state = ck[\"model_state\"] if \"model_state\" in ck else ck\n",
    "\n",
    "model = SASRecSmall(vocab_size_mars).to(device)\n",
    "\n",
    "# copy weights safely\n",
    "ms = model.state_dict()\n",
    "for k,v in state.items():\n",
    "    if k in ms and ms[k].shape == v.shape:\n",
    "        ms[k] = v\n",
    "model.load_state_dict(ms)\n",
    "print(\"Weights loaded.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5) LOSS + OPTIMIZER\n",
    "# ------------------------------\n",
    "def sampled_softmax_loss(final, y, emb_W, neg=FT_NEG):\n",
    "    pos = (final * emb_W[y]).sum(dim=1)\n",
    "    V = emb_W.size(0)\n",
    "    B = final.size(0)\n",
    "    neg_ids = torch.randint(0, V, (B, neg), device=final.device)\n",
    "    negW = emb_W[neg_ids]\n",
    "    neg_scores = (negW * final.unsqueeze(1)).sum(dim=2)\n",
    "    logits = torch.cat([pos.unsqueeze(1), neg_scores], dim=1)\n",
    "    labels = torch.zeros(B, dtype=torch.long, device=final.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Validation function\n",
    "# ------------------------------\n",
    "def validate(model, k=20):\n",
    "    model.eval()\n",
    "    hits, rr_sum = 0, 0.0\n",
    "    total = val_P.size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(total):\n",
    "            x = val_P[i].unsqueeze(0).to(device)\n",
    "            target = val_T[i].item()\n",
    "            _, final = model(x)\n",
    "            scores = torch.matmul(final, model.item_emb.weight.t())\n",
    "            topk = scores.topk(k, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            if target in topk:\n",
    "                hits += 1\n",
    "                rank = int((topk == target).nonzero()[0]) + 1\n",
    "                rr_sum += 1.0 / rank\n",
    "\n",
    "    return hits/total, rr_sum/total\n",
    "\n",
    "# ------------------------------\n",
    "# 7) QUICK 5-EPOCH FULL FINE-TUNE\n",
    "# ------------------------------\n",
    "EPOCHS = 5\n",
    "print(\"\\n=== QUICK UNFROZEN FINE-TUNE (5 epochs) ===\")\n",
    "\n",
    "best_val = -1\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    running = 0\n",
    "\n",
    "    for i in range(train_P.size(0)):\n",
    "        x = train_P[i].unsqueeze(0).to(device)\n",
    "        y = train_T[i].unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            _, final = model(x)\n",
    "            loss = sampled_softmax_loss(final, y, model.item_emb.weight)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "        running += loss.item()\n",
    "\n",
    "    train_loss = running / train_P.size(0)\n",
    "    val_rec, val_mrr = validate(model)\n",
    "\n",
    "    print(f\"[Epoch {epoch}] loss={train_loss:.4f}  val_rec@20={val_rec:.4f}  mrr={val_mrr:.4f}  time={time.time()-t0:.1f}s\")\n",
    "\n",
    "    if val_rec > best_val:\n",
    "        best_val = val_rec\n",
    "        best_state = deepcopy(model.state_dict())\n",
    "\n",
    "# save best\n",
    "torch.save({\"model_state\": best_state}, CKPT_DIR/\"mars_finetune_unfrozen_5epoch.pt\")\n",
    "print(\"\\nSaved:\", CKPT_DIR/\"mars_finetune_unfrozen_5epoch.pt\")\n",
    "print(\"Best val Recall@20 =\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "245fd0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\799690715.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mp = torch.load(MARS_SHARD_FILE)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\799690715.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(path, map_location='cpu')\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\799690715.py:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rank = int((topk==target).nonzero()[0]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating checkpoints: {'pretrained_unfrozen_5epoch': WindowsPath('../models/mars_finetune_unfrozen_5epoch.pt'), 'reinit_emb_best': WindowsPath('../models/mars_reinit_emb_best.pt'), 'finetune_best': WindowsPath('../models/mars_finetune_best.pt')}\n",
      "Loading pretrained_unfrozen_5epoch ..\\models\\mars_finetune_unfrozen_5epoch.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_43092\\799690715.py:80: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rank = int((topk==target).nonzero()[0]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pretrained_unfrozen_5epoch val_recall@20=0.0420 val_mrr=0.0068 test_recall@20=0.025210084033613446 test_mrr=0.003011204481792717\n",
      "Loading reinit_emb_best ..\\models\\mars_reinit_emb_best.pt\n",
      " reinit_emb_best val_recall@20=0.0714 val_mrr=0.0244 test_recall@20=0.046218487394957986 test_mrr=0.01974789915966386\n",
      "Loading finetune_best ..\\models\\mars_finetune_best.pt\n",
      " finetune_best val_recall@20=0.0252 val_mrr=0.0021 test_recall@20=0.029411764705882353 test_mrr=0.002663102721152257\n",
      "| name                       | path                             |   val_recall20 |    val_mrr |   test_recall20 |   test_mrr |\n",
      "|:---------------------------|:---------------------------------|---------------:|-----------:|----------------:|-----------:|\n",
      "| pretrained_unfrozen_5epoch | mars_finetune_unfrozen_5epoch.pt |      0.0420168 | 0.00679716 |       0.0252101 |  0.0030112 |\n",
      "| reinit_emb_best            | mars_reinit_emb_best.pt          |      0.0714286 | 0.0243937  |       0.0462185 |  0.0197479 |\n",
      "| finetune_best              | mars_finetune_best.pt            |      0.0252101 | 0.00210539 |       0.0294118 |  0.0026631 |\n",
      "Saved CKPT eval summary to ..\\models\\ckpt_eval_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluate multiple checkpoints on val + test (single-run)\n",
    "import torch, json, numpy as np, pandas as pd, time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ROOT = Path('..')\n",
    "DATA_DIR = ROOT/'data'/'processed'\n",
    "CKPT_DIR = ROOT/'models'\n",
    "MARS_SHARD_FILE = DATA_DIR/'mars_shards'/'mars_shard_full.pt'\n",
    "TEST_PAIRS = DATA_DIR/'mars_test_pairs.parquet'\n",
    "\n",
    "# Simple SASRecSmall definition (must match model used in checkpoints)\n",
    "class SASRecSmall(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, max_len=20):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "        layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=2048, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=2)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    def forward(self,x):\n",
    "        B,L = x.size()\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0).expand(B,L)\n",
    "        seq = self.item_emb(x) + self.pos_emb(pos)\n",
    "        seq = self.encoder(seq)\n",
    "        last = seq[:,-1,:]\n",
    "        logits = self.out(last)\n",
    "        return logits, last\n",
    "\n",
    "# Load shard\n",
    "mp = torch.load(MARS_SHARD_FILE)\n",
    "P_all = mp['prefix']; T_all = mp['target']\n",
    "N = P_all.size(0)\n",
    "val_n = max(1, int(0.2 * N))\n",
    "train_n = N - val_n\n",
    "val_P = P_all[train_n:]; val_T = T_all[train_n:]\n",
    "\n",
    "# load test df if exists\n",
    "if TEST_PAIRS.exists():\n",
    "    df_test = pd.read_parquet(TEST_PAIRS)\n",
    "else:\n",
    "    df_test = None\n",
    "\n",
    "# helper eval on prefix tensors\n",
    "def eval_prefix_tensor_model(model, P_tensor, T_tensor, K=20):\n",
    "    model.eval()\n",
    "    hits=0; rr=0.0; tot=P_tensor.size(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(tot):\n",
    "            X = P_tensor[i].unsqueeze(0).to(device)\n",
    "            target = int(T_tensor[i].item())\n",
    "            _, final = model(X)\n",
    "            scores = final @ model.item_emb.weight.t()\n",
    "            topk = scores.topk(K, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            if target in topk:\n",
    "                hits += 1\n",
    "                rank = int((topk==target).nonzero()[0]) + 1\n",
    "                rr += 1.0/rank\n",
    "    return hits/tot, rr/tot\n",
    "\n",
    "def eval_on_test_df(model, df_test, item2id):\n",
    "    # df_test has 'prefix' as string of ids and 'target' as int id\n",
    "    model.eval()\n",
    "    hits=0; rr=0.0; tot=len(df_test)\n",
    "    with torch.no_grad():\n",
    "        for _, r in df_test.iterrows():\n",
    "            pref = r['prefix'] if isinstance(r['prefix'], str) else ''\n",
    "            pref_ids = [int(x) for x in pref.split()] if pref else []\n",
    "            if len(pref_ids) > 20:\n",
    "                pref_ids = pref_ids[-20:]\n",
    "            padded = [0]*(20-len(pref_ids)) + pref_ids\n",
    "            X = torch.LongTensor([padded]).to(device)\n",
    "            target = int(r['target'])\n",
    "            _, final = model(X)\n",
    "            scores = final @ model.item_emb.weight.t()\n",
    "            topk = scores.topk(20, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            if target in topk:\n",
    "                hits += 1\n",
    "                rank = int((topk==target).nonzero()[0]) + 1\n",
    "                rr += 1.0/rank\n",
    "    return hits/max(1,tot), rr/max(1,tot)\n",
    "\n",
    "# set checkpoint list (edit to include the ckpts you want)\n",
    "candidates = {\n",
    "    'scratch': CKPT_DIR / 'sasrec_scratch_mars_best.pt',                # if you saved it; else set to None\n",
    "    'pretrained_unfrozen_5epoch': CKPT_DIR / 'mars_finetune_unfrozen_5epoch.pt',\n",
    "    'reinit_emb_best': CKPT_DIR / 'mars_reinit_emb_best.pt',\n",
    "    'adapters_best': CKPT_DIR / 'mars_adapters.pt',\n",
    "    'finetune_best': CKPT_DIR / 'mars_finetune_best.pt'                 # earlier best from your pipeline\n",
    "}\n",
    "\n",
    "# filter existing ckpts\n",
    "candidates = {k:v for k,v in candidates.items() if v is not None and v.exists()}\n",
    "print(\"Evaluating checkpoints:\", candidates)\n",
    "\n",
    "# load item2id for mapping (if needed)\n",
    "item2id = json.load(open(DATA_DIR/'vocab_mars'/'item2id_mars.json'))\n",
    "\n",
    "rows=[]\n",
    "for name, path in candidates.items():\n",
    "    print(\"Loading\", name, path)\n",
    "    ck = torch.load(path, map_location='cpu')\n",
    "    state = ck.get('model_state', ck)\n",
    "    model = SASRecSmall(vocab_size=len(item2id)).to(device)\n",
    "    # safe load: only keys matching shapes\n",
    "    ms = model.state_dict()\n",
    "    for k,v in state.items():\n",
    "        if k in ms and ms[k].shape == v.shape:\n",
    "            ms[k] = v\n",
    "    model.load_state_dict(ms)\n",
    "    # eval val\n",
    "    vrec, vmrr = eval_prefix_tensor_model(model, val_P, val_T, K=20)\n",
    "    # eval test if available\n",
    "    if df_test is not None:\n",
    "        trec, tmrr = eval_on_test_df(model, df_test, item2id)\n",
    "    else:\n",
    "        trec, tmrr = None, None\n",
    "    rows.append({'name': name, 'path': str(path.name), 'val_recall20': vrec, 'val_mrr': vmrr, 'test_recall20': trec, 'test_mrr': tmrr})\n",
    "    print(f\" {name} val_recall@20={vrec:.4f} val_mrr={vmrr:.4f} test_recall@20={trec} test_mrr={tmrr}\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.to_markdown(index=False))\n",
    "df.to_csv(CKPT_DIR/'ckpt_eval_summary.csv', index=False)\n",
    "print(\"Saved CKPT eval summary to\", CKPT_DIR/'ckpt_eval_summary.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}