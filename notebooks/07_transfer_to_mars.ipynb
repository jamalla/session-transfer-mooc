{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173b4d36",
   "metadata": {},
   "source": [
    "### imports, config, and build MARS shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74989259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\n"
     ]
    }
   ],
   "source": [
    "# Quick (unsafe) workaround to avoid the libiomp5md.dll crash.\n",
    "# Use this only to continue working in the notebook quickly.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "print(\"Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22024595",
   "metadata": {},
   "source": [
    "#### Purpose: transfer the pretrained SASRec encoder to MARS, fine-tune, validate, and save results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7996442",
   "metadata": {},
   "source": [
    "### Imports, global config, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9181c55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Paths set.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "# Paths (adjust if needed)\n",
    "ROOT = Path('..')\n",
    "DATA_DIR = ROOT / 'data' / 'processed'\n",
    "CKPT_DIR = ROOT / 'models'\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "MARS_VOCAB_DIR = DATA_DIR / 'vocab_mars'\n",
    "MARS_SHARD_DIR = DATA_DIR / 'mars_shards'\n",
    "MARS_SHARD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "\n",
    "# Fine-tune hyperparams (safe defaults)\n",
    "MAX_PREFIX_LEN = 20\n",
    "EMBED_DIM = 64\n",
    "FT_BATCH_SIZE = 32\n",
    "FT_LR = 1e-5\n",
    "FT_EPOCHS = 12\n",
    "FT_NEG = 32\n",
    "UNFREEZE_AFTER = 3\n",
    "EARLY_STOPPING_PATIENCE = 4\n",
    "FP16 = True\n",
    "VAL_FRAC = 0.2\n",
    "\n",
    "\n",
    "# Filenames\n",
    "MARS_INTERACTIONS = DATA_DIR / 'mars_interactions.parquet'\n",
    "MARS_PAIRS = DATA_DIR / 'mars_prefix_target.parquet'\n",
    "MARS_SHARD_FILE = MARS_SHARD_DIR / 'mars_shard_full.pt'\n",
    "\n",
    "\n",
    "print('Paths set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170c782",
   "metadata": {},
   "source": [
    "### SASRecSmall model (exact compatible implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d806cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SASRecSmall defined\n"
     ]
    }
   ],
   "source": [
    "class SASRecSmall(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=EMBED_DIM, max_len=MAX_PREFIX_LEN, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "\n",
    "        self.item_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "        d_model=embed_dim,\n",
    "        nhead=num_heads,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=dropout,\n",
    "        batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "\n",
    "        # output head: map embedding -> embedding (we use sampled softmax that uses final*item_emb.T)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L)\n",
    "        B, L = x.size()\n",
    "        pos_ids = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        seq = self.item_emb(x) + self.pos_emb(pos_ids)\n",
    "        seq = self.encoder(seq)\n",
    "        last = seq[:, -1, :]\n",
    "        logits = self.out(last)\n",
    "        return logits, last\n",
    "\n",
    "\n",
    "    print('SASRecSmall defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a86005e",
   "metadata": {},
   "source": [
    "### Build or load MARS vocab and shard (prefix-target pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b684151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARS vocab size: 777\n",
      "MARS shard exists: ..\\data\\processed\\mars_shards\\mars_shard_full.pt\n",
      "Loaded MARS shard with pairs: 2380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16032\\774182814.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mp = torch.load(MARS_SHARD_FILE)\n"
     ]
    }
   ],
   "source": [
    "if not (MARS_VOCAB_DIR / 'item2id_mars.json').exists():\n",
    "    if not MARS_INTERACTIONS.exists():\n",
    "        raise FileNotFoundError(f\"Missing {MARS_INTERACTIONS} — run sessionization notebook first\")\n",
    "    df_m = pd.read_parquet(MARS_INTERACTIONS)\n",
    "    mars_items = sorted(df_m['item_id'].astype(str).unique())\n",
    "    item2id_mars = {it: idx+1 for idx, it in enumerate(mars_items)}\n",
    "    item2id_mars['<OOV>'] = 0\n",
    "    MARS_VOCAB_DIR.mkdir(exist_ok=True)\n",
    "    json.dump(item2id_mars, open(MARS_VOCAB_DIR / 'item2id_mars.json','w'))\n",
    "    print('Saved item2id_mars.json')\n",
    "else:\n",
    "    item2id_mars = json.load(open(MARS_VOCAB_DIR / 'item2id_mars.json'))\n",
    "\n",
    "vocab_size_mars = len(item2id_mars)\n",
    "print('MARS vocab size:', vocab_size_mars)\n",
    "\n",
    "# Build shard from MARS pairs if missing\n",
    "if not MARS_SHARD_FILE.exists():\n",
    "    if not MARS_PAIRS.exists():\n",
    "        raise FileNotFoundError(f\"Missing {MARS_PAIRS} — create prefix-target pairs first\")\n",
    "    print('Building MARS shard from', MARS_PAIRS)\n",
    "    df_pairs = pd.read_parquet(MARS_PAIRS)\n",
    "    prefixes, targets, lengths = [], [], []\n",
    "    for _, r in df_pairs.iterrows():\n",
    "        pref = r['prefix'] if isinstance(r['prefix'], str) else ''\n",
    "        pref_ids = [ item2id_mars.get(x, 0) for x in pref.split() ] if pref else []\n",
    "        if len(pref_ids) > MAX_PREFIX_LEN:\n",
    "            pref_ids = pref_ids[-MAX_PREFIX_LEN:]\n",
    "        padded = [0]*(MAX_PREFIX_LEN - len(pref_ids)) + pref_ids\n",
    "        prefixes.append(padded)\n",
    "        targets.append(item2id_mars.get(str(r['target']), 0))\n",
    "        lengths.append(len(pref_ids))\n",
    "    pt = {\n",
    "        'prefix': torch.LongTensor(prefixes),\n",
    "        'target': torch.LongTensor(targets),\n",
    "        'length': torch.LongTensor(lengths)\n",
    "    }\n",
    "    torch.save(pt, MARS_SHARD_FILE)\n",
    "    print('Wrote MARS shard:', MARS_SHARD_FILE, 'pairs:', len(prefixes))\n",
    "else:\n",
    "    print('MARS shard exists:', MARS_SHARD_FILE)\n",
    "\n",
    "# Load shard into memory for small dataset\n",
    "mp = torch.load(MARS_SHARD_FILE)\n",
    "print('Loaded MARS shard with pairs:', mp['prefix'].size(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86025d6c",
   "metadata": {},
   "source": [
    "### Auto-find pretrained checkpoint, create mars_model, copy weights, freeze encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaac12c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found models: ['sasrec_full_top200000_epoch0.pt_epoch0.pt', 'sasrec_phaseB_top200000_epoch1_epoch1.pt', 'sasrec_phaseB_top200000_epoch0_epoch0.pt', 'sasrec_warmup_top200000_epoch2_epoch2.pt', 'sasrec_warmup_top200000_epoch1_epoch1.pt', 'sasrec_warmup_top200000_epoch0_epoch0.pt']\n",
      "Auto-selected checkpoint: ..\\models\\sasrec_full_top200000_epoch0.pt_epoch0.pt\n",
      "Loaded checkpoint: sasrec_full_top200000_epoch0.pt_epoch0.pt\n",
      "Copied exact: 25 partial: 1 skipped: 2\n",
      "Froze encoder + embeddings for initial fine-tune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16032\\2355617064.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(PRETRAIN_CKPT, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "models = sorted(CKPT_DIR.glob('*.pt'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "print('Found models:', [p.name for p in models[:10]])\n",
    "\n",
    "# prefer 'full' > 'phaseb' > 'warmup' > newest\n",
    "def score_name(fn):\n",
    "    n = fn.name.lower()\n",
    "    if 'full' in n: return 100\n",
    "    if 'phaseb' in n: return 80\n",
    "    if 'warmup' in n: return 50\n",
    "    return 10\n",
    "\n",
    "models = sorted(models, key=lambda p: (score_name(p), p.stat().st_mtime), reverse=True)\n",
    "PRETRAIN_CKPT = models[0] if models else None\n",
    "print('Auto-selected checkpoint:', PRETRAIN_CKPT)\n",
    "\n",
    "# instantiate mars model\n",
    "mars_model = SASRecSmall(vocab_size=vocab_size_mars, embed_dim=EMBED_DIM, max_len=MAX_PREFIX_LEN).to(device)\n",
    "\n",
    "# load checkpoint & copy weights safely\n",
    "if PRETRAIN_CKPT and PRETRAIN_CKPT.exists():\n",
    "    ck = torch.load(PRETRAIN_CKPT, map_location=device)\n",
    "    pretrained_state = ck['model_state'] if 'model_state' in ck else ck\n",
    "    mars_state = mars_model.state_dict()\n",
    "    copied, partial, skipped = [], [], []\n",
    "    for k,v in pretrained_state.items():\n",
    "        if k in mars_state:\n",
    "            try:\n",
    "                if mars_state[k].shape == v.shape:\n",
    "                    mars_state[k] = v\n",
    "                    copied.append(k)\n",
    "                else:\n",
    "                    if 'item_emb.weight' in k and v.ndim==2:\n",
    "                        n = min(mars_state[k].shape[0], v.shape[0])\n",
    "                        mars_state[k][:n] = v[:n]\n",
    "                        partial.append((k, n))\n",
    "                    else:\n",
    "                        skipped.append((k, 'shape_mismatch'))\n",
    "            except Exception as e:\n",
    "                skipped.append((k, f'error:{e}'))\n",
    "        else:\n",
    "            skipped.append((k, 'missing_in_target'))\n",
    "    mars_model.load_state_dict(mars_state)\n",
    "    print('Loaded checkpoint:', PRETRAIN_CKPT.name)\n",
    "    print('Copied exact:', len(copied), 'partial:', len(partial), 'skipped:', len(skipped))\n",
    "else:\n",
    "    print('No pretrained checkpoint found — training from scratch')\n",
    "\n",
    "# Freeze encoder/embeddings initially\n",
    "for name, p in mars_model.named_parameters():\n",
    "    if name.startswith('encoder') or name.startswith('item_emb') or name.startswith('pos_emb'):\n",
    "        p.requires_grad = False\n",
    "print('Froze encoder + embeddings for initial fine-tune')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6421506",
   "metadata": {},
   "source": [
    "### Build DataLoader and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614947be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders ready. train_pairs: 1904 val_pairs: 476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16032\\2731936318.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mp = torch.load(MARS_SHARD_FILE)\n"
     ]
    }
   ],
   "source": [
    "class MarsInMem(IterableDataset):\n",
    "    def __init__(self, pt): self.pt = pt\n",
    "    def __iter__(self):\n",
    "        P, L, T = self.pt['prefix'], self.pt['length'], self.pt['target']\n",
    "        for i in range(P.size(0)):\n",
    "            yield P[i], int(L[i].item()), int(T[i].item())\n",
    "\n",
    "mp = torch.load(MARS_SHARD_FILE)\n",
    "num_pairs = mp['prefix'].size(0)\n",
    "val_n = max(1, int(num_pairs * VAL_FRAC))\n",
    "train_n = num_pairs - val_n\n",
    "\n",
    "P_all = mp['prefix']\n",
    "T_all = mp['target']\n",
    "val_prefixes = P_all[train_n:]\n",
    "val_targets = T_all[train_n:]\n",
    "\n",
    "mars_ds = MarsInMem(mp)\n",
    "# Note: DataLoader with IterableDataset does not accept shuffle=True. If you need shuffling,\n",
    "# either implement shuffling inside the IterableDataset or set up a map-style Dataset.\n",
    "# For MARS (small dataset), deterministic ordering is fine.\n",
    "mars_loader = DataLoader(mars_ds, batch_size=FT_BATCH_SIZE, collate_fn=lambda b: (\n",
    "    torch.stack([x[0] for x in b], dim=0).to(device),\n",
    "    torch.tensor([x[1] for x in b], dtype=torch.long).to(device),\n",
    "    torch.tensor([x[2] for x in b], dtype=torch.long).to(device)\n",
    "), num_workers=0)\n",
    "\n",
    "print('Dataloaders ready. train_pairs:', train_n, 'val_pairs:', val_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4ae35",
   "metadata": {},
   "source": [
    "### Evaluation helpers (Recall@K & MRR) and sampled softmax loss stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d9042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_on_validation(model, k=20):\n",
    "    model.eval()\n",
    "    hits = 0\n",
    "    rr_sum = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(val_prefixes.size(0)):\n",
    "            X = val_prefixes[i].unsqueeze(0).to(device)\n",
    "            target = int(val_targets[i].item())\n",
    "            logits, final = model(X)\n",
    "            scores = torch.matmul(final, model.item_emb.weight.t())\n",
    "            topk = scores.topk(k, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            total += 1\n",
    "            if target in topk:\n",
    "                hits += 1\n",
    "                rank_idx = int((topk == target).nonzero()[0]) + 1\n",
    "                rr_sum += 1.0 / rank_idx\n",
    "    recall_at_k = hits / total if total>0 else 0.0\n",
    "    mrr = rr_sum / total if total>0 else 0.0\n",
    "    return recall_at_k, mrr\n",
    "\n",
    "# Simple sampled softmax loss (reuse existing implementation if available)\n",
    "def sampled_softmax_loss(final, y, emb_weights, num_negatives=32):\n",
    "    # final: (B, D), emb_weights: (V, D)\n",
    "    # Build positives scores\n",
    "    pos_scores = (final * emb_weights[y]).sum(dim=1)  # assumes y is LongTensor of indices\n",
    "    # sample negatives uniformly\n",
    "    V = emb_weights.size(0)\n",
    "    batch = final.size(0)\n",
    "    neg_idx = torch.randint(0, V, (batch, num_negatives), device=final.device)\n",
    "    neg_w = emb_weights[neg_idx]  # (B, N, D)\n",
    "    neg_scores = (neg_w * final.unsqueeze(1)).sum(dim=2)  # (B, N)\n",
    "    # combine\n",
    "    logits = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)  # (B, 1+N)\n",
    "    labels = torch.zeros(batch, dtype=torch.long, device=final.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "print('Helpers ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4a893",
   "metadata": {},
   "source": [
    "### Fine-tune loop with freeze/unfreeze, early stopping, saving best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1e3c505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16032\\1609488020.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16032\\1609488020.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16032\\1609488020.py:42: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16032\\1609488020.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameter names (sample): ['out.weight']\n",
      "ERROR: computed loss does not require grad. Diagnostics:\n",
      "  - number of trainable params: 1\n",
      "  - trainable param names: ['out.weight']\n",
      "Attempting to unfreeze all parameters and reinitialize optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16032\\2064896803.py:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rank_idx = int((topk == target).nonzero()[0]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FT] epoch 0 train_loss=4.2654 val_rec@20=0.0189 val_mrr=0.0021 time=1.1s\n",
      "Saved checkpoint: ..\\models\\mars_finetune_epoch0.pt\n",
      "New best val_rec@20: 0.018907563025210083\n",
      "[FT] epoch 1 train_loss=4.0094 val_rec@20=0.0252 val_mrr=0.0021 time=1.0s\n",
      "Saved checkpoint: ..\\models\\mars_finetune_epoch1.pt\n",
      "New best val_rec@20: 0.025210084033613446\n",
      "[FT] epoch 2 train_loss=3.8746 val_rec@20=0.0189 val_mrr=0.0018 time=1.0s\n",
      "Saved checkpoint: ..\\models\\mars_finetune_epoch2.pt\n",
      "No improvement, patience 1 / 4\n",
      "[FT] epoch 3 train_loss=3.7910 val_rec@20=0.0189 val_mrr=0.0020 time=1.0s\n",
      "Saved checkpoint: ..\\models\\mars_finetune_epoch3.pt\n",
      "No improvement, patience 2 / 4\n",
      "Unfreezing encoder and reinitializing optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16032\\1609488020.py:85: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FT] epoch 4 train_loss=3.7791 val_rec@20=0.0189 val_mrr=0.0025 time=1.0s\n",
      "Saved checkpoint: ..\\models\\mars_finetune_epoch4.pt\n",
      "No improvement, patience 3 / 4\n",
      "[FT] epoch 5 train_loss=3.6720 val_rec@20=0.0189 val_mrr=0.0031 time=1.0s\n",
      "Saved checkpoint: ..\\models\\mars_finetune_epoch5.pt\n",
      "No improvement, patience 4 / 4\n",
      "Early stopping triggered\n",
      "Saved best model to mars_finetune_best.pt\n",
      "Saved encoder-only to mars_encoder_only.pt\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer for parameters that require grad; if none, fall back to all params\n",
    "trainable_params = [p for p in mars_model.parameters() if p.requires_grad]\n",
    "if len(trainable_params) == 0:\n",
    "    print('Warning: no trainable params found. Unfreezing output head (out) and using all params for optimizer.')\n",
    "    # ensure at least the output head is trainable\n",
    "    for name, p in mars_model.named_parameters():\n",
    "        if name.startswith('out'):\n",
    "            p.requires_grad = True\n",
    "    trainable_params = [p for p in mars_model.parameters() if p.requires_grad]\n",
    "\n",
    "opt = torch.optim.AdamW(trainable_params, lr=FT_LR, weight_decay=1e-6)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
    "\n",
    "# Sanity: print trainable parameter names\n",
    "trainable_names = [name for name, p in mars_model.named_parameters() if p.requires_grad]\n",
    "print('Trainable parameter names (sample):', trainable_names[:20])\n",
    "\n",
    "best_val = -1.0\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(FT_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    mars_model.train()\n",
    "    running = 0.0\n",
    "    steps = 0\n",
    "    for step, (X, L, y) in enumerate(mars_loader):\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "            logits, final = mars_model(X)\n",
    "            loss = sampled_softmax_loss(final, y, mars_model.item_emb.weight, num_negatives=FT_NEG)\n",
    "\n",
    "        # If loss has no grad_fn (no trainable params), raise informative error and try to recover\n",
    "        if not getattr(loss, 'requires_grad', False):\n",
    "            print('ERROR: computed loss does not require grad. Diagnostics:')\n",
    "            print('  - number of trainable params:', len([p for p in mars_model.parameters() if p.requires_grad]))\n",
    "            print('  - trainable param names:', [n for n, p in mars_model.named_parameters() if p.requires_grad])\n",
    "            # Attempt to recover by unfreezing all params and reinitializing optimizer\n",
    "            print('Attempting to unfreeze all parameters and reinitialize optimizer...')\n",
    "            for p in mars_model.parameters():\n",
    "                p.requires_grad = True\n",
    "            opt = torch.optim.AdamW(mars_model.parameters(), lr=FT_LR, weight_decay=1e-6)\n",
    "            scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
    "            # recompute loss once more (forward again)\n",
    "            with torch.cuda.amp.autocast(enabled=FP16):\n",
    "                logits, final = mars_model(X)\n",
    "                loss = sampled_softmax_loss(final, y, mars_model.item_emb.weight, num_negatives=FT_NEG)\n",
    "            if not getattr(loss, 'requires_grad', False):\n",
    "                raise RuntimeError('Recovery failed: loss still does not require grad after unfreezing. Please check model parameter requires_grad flags.')\n",
    "\n",
    "        opt.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        running += float(loss.item())\n",
    "        steps += 1\n",
    "    train_time = time.time() - t0\n",
    "    avg_loss = running / max(1, steps)\n",
    "    val_recall, val_mrr = evaluate_on_validation(mars_model)\n",
    "    print(f\"[FT] epoch {epoch} train_loss={avg_loss:.4f} val_rec@20={val_recall:.4f} val_mrr={val_mrr:.4f} time={train_time:.1f}s\")\n",
    "\n",
    "    # save checkpoint\n",
    "    ckpt = CKPT_DIR / f\"mars_finetune_epoch{epoch}.pt\"\n",
    "    torch.save({\"epoch\": epoch, \"model_state\": mars_model.state_dict(), \"opt_state\": opt.state_dict()}, ckpt)\n",
    "    print('Saved checkpoint:', ckpt)\n",
    "\n",
    "    # early stopping\n",
    "    if val_recall > best_val:\n",
    "        best_val = val_recall\n",
    "        best_state = deepcopy(mars_model.state_dict())\n",
    "        no_improve = 0\n",
    "        print('New best val_rec@20:', best_val)\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print('No improvement, patience', no_improve, '/', EARLY_STOPPING_PATIENCE)\n",
    "        if no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print('Early stopping triggered')\n",
    "            break\n",
    "\n",
    "    # unfreeze after UNFREEZE_AFTER\n",
    "    if epoch == UNFREEZE_AFTER:\n",
    "        print('Unfreezing encoder and reinitializing optimizer')\n",
    "        for p in mars_model.parameters():\n",
    "            p.requires_grad = True\n",
    "        opt = torch.optim.AdamW(mars_model.parameters(), lr=FT_LR, weight_decay=1e-6)\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
    "\n",
    "# restore best\n",
    "if best_state is not None:\n",
    "    mars_model.load_state_dict(best_state)\n",
    "    torch.save({'model_state': best_state}, CKPT_DIR / 'mars_finetune_best.pt')\n",
    "    print('Saved best model to mars_finetune_best.pt')\n",
    "\n",
    "# save encoder-only\n",
    "encoder_state = {k:v for k,v in mars_model.state_dict().items() if k.startswith('encoder') or k.startswith('item_emb') or k.startswith('pos_emb')}\n",
    "torch.save(encoder_state, CKPT_DIR / 'mars_encoder_only.pt')\n",
    "print('Saved encoder-only to mars_encoder_only.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f88a4",
   "metadata": {},
   "source": [
    "### Final evaluation on test pairs (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63792418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mars_test_pairs.parquet — create a test split and run this cell later\n"
     ]
    }
   ],
   "source": [
    "TEST_PAIRS = DATA_DIR / 'mars_test_pairs.parquet'\n",
    "if not TEST_PAIRS.exists():\n",
    "    print('No mars_test_pairs.parquet — create a test split and run this cell later')\n",
    "else:\n",
    "    df_test = pd.read_parquet(TEST_PAIRS)\n",
    "    mars_model.eval()\n",
    "    K = 20\n",
    "    hits = 0\n",
    "    rr_sum = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, r in df_test.iterrows():\n",
    "            pref = r['prefix'] if isinstance(r['prefix'], str) else ''\n",
    "            pref_ids = [ item2id_mars.get(x,0) for x in pref.split() ] if pref else []\n",
    "            if len(pref_ids) > MAX_PREFIX_LEN:\n",
    "                pref_ids = pref_ids[-MAX_PREFIX_LEN:]\n",
    "            padded = [0]*(MAX_PREFIX_LEN-len(pref_ids)) + pref_ids\n",
    "            X = torch.LongTensor([padded]).to(device)\n",
    "            logits, final = mars_model(X)\n",
    "            scores = torch.matmul(final, mars_model.item_emb.weight.t())\n",
    "            topk = scores.topk(K, dim=1).indices.squeeze(0).cpu().numpy()\n",
    "            target = item2id_mars.get(str(r['target']), 0)\n",
    "            total += 1\n",
    "            if target in topk:\n",
    "                hits += 1\n",
    "                rank = int((topk == target).nonzero()[0]) + 1\n",
    "                rr_sum += 1.0 / rank\n",
    "    recall_at_k = hits / total if total>0 else 0.0\n",
    "    mrr = rr_sum / total if total>0 else 0.0\n",
    "    print(f'Final Eval Recall@{K}: {recall_at_k:.4f}, MRR: {mrr:.4f} (n={total})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
