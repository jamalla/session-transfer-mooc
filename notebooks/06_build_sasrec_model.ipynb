{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd84432b",
   "metadata": {},
   "source": [
    "### A — Config & device check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee56091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\n"
     ]
    }
   ],
   "source": [
    "# Quick (unsafe) workaround to avoid the libiomp5md.dll crash.\n",
    "# Use this only to continue working in the notebook quickly.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "print(\"Set KMP_DUPLICATE_LIB_OK=TRUE — restart kernel and re-run cells now.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ca9b260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "shards = sorted((DATA_DIR/\"tensor_shards\").glob(\"shard_*.pt\"))\n",
    "len(shards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "888dcd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU Monitor (GTX 1650) ===\n",
      "Utilization:      20%\n",
      "Memory Used:      2202 / 4096 MB\n",
      "Temperature:      39°C\n",
      "Effective Free:   1894 MB\n",
      "Monitoring every 1.0s... (press stop button to end)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m         time.sleep(interval)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Start monitoring indefinitely (stop manually)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mmonitor_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mmonitor_gpu\u001b[39m\u001b[34m(interval, runtime)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMonitor finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m time.sleep(interval)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Live GPU monitor for Jupyter (Windows + NVIDIA)\n",
    "# Updates every ~1s. Safe to run in a separate cell while training runs in another.\n",
    "\n",
    "import subprocess, time, re\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def parse_nvidia_smi():\n",
    "    \"\"\"Returns dict with util %, mem_used MB, mem_total, temp C.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu',\n",
    "             '--format=csv,noheader,nounits']\n",
    "        ).decode('utf-8').strip()\n",
    "\n",
    "        util, mem_used, mem_total, temp = map(int, result.split(', '))\n",
    "        return {\n",
    "            \"util\": util,\n",
    "            \"mem_used\": mem_used,\n",
    "            \"mem_total\": mem_total,\n",
    "            \"temp\": temp\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def monitor_gpu(interval=1.0, runtime=None):\n",
    "    \"\"\"\n",
    "    interval: seconds between updates\n",
    "    runtime: if provided, monitor stops after X seconds\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        stats = parse_nvidia_smi()\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if \"error\" in stats:\n",
    "            print(\"NVIDIA-SMI Error:\", stats[\"error\"])\n",
    "        else:\n",
    "            print(f\"=== GPU Monitor (GTX 1650) ===\")\n",
    "            print(f\"Utilization:      {stats['util']}%\")\n",
    "            print(f\"Memory Used:      {stats['mem_used']} / {stats['mem_total']} MB\")\n",
    "            print(f\"Temperature:      {stats['temp']}°C\")\n",
    "            print(f\"Effective Free:   {stats['mem_total'] - stats['mem_used']} MB\")\n",
    "            print(f\"Monitoring every {interval}s... (press stop button to end)\")\n",
    "\n",
    "        if runtime and (time.time() - start) > runtime:\n",
    "            print(\"Monitor finished.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(interval)\n",
    "\n",
    "# Start monitoring indefinitely (stop manually)\n",
    "monitor_gpu(interval=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "508422f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 10 shards: shard_000.pt ... shard_009.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2588\\559748316.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch = torch.load(p)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2588\\3293107640.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 200 avg_loss 2.6186\n",
      "epoch 0 step 400 avg_loss 2.6394\n",
      "epoch 0 step 600 avg_loss 2.6488\n",
      "epoch 0 step 800 avg_loss 2.6383\n",
      "epoch 0 step 1000 avg_loss 2.6356\n",
      "epoch 0 step 1200 avg_loss 2.6683\n",
      "epoch 0 step 1400 avg_loss 2.6446\n",
      "epoch 0 step 1600 avg_loss 2.6434\n",
      "epoch 0 step 1800 avg_loss 2.6287\n",
      "epoch 0 step 2000 avg_loss 2.6343\n",
      "epoch 0 step 2200 avg_loss 2.6317\n",
      "epoch 0 step 2400 avg_loss 2.6245\n",
      "epoch 0 step 2600 avg_loss 2.6262\n",
      "epoch 0 step 2800 avg_loss 2.6138\n",
      "epoch 0 step 3000 avg_loss 2.6117\n",
      "epoch 0 step 3200 avg_loss 2.6021\n",
      "epoch 0 step 3400 avg_loss 2.6090\n",
      "epoch 0 step 3600 avg_loss 2.6032\n",
      "epoch 0 step 3800 avg_loss 2.5930\n",
      "epoch 0 step 4000 avg_loss 2.5958\n",
      "epoch 0 step 4200 avg_loss 2.6000\n",
      "epoch 0 step 4400 avg_loss 2.5997\n",
      "epoch 0 step 4600 avg_loss 2.5933\n",
      "epoch 0 step 4800 avg_loss 2.5925\n",
      "epoch 0 step 5000 avg_loss 2.5927\n",
      "epoch 0 step 5200 avg_loss 2.5892\n",
      "epoch 0 step 5400 avg_loss 2.5901\n",
      "epoch 0 step 5600 avg_loss 2.5913\n",
      "epoch 0 step 5800 avg_loss 2.5964\n",
      "epoch 0 step 6000 avg_loss 2.5968\n",
      "epoch 0 step 6200 avg_loss 2.5986\n",
      "epoch 0 step 6400 avg_loss 2.5974\n",
      "epoch 0 step 6600 avg_loss 2.6016\n",
      "epoch 0 step 6800 avg_loss 2.5984\n",
      "epoch 0 step 7000 avg_loss 2.6000\n",
      "epoch 0 step 7200 avg_loss 2.6020\n",
      "epoch 0 step 7400 avg_loss 2.6034\n",
      "epoch 0 step 7600 avg_loss 2.6023\n",
      "epoch 0 step 7800 avg_loss 2.6044\n",
      "epoch 0 step 8000 avg_loss 2.6010\n",
      "epoch 0 step 8200 avg_loss 2.6000\n",
      "epoch 0 step 8400 avg_loss 2.5978\n",
      "epoch 0 step 8600 avg_loss 2.5954\n",
      "epoch 0 step 8800 avg_loss 2.5936\n",
      "epoch 0 step 9000 avg_loss 2.5899\n",
      "epoch 0 step 9200 avg_loss 2.5874\n",
      "epoch 0 step 9400 avg_loss 2.5865\n",
      "epoch 0 step 9600 avg_loss 2.5881\n",
      "epoch 0 step 9800 avg_loss 2.5862\n",
      "epoch 0 step 10000 avg_loss 2.5851\n",
      "epoch 0 step 10200 avg_loss 2.5862\n",
      "epoch 0 step 10400 avg_loss 2.5847\n",
      "epoch 0 step 10600 avg_loss 2.5856\n",
      "epoch 0 step 10800 avg_loss 2.5817\n",
      "epoch 0 step 11000 avg_loss 2.5811\n",
      "epoch 0 step 11200 avg_loss 2.5801\n",
      "epoch 0 step 11400 avg_loss 2.5809\n",
      "epoch 0 step 11600 avg_loss 2.5830\n",
      "epoch 0 step 11800 avg_loss 2.5813\n",
      "epoch 0 step 12000 avg_loss 2.5793\n",
      "epoch 0 step 12200 avg_loss 2.5774\n",
      "epoch 0 step 12400 avg_loss 2.5767\n",
      "epoch 0 step 12600 avg_loss 2.5775\n",
      "epoch 0 step 12800 avg_loss 2.5744\n",
      "epoch 0 step 13000 avg_loss 2.5716\n",
      "epoch 0 step 13200 avg_loss 2.5716\n",
      "epoch 0 step 13400 avg_loss 2.5696\n",
      "epoch 0 step 13600 avg_loss 2.5691\n",
      "epoch 0 step 13800 avg_loss 2.5713\n",
      "epoch 0 step 14000 avg_loss 2.5676\n",
      "epoch 0 step 14200 avg_loss 2.5666\n",
      "epoch 0 step 14400 avg_loss 2.5648\n",
      "epoch 0 step 14600 avg_loss 2.5638\n",
      "epoch 0 step 14800 avg_loss 2.5632\n",
      "epoch 0 step 15000 avg_loss 2.5616\n",
      "epoch 0 step 15200 avg_loss 2.5581\n",
      "epoch 0 step 15400 avg_loss 2.5579\n",
      "epoch 0 step 15600 avg_loss 2.5563\n",
      "epoch 0 step 15800 avg_loss 2.5551\n",
      "epoch 0 step 16000 avg_loss 2.5542\n",
      "epoch 0 step 16200 avg_loss 2.5526\n",
      "epoch 0 step 16400 avg_loss 2.5511\n",
      "epoch 0 step 16600 avg_loss 2.5520\n",
      "epoch 0 step 16800 avg_loss 2.5488\n",
      "epoch 0 step 17000 avg_loss 2.5483\n",
      "epoch 0 step 17200 avg_loss 2.5445\n",
      "epoch 0 step 17400 avg_loss 2.5417\n",
      "epoch 0 step 17600 avg_loss 2.5405\n",
      "epoch 0 step 17800 avg_loss 2.5401\n",
      "epoch 0 step 18000 avg_loss 2.5397\n",
      "epoch 0 step 18200 avg_loss 2.5408\n",
      "epoch 0 step 18400 avg_loss 2.5393\n",
      "epoch 0 step 18600 avg_loss 2.5372\n",
      "epoch 0 step 18800 avg_loss 2.5352\n",
      "epoch 0 step 19000 avg_loss 2.5329\n",
      "epoch 0 step 19200 avg_loss 2.5314\n",
      "epoch 0 step 19400 avg_loss 2.5295\n",
      "epoch 0 step 19600 avg_loss 2.5283\n",
      "epoch 0 step 19800 avg_loss 2.5295\n",
      "epoch 0 step 20000 avg_loss 2.5278\n",
      "epoch 0 step 20200 avg_loss 2.5260\n",
      "epoch 0 step 20400 avg_loss 2.5242\n",
      "epoch 0 step 20600 avg_loss 2.5251\n",
      "epoch 0 step 20800 avg_loss 2.5226\n",
      "epoch 0 step 21000 avg_loss 2.5212\n",
      "epoch 0 step 21200 avg_loss 2.5209\n",
      "epoch 0 step 21400 avg_loss 2.5188\n",
      "epoch 0 step 21600 avg_loss 2.5172\n",
      "epoch 0 step 21800 avg_loss 2.5165\n",
      "epoch 0 step 22000 avg_loss 2.5155\n",
      "epoch 0 step 22200 avg_loss 2.5140\n",
      "epoch 0 step 22400 avg_loss 2.5131\n",
      "epoch 0 step 22600 avg_loss 2.5128\n",
      "epoch 0 step 22800 avg_loss 2.5118\n",
      "epoch 0 step 23000 avg_loss 2.5103\n",
      "epoch 0 step 23200 avg_loss 2.5093\n",
      "epoch 0 step 23400 avg_loss 2.5085\n",
      "epoch 0 step 23600 avg_loss 2.5052\n",
      "epoch 0 step 23800 avg_loss 2.5046\n",
      "epoch 0 step 24000 avg_loss 2.5025\n",
      "epoch 0 step 24200 avg_loss 2.5019\n",
      "epoch 0 step 24400 avg_loss 2.4977\n",
      "epoch 0 step 24600 avg_loss 2.4972\n",
      "epoch 0 step 24800 avg_loss 2.4955\n",
      "epoch 0 step 25000 avg_loss 2.4952\n",
      "epoch 0 step 25200 avg_loss 2.4949\n",
      "epoch 0 step 25400 avg_loss 2.4939\n",
      "epoch 0 step 25600 avg_loss 2.4926\n",
      "epoch 0 step 25800 avg_loss 2.4919\n",
      "epoch 0 step 26000 avg_loss 2.4900\n",
      "epoch 0 step 26200 avg_loss 2.4887\n",
      "epoch 0 step 26400 avg_loss 2.4879\n",
      "epoch 0 step 26600 avg_loss 2.4856\n",
      "epoch 0 step 26800 avg_loss 2.4849\n",
      "epoch 0 step 27000 avg_loss 2.4849\n",
      "epoch 0 step 27200 avg_loss 2.4837\n",
      "epoch 0 step 27400 avg_loss 2.4830\n",
      "epoch 0 step 27600 avg_loss 2.4833\n",
      "epoch 0 step 27800 avg_loss 2.4815\n",
      "epoch 0 step 28000 avg_loss 2.4801\n",
      "epoch 0 step 28200 avg_loss 2.4791\n",
      "epoch 0 step 28400 avg_loss 2.4788\n",
      "epoch 0 step 28600 avg_loss 2.4782\n",
      "epoch 0 step 28800 avg_loss 2.4764\n",
      "epoch 0 step 29000 avg_loss 2.4750\n",
      "epoch 0 step 29200 avg_loss 2.4746\n",
      "epoch 0 step 29400 avg_loss 2.4737\n",
      "epoch 0 step 29600 avg_loss 2.4740\n",
      "epoch 0 step 29800 avg_loss 2.4737\n",
      "epoch 0 step 30000 avg_loss 2.4730\n",
      "epoch 0 step 30200 avg_loss 2.4716\n",
      "epoch 0 step 30400 avg_loss 2.4709\n",
      "epoch 0 step 30600 avg_loss 2.4704\n",
      "epoch 0 step 30800 avg_loss 2.4690\n",
      "epoch 0 step 31000 avg_loss 2.4661\n",
      "epoch 0 step 31200 avg_loss 2.4657\n",
      "epoch 0 step 31400 avg_loss 2.4650\n",
      "epoch 0 step 31600 avg_loss 2.4645\n",
      "epoch 0 step 31800 avg_loss 2.4630\n",
      "epoch 0 step 32000 avg_loss 2.4627\n",
      "epoch 0 step 32200 avg_loss 2.4617\n",
      "epoch 0 step 32400 avg_loss 2.4621\n",
      "epoch 0 step 32600 avg_loss 2.4621\n",
      "epoch 0 step 32800 avg_loss 2.4622\n",
      "epoch 0 step 33000 avg_loss 2.4619\n",
      "epoch 0 step 33200 avg_loss 2.4618\n",
      "epoch 0 step 33400 avg_loss 2.4618\n",
      "epoch 0 step 33600 avg_loss 2.4603\n",
      "epoch 0 step 33800 avg_loss 2.4595\n",
      "epoch 0 step 34000 avg_loss 2.4589\n",
      "epoch 0 step 34200 avg_loss 2.4589\n",
      "epoch 0 step 34400 avg_loss 2.4582\n",
      "epoch 0 step 34600 avg_loss 2.4573\n",
      "epoch 0 step 34800 avg_loss 2.4575\n",
      "epoch 0 step 35000 avg_loss 2.4566\n",
      "epoch 0 step 35200 avg_loss 2.4554\n",
      "epoch 0 step 35400 avg_loss 2.4549\n",
      "epoch 0 step 35600 avg_loss 2.4540\n",
      "epoch 0 step 35800 avg_loss 2.4542\n",
      "epoch 0 step 36000 avg_loss 2.4531\n",
      "epoch 0 step 36200 avg_loss 2.4519\n",
      "epoch 0 step 36400 avg_loss 2.4513\n",
      "epoch 0 step 36600 avg_loss 2.4508\n",
      "epoch 0 step 36800 avg_loss 2.4499\n",
      "epoch 0 step 37000 avg_loss 2.4492\n",
      "epoch 0 step 37200 avg_loss 2.4492\n",
      "epoch 0 step 37400 avg_loss 2.4483\n",
      "epoch 0 step 37600 avg_loss 2.4490\n",
      "epoch 0 step 37800 avg_loss 2.4482\n",
      "epoch 0 step 38000 avg_loss 2.4472\n",
      "epoch 0 step 38200 avg_loss 2.4473\n",
      "epoch 0 step 38400 avg_loss 2.4470\n",
      "epoch 0 step 38600 avg_loss 2.4460\n",
      "epoch 0 step 38800 avg_loss 2.4451\n",
      "epoch 0 step 39000 avg_loss 2.4445\n",
      "Epoch 0 finished avg_loss 2.4439\n",
      "Epoch 0 done — time 1126.2s, avg_loss 2.4439\n",
      "Samples processed (approx/exact): 2500000, throughput: 2219.8 samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2588\\2125537838.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(sp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ..\\models\\sasrec_warmup_top200000_epoch0.pt\n",
      "Running quick validation on 2 shards...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2588\\2125537838.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation (approx) loss: 2.2538 over 201 steps\n",
      "epoch 1 step 200 avg_loss 2.2189\n",
      "epoch 1 step 400 avg_loss 2.2712\n",
      "epoch 1 step 600 avg_loss 2.2648\n",
      "epoch 1 step 800 avg_loss 2.2574\n",
      "epoch 1 step 1000 avg_loss 2.2664\n",
      "epoch 1 step 1200 avg_loss 2.2437\n",
      "epoch 1 step 1400 avg_loss 2.2589\n",
      "epoch 1 step 1600 avg_loss 2.2631\n",
      "epoch 1 step 1800 avg_loss 2.2756\n",
      "epoch 1 step 2000 avg_loss 2.2755\n",
      "epoch 1 step 2200 avg_loss 2.2745\n",
      "epoch 1 step 2400 avg_loss 2.2852\n",
      "epoch 1 step 2600 avg_loss 2.2853\n",
      "epoch 1 step 2800 avg_loss 2.2915\n",
      "epoch 1 step 3000 avg_loss 2.2845\n",
      "epoch 1 step 3200 avg_loss 2.2882\n",
      "epoch 1 step 3400 avg_loss 2.2887\n",
      "epoch 1 step 3600 avg_loss 2.2960\n",
      "epoch 1 step 3800 avg_loss 2.3039\n",
      "epoch 1 step 4000 avg_loss 2.3047\n",
      "epoch 1 step 4200 avg_loss 2.3029\n",
      "epoch 1 step 4400 avg_loss 2.3060\n",
      "epoch 1 step 4600 avg_loss 2.3048\n",
      "epoch 1 step 4800 avg_loss 2.2999\n",
      "epoch 1 step 5000 avg_loss 2.2956\n",
      "epoch 1 step 5200 avg_loss 2.2977\n",
      "epoch 1 step 5400 avg_loss 2.2963\n",
      "epoch 1 step 5600 avg_loss 2.2938\n",
      "epoch 1 step 5800 avg_loss 2.2942\n",
      "epoch 1 step 6000 avg_loss 2.2970\n",
      "epoch 1 step 6200 avg_loss 2.2993\n",
      "epoch 1 step 6400 avg_loss 2.3019\n",
      "epoch 1 step 6600 avg_loss 2.2961\n",
      "epoch 1 step 6800 avg_loss 2.2990\n",
      "epoch 1 step 7000 avg_loss 2.3016\n",
      "epoch 1 step 7200 avg_loss 2.3002\n",
      "epoch 1 step 7400 avg_loss 2.2978\n",
      "epoch 1 step 7600 avg_loss 2.2986\n",
      "epoch 1 step 7800 avg_loss 2.2931\n",
      "epoch 1 step 8000 avg_loss 2.2934\n",
      "epoch 1 step 8200 avg_loss 2.2929\n",
      "epoch 1 step 8400 avg_loss 2.2888\n",
      "epoch 1 step 8600 avg_loss 2.2919\n",
      "epoch 1 step 8800 avg_loss 2.2904\n",
      "epoch 1 step 9000 avg_loss 2.2912\n",
      "epoch 1 step 9200 avg_loss 2.2944\n",
      "epoch 1 step 9400 avg_loss 2.2958\n",
      "epoch 1 step 9600 avg_loss 2.2959\n",
      "epoch 1 step 9800 avg_loss 2.2961\n",
      "epoch 1 step 10000 avg_loss 2.2976\n",
      "epoch 1 step 10200 avg_loss 2.2943\n",
      "epoch 1 step 10400 avg_loss 2.2919\n",
      "epoch 1 step 10600 avg_loss 2.2918\n",
      "epoch 1 step 10800 avg_loss 2.2935\n",
      "epoch 1 step 11000 avg_loss 2.2927\n",
      "epoch 1 step 11200 avg_loss 2.2915\n",
      "epoch 1 step 11400 avg_loss 2.2926\n",
      "epoch 1 step 11600 avg_loss 2.2915\n",
      "epoch 1 step 11800 avg_loss 2.2885\n",
      "epoch 1 step 12000 avg_loss 2.2889\n",
      "epoch 1 step 12200 avg_loss 2.2885\n",
      "epoch 1 step 12400 avg_loss 2.2884\n",
      "epoch 1 step 12600 avg_loss 2.2877\n",
      "epoch 1 step 12800 avg_loss 2.2895\n",
      "epoch 1 step 13000 avg_loss 2.2881\n",
      "epoch 1 step 13200 avg_loss 2.2876\n",
      "epoch 1 step 13400 avg_loss 2.2871\n",
      "epoch 1 step 13600 avg_loss 2.2846\n",
      "epoch 1 step 13800 avg_loss 2.2859\n",
      "epoch 1 step 14000 avg_loss 2.2864\n",
      "epoch 1 step 14200 avg_loss 2.2847\n",
      "epoch 1 step 14400 avg_loss 2.2839\n",
      "epoch 1 step 14600 avg_loss 2.2825\n",
      "epoch 1 step 14800 avg_loss 2.2821\n",
      "epoch 1 step 15000 avg_loss 2.2807\n",
      "epoch 1 step 15200 avg_loss 2.2801\n",
      "epoch 1 step 15400 avg_loss 2.2773\n",
      "epoch 1 step 15600 avg_loss 2.2763\n",
      "epoch 1 step 15800 avg_loss 2.2788\n",
      "epoch 1 step 16000 avg_loss 2.2782\n",
      "epoch 1 step 16200 avg_loss 2.2750\n",
      "epoch 1 step 16400 avg_loss 2.2767\n",
      "epoch 1 step 16600 avg_loss 2.2763\n",
      "epoch 1 step 16800 avg_loss 2.2765\n",
      "epoch 1 step 17000 avg_loss 2.2746\n",
      "epoch 1 step 17200 avg_loss 2.2738\n",
      "epoch 1 step 17400 avg_loss 2.2733\n",
      "epoch 1 step 17600 avg_loss 2.2714\n",
      "epoch 1 step 17800 avg_loss 2.2714\n",
      "epoch 1 step 18000 avg_loss 2.2716\n",
      "epoch 1 step 18200 avg_loss 2.2716\n",
      "epoch 1 step 18400 avg_loss 2.2707\n",
      "epoch 1 step 18600 avg_loss 2.2708\n",
      "epoch 1 step 18800 avg_loss 2.2704\n",
      "epoch 1 step 19000 avg_loss 2.2701\n",
      "epoch 1 step 19200 avg_loss 2.2685\n",
      "epoch 1 step 19400 avg_loss 2.2680\n",
      "epoch 1 step 19600 avg_loss 2.2690\n",
      "epoch 1 step 19800 avg_loss 2.2696\n",
      "epoch 1 step 20000 avg_loss 2.2698\n",
      "epoch 1 step 20200 avg_loss 2.2689\n",
      "epoch 1 step 20400 avg_loss 2.2678\n",
      "epoch 1 step 20600 avg_loss 2.2685\n",
      "epoch 1 step 20800 avg_loss 2.2672\n",
      "epoch 1 step 21000 avg_loss 2.2674\n",
      "epoch 1 step 21200 avg_loss 2.2682\n",
      "epoch 1 step 21400 avg_loss 2.2678\n",
      "epoch 1 step 21600 avg_loss 2.2695\n",
      "epoch 1 step 21800 avg_loss 2.2700\n",
      "epoch 1 step 22000 avg_loss 2.2698\n",
      "epoch 1 step 22200 avg_loss 2.2707\n",
      "epoch 1 step 22400 avg_loss 2.2698\n",
      "epoch 1 step 22600 avg_loss 2.2703\n",
      "epoch 1 step 22800 avg_loss 2.2704\n",
      "epoch 1 step 23000 avg_loss 2.2714\n",
      "epoch 1 step 23200 avg_loss 2.2712\n",
      "epoch 1 step 23400 avg_loss 2.2722\n",
      "epoch 1 step 23600 avg_loss 2.2699\n",
      "epoch 1 step 23800 avg_loss 2.2698\n",
      "epoch 1 step 24000 avg_loss 2.2683\n",
      "epoch 1 step 24200 avg_loss 2.2681\n",
      "epoch 1 step 24400 avg_loss 2.2645\n",
      "epoch 1 step 24600 avg_loss 2.2645\n",
      "epoch 1 step 24800 avg_loss 2.2634\n",
      "epoch 1 step 25000 avg_loss 2.2636\n",
      "epoch 1 step 25200 avg_loss 2.2638\n",
      "epoch 1 step 25400 avg_loss 2.2634\n",
      "epoch 1 step 25600 avg_loss 2.2625\n",
      "epoch 1 step 25800 avg_loss 2.2624\n",
      "epoch 1 step 26000 avg_loss 2.2610\n",
      "epoch 1 step 26200 avg_loss 2.2602\n",
      "epoch 1 step 26400 avg_loss 2.2599\n",
      "epoch 1 step 26600 avg_loss 2.2581\n",
      "epoch 1 step 26800 avg_loss 2.2579\n",
      "epoch 1 step 27000 avg_loss 2.2584\n",
      "epoch 1 step 27200 avg_loss 2.2577\n",
      "epoch 1 step 27400 avg_loss 2.2574\n",
      "epoch 1 step 27600 avg_loss 2.2582\n",
      "epoch 1 step 27800 avg_loss 2.2568\n",
      "epoch 1 step 28000 avg_loss 2.2560\n",
      "epoch 1 step 28200 avg_loss 2.2554\n",
      "epoch 1 step 28400 avg_loss 2.2555\n",
      "epoch 1 step 28600 avg_loss 2.2553\n",
      "epoch 1 step 28800 avg_loss 2.2540\n",
      "epoch 1 step 29000 avg_loss 2.2531\n",
      "epoch 1 step 29200 avg_loss 2.2531\n",
      "epoch 1 step 29400 avg_loss 2.2526\n",
      "epoch 1 step 29600 avg_loss 2.2533\n",
      "epoch 1 step 29800 avg_loss 2.2536\n",
      "epoch 1 step 30000 avg_loss 2.2533\n",
      "epoch 1 step 30200 avg_loss 2.2525\n",
      "epoch 1 step 30400 avg_loss 2.2522\n",
      "epoch 1 step 30600 avg_loss 2.2521\n",
      "epoch 1 step 30800 avg_loss 2.2511\n",
      "epoch 1 step 31000 avg_loss 2.2489\n",
      "epoch 1 step 31200 avg_loss 2.2488\n",
      "epoch 1 step 31400 avg_loss 2.2483\n",
      "epoch 1 step 31600 avg_loss 2.2480\n",
      "epoch 1 step 31800 avg_loss 2.2485\n",
      "epoch 1 step 32000 avg_loss 2.2479\n",
      "epoch 1 step 32200 avg_loss 2.2487\n",
      "epoch 1 step 32400 avg_loss 2.2476\n",
      "epoch 1 step 32600 avg_loss 2.2480\n",
      "epoch 1 step 32800 avg_loss 2.2465\n",
      "epoch 1 step 33000 avg_loss 2.2457\n",
      "epoch 1 step 33200 avg_loss 2.2455\n",
      "epoch 1 step 33400 avg_loss 2.2457\n",
      "epoch 1 step 33600 avg_loss 2.2460\n",
      "epoch 1 step 33800 avg_loss 2.2470\n",
      "epoch 1 step 34000 avg_loss 2.2466\n",
      "epoch 1 step 34200 avg_loss 2.2460\n",
      "epoch 1 step 34400 avg_loss 2.2455\n",
      "epoch 1 step 34600 avg_loss 2.2445\n",
      "epoch 1 step 34800 avg_loss 2.2442\n",
      "epoch 1 step 35000 avg_loss 2.2437\n",
      "epoch 1 step 35200 avg_loss 2.2433\n",
      "epoch 1 step 35400 avg_loss 2.2429\n",
      "epoch 1 step 35600 avg_loss 2.2428\n",
      "epoch 1 step 35800 avg_loss 2.2427\n",
      "epoch 1 step 36000 avg_loss 2.2434\n",
      "epoch 1 step 36200 avg_loss 2.2430\n",
      "epoch 1 step 36400 avg_loss 2.2423\n",
      "epoch 1 step 36600 avg_loss 2.2425\n",
      "epoch 1 step 36800 avg_loss 2.2422\n",
      "epoch 1 step 37000 avg_loss 2.2423\n",
      "epoch 1 step 37200 avg_loss 2.2436\n",
      "epoch 1 step 37400 avg_loss 2.2429\n",
      "epoch 1 step 37600 avg_loss 2.2425\n",
      "epoch 1 step 37800 avg_loss 2.2422\n",
      "epoch 1 step 38000 avg_loss 2.2423\n",
      "epoch 1 step 38200 avg_loss 2.2423\n",
      "epoch 1 step 38400 avg_loss 2.2423\n",
      "epoch 1 step 38600 avg_loss 2.2414\n",
      "epoch 1 step 38800 avg_loss 2.2409\n",
      "epoch 1 step 39000 avg_loss 2.2411\n",
      "Epoch 1 finished avg_loss 2.2412\n",
      "Epoch 1 done — time 1084.7s, avg_loss 2.2412\n",
      "Samples processed (approx/exact): 2500000, throughput: 2304.7 samples/sec\n",
      "Saved checkpoint: ..\\models\\sasrec_warmup_top200000_epoch1.pt\n",
      "Running quick validation on 2 shards...\n",
      "Validation (approx) loss: 2.1743 over 201 steps\n",
      "epoch 2 step 200 avg_loss 2.1356\n",
      "epoch 2 step 400 avg_loss 2.1759\n",
      "epoch 2 step 600 avg_loss 2.1800\n",
      "epoch 2 step 800 avg_loss 2.1758\n",
      "epoch 2 step 1000 avg_loss 2.2120\n",
      "epoch 2 step 1200 avg_loss 2.1877\n",
      "epoch 2 step 1400 avg_loss 2.1877\n",
      "epoch 2 step 1600 avg_loss 2.1531\n",
      "epoch 2 step 1800 avg_loss 2.1545\n",
      "epoch 2 step 2000 avg_loss 2.1544\n",
      "epoch 2 step 2200 avg_loss 2.1676\n",
      "epoch 2 step 2400 avg_loss 2.1760\n",
      "epoch 2 step 2600 avg_loss 2.1871\n",
      "epoch 2 step 2800 avg_loss 2.1849\n",
      "epoch 2 step 3000 avg_loss 2.1766\n",
      "epoch 2 step 3200 avg_loss 2.1734\n",
      "epoch 2 step 3400 avg_loss 2.1690\n",
      "epoch 2 step 3600 avg_loss 2.1647\n",
      "epoch 2 step 3800 avg_loss 2.1631\n",
      "epoch 2 step 4000 avg_loss 2.1654\n",
      "epoch 2 step 4200 avg_loss 2.1691\n",
      "epoch 2 step 4400 avg_loss 2.1627\n",
      "epoch 2 step 4600 avg_loss 2.1583\n",
      "epoch 2 step 4800 avg_loss 2.1578\n",
      "epoch 2 step 5000 avg_loss 2.1585\n",
      "epoch 2 step 5200 avg_loss 2.1567\n",
      "epoch 2 step 5400 avg_loss 2.1537\n",
      "epoch 2 step 5600 avg_loss 2.1498\n",
      "epoch 2 step 5800 avg_loss 2.1521\n",
      "epoch 2 step 6000 avg_loss 2.1506\n",
      "epoch 2 step 6200 avg_loss 2.1548\n",
      "epoch 2 step 6400 avg_loss 2.1565\n",
      "epoch 2 step 6600 avg_loss 2.1564\n",
      "epoch 2 step 6800 avg_loss 2.1537\n",
      "epoch 2 step 7000 avg_loss 2.1533\n",
      "epoch 2 step 7200 avg_loss 2.1545\n",
      "epoch 2 step 7400 avg_loss 2.1488\n",
      "epoch 2 step 7600 avg_loss 2.1440\n",
      "epoch 2 step 7800 avg_loss 2.1431\n",
      "epoch 2 step 8000 avg_loss 2.1439\n",
      "epoch 2 step 8200 avg_loss 2.1438\n",
      "epoch 2 step 8400 avg_loss 2.1406\n",
      "epoch 2 step 8600 avg_loss 2.1445\n",
      "epoch 2 step 8800 avg_loss 2.1437\n",
      "epoch 2 step 9000 avg_loss 2.1447\n",
      "epoch 2 step 9200 avg_loss 2.1489\n",
      "epoch 2 step 9400 avg_loss 2.1512\n",
      "epoch 2 step 9600 avg_loss 2.1517\n",
      "epoch 2 step 9800 avg_loss 2.1525\n",
      "epoch 2 step 10000 avg_loss 2.1538\n",
      "epoch 2 step 10200 avg_loss 2.1513\n",
      "epoch 2 step 10400 avg_loss 2.1496\n",
      "epoch 2 step 10600 avg_loss 2.1501\n",
      "epoch 2 step 10800 avg_loss 2.1523\n",
      "epoch 2 step 11000 avg_loss 2.1522\n",
      "epoch 2 step 11200 avg_loss 2.1516\n",
      "epoch 2 step 11400 avg_loss 2.1530\n",
      "epoch 2 step 11600 avg_loss 2.1524\n",
      "epoch 2 step 11800 avg_loss 2.1487\n",
      "epoch 2 step 12000 avg_loss 2.1471\n",
      "epoch 2 step 12200 avg_loss 2.1473\n",
      "epoch 2 step 12400 avg_loss 2.1458\n",
      "epoch 2 step 12600 avg_loss 2.1420\n",
      "epoch 2 step 12800 avg_loss 2.1392\n",
      "epoch 2 step 13000 avg_loss 2.1407\n",
      "epoch 2 step 13200 avg_loss 2.1405\n",
      "epoch 2 step 13400 avg_loss 2.1402\n",
      "epoch 2 step 13600 avg_loss 2.1407\n",
      "epoch 2 step 13800 avg_loss 2.1407\n",
      "epoch 2 step 14000 avg_loss 2.1398\n",
      "epoch 2 step 14200 avg_loss 2.1400\n",
      "epoch 2 step 14400 avg_loss 2.1379\n",
      "epoch 2 step 14600 avg_loss 2.1372\n",
      "epoch 2 step 14800 avg_loss 2.1365\n",
      "epoch 2 step 15000 avg_loss 2.1343\n",
      "epoch 2 step 15200 avg_loss 2.1357\n",
      "epoch 2 step 15400 avg_loss 2.1353\n",
      "epoch 2 step 15600 avg_loss 2.1349\n",
      "epoch 2 step 15800 avg_loss 2.1342\n",
      "epoch 2 step 16000 avg_loss 2.1347\n",
      "epoch 2 step 16200 avg_loss 2.1358\n",
      "epoch 2 step 16400 avg_loss 2.1360\n",
      "epoch 2 step 16600 avg_loss 2.1364\n",
      "epoch 2 step 16800 avg_loss 2.1389\n",
      "epoch 2 step 17000 avg_loss 2.1380\n",
      "epoch 2 step 17200 avg_loss 2.1379\n",
      "epoch 2 step 17400 avg_loss 2.1370\n",
      "epoch 2 step 17600 avg_loss 2.1375\n",
      "epoch 2 step 17800 avg_loss 2.1375\n",
      "epoch 2 step 18000 avg_loss 2.1375\n",
      "epoch 2 step 18200 avg_loss 2.1377\n",
      "epoch 2 step 18400 avg_loss 2.1377\n",
      "epoch 2 step 18600 avg_loss 2.1370\n",
      "epoch 2 step 18800 avg_loss 2.1361\n",
      "epoch 2 step 19000 avg_loss 2.1366\n",
      "epoch 2 step 19200 avg_loss 2.1361\n",
      "epoch 2 step 19400 avg_loss 2.1345\n",
      "epoch 2 step 19600 avg_loss 2.1351\n",
      "epoch 2 step 19800 avg_loss 2.1359\n",
      "epoch 2 step 20000 avg_loss 2.1363\n",
      "epoch 2 step 20200 avg_loss 2.1357\n",
      "epoch 2 step 20400 avg_loss 2.1350\n",
      "epoch 2 step 20600 avg_loss 2.1361\n",
      "epoch 2 step 20800 avg_loss 2.1352\n",
      "epoch 2 step 21000 avg_loss 2.1357\n",
      "epoch 2 step 21200 avg_loss 2.1366\n",
      "epoch 2 step 21400 avg_loss 2.1367\n",
      "epoch 2 step 21600 avg_loss 2.1388\n",
      "epoch 2 step 21800 avg_loss 2.1396\n",
      "epoch 2 step 22000 avg_loss 2.1396\n",
      "epoch 2 step 22200 avg_loss 2.1408\n",
      "epoch 2 step 22400 avg_loss 2.1403\n",
      "epoch 2 step 22600 avg_loss 2.1410\n",
      "epoch 2 step 22800 avg_loss 2.1412\n",
      "epoch 2 step 23000 avg_loss 2.1424\n",
      "epoch 2 step 23200 avg_loss 2.1424\n",
      "epoch 2 step 23400 avg_loss 2.1437\n",
      "epoch 2 step 23600 avg_loss 2.1432\n",
      "epoch 2 step 23800 avg_loss 2.1433\n",
      "epoch 2 step 24000 avg_loss 2.1428\n",
      "epoch 2 step 24200 avg_loss 2.1429\n",
      "epoch 2 step 24400 avg_loss 2.1427\n",
      "epoch 2 step 24600 avg_loss 2.1425\n",
      "epoch 2 step 24800 avg_loss 2.1414\n",
      "epoch 2 step 25000 avg_loss 2.1418\n",
      "epoch 2 step 25200 avg_loss 2.1429\n",
      "epoch 2 step 25400 avg_loss 2.1426\n",
      "epoch 2 step 25600 avg_loss 2.1427\n",
      "epoch 2 step 25800 avg_loss 2.1435\n",
      "epoch 2 step 26000 avg_loss 2.1439\n",
      "epoch 2 step 26200 avg_loss 2.1447\n",
      "epoch 2 step 26400 avg_loss 2.1435\n",
      "epoch 2 step 26600 avg_loss 2.1440\n",
      "epoch 2 step 26800 avg_loss 2.1434\n",
      "epoch 2 step 27000 avg_loss 2.1449\n",
      "epoch 2 step 27200 avg_loss 2.1460\n",
      "epoch 2 step 27400 avg_loss 2.1464\n",
      "epoch 2 step 27600 avg_loss 2.1482\n",
      "epoch 2 step 27800 avg_loss 2.1476\n",
      "epoch 2 step 28000 avg_loss 2.1472\n",
      "epoch 2 step 28200 avg_loss 2.1468\n",
      "epoch 2 step 28400 avg_loss 2.1482\n",
      "epoch 2 step 28600 avg_loss 2.1474\n",
      "epoch 2 step 28800 avg_loss 2.1472\n",
      "epoch 2 step 29000 avg_loss 2.1476\n",
      "epoch 2 step 29200 avg_loss 2.1467\n",
      "epoch 2 step 29400 avg_loss 2.1459\n",
      "epoch 2 step 29600 avg_loss 2.1465\n",
      "epoch 2 step 29800 avg_loss 2.1461\n",
      "epoch 2 step 30000 avg_loss 2.1459\n",
      "epoch 2 step 30200 avg_loss 2.1459\n",
      "epoch 2 step 30400 avg_loss 2.1466\n",
      "epoch 2 step 30600 avg_loss 2.1463\n",
      "epoch 2 step 30800 avg_loss 2.1459\n",
      "epoch 2 step 31000 avg_loss 2.1458\n",
      "epoch 2 step 31200 avg_loss 2.1460\n",
      "epoch 2 step 31400 avg_loss 2.1459\n",
      "epoch 2 step 31600 avg_loss 2.1460\n",
      "epoch 2 step 31800 avg_loss 2.1468\n",
      "epoch 2 step 32000 avg_loss 2.1467\n",
      "epoch 2 step 32200 avg_loss 2.1457\n",
      "epoch 2 step 32400 avg_loss 2.1457\n",
      "epoch 2 step 32600 avg_loss 2.1456\n",
      "epoch 2 step 32800 avg_loss 2.1455\n",
      "epoch 2 step 33000 avg_loss 2.1451\n",
      "epoch 2 step 33200 avg_loss 2.1451\n",
      "epoch 2 step 33400 avg_loss 2.1455\n",
      "epoch 2 step 33600 avg_loss 2.1466\n",
      "epoch 2 step 33800 avg_loss 2.1469\n",
      "epoch 2 step 34000 avg_loss 2.1465\n",
      "epoch 2 step 34200 avg_loss 2.1466\n",
      "epoch 2 step 34400 avg_loss 2.1471\n",
      "epoch 2 step 34600 avg_loss 2.1470\n",
      "epoch 2 step 34800 avg_loss 2.1469\n",
      "epoch 2 step 35000 avg_loss 2.1473\n",
      "epoch 2 step 35200 avg_loss 2.1463\n",
      "epoch 2 step 35400 avg_loss 2.1460\n",
      "epoch 2 step 35600 avg_loss 2.1459\n",
      "epoch 2 step 35800 avg_loss 2.1459\n",
      "epoch 2 step 36000 avg_loss 2.1467\n",
      "epoch 2 step 36200 avg_loss 2.1464\n",
      "epoch 2 step 36400 avg_loss 2.1458\n",
      "epoch 2 step 36600 avg_loss 2.1460\n",
      "epoch 2 step 36800 avg_loss 2.1458\n",
      "epoch 2 step 37000 avg_loss 2.1460\n",
      "epoch 2 step 37200 avg_loss 2.1474\n",
      "epoch 2 step 37400 avg_loss 2.1468\n",
      "epoch 2 step 37600 avg_loss 2.1464\n",
      "epoch 2 step 37800 avg_loss 2.1462\n",
      "epoch 2 step 38000 avg_loss 2.1463\n",
      "epoch 2 step 38200 avg_loss 2.1465\n",
      "epoch 2 step 38400 avg_loss 2.1465\n",
      "epoch 2 step 38600 avg_loss 2.1456\n",
      "epoch 2 step 38800 avg_loss 2.1452\n",
      "epoch 2 step 39000 avg_loss 2.1455\n",
      "Epoch 2 finished avg_loss 2.1456\n",
      "Epoch 2 done — time 1085.6s, avg_loss 2.1456\n",
      "Samples processed (approx/exact): 2500000, throughput: 2303.0 samples/sec\n",
      "Saved checkpoint: ..\\models\\sasrec_warmup_top200000_epoch2.pt\n",
      "Running quick validation on 2 shards...\n",
      "Validation (approx) loss: 2.3234 over 201 steps\n",
      "Warm-up training complete.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Updated warm-up training cell (replacement) ----------\n",
    "# Trains on first N shards, with correct reporting and resume support.\n",
    "import time, math\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Config (tweakable) ---\n",
    "NUM_WARMUP_SHARDS = 10        # warm-up shards\n",
    "EPOCHS = 3                    # number of epochs to run for warm-up\n",
    "SAVE_EVERY_EPOCH = True\n",
    "VALIDATION_SHARDS = []        # e.g., shard_paths[10:12] for small val set (empty -> no val)\n",
    "RESUME_CKPT = None            # path to checkpoint to resume, or None\n",
    "MAX_STEPS_PER_EPOCH = None    # set int to limit steps per epoch for quick iteration, or None\n",
    "\n",
    "# Ensure required objects exist\n",
    "shard_paths = sorted(SHARD_DIR.glob(\"shard_*.pt\"))\n",
    "if len(shard_paths) == 0:\n",
    "    raise RuntimeError(f\"No shards found in {SHARD_DIR}; run shard builder first.\")\n",
    "\n",
    "train_shards = shard_paths[:NUM_WARMUP_SHARDS]\n",
    "print(f\"Training on {len(train_shards)} shards: {train_shards[0].name} ... {train_shards[-1].name}\")\n",
    "\n",
    "# resume if needed\n",
    "start_epoch = 0\n",
    "if RESUME_CKPT:\n",
    "    ck = torch.load(RESUME_CKPT, map_location=device)\n",
    "    model.load_state_dict(ck[\"model_state\"])\n",
    "    optimizer.load_state_dict(ck[\"opt_state\"])\n",
    "    start_epoch = int(ck.get(\"epoch\", 0)) + 1\n",
    "    print(\"Resumed from\", RESUME_CKPT, \"starting epoch\", start_epoch)\n",
    "\n",
    "# dataloader factory for chosen shards\n",
    "def make_loader_for_shards(shards, batch_size=BATCH_SIZE):\n",
    "    ds = TensorShardDataset(shards, shuffle_shards=True)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, collate_fn=collate_shard_batch, num_workers=0)\n",
    "    return loader\n",
    "\n",
    "# helper to compute exact sample count in chosen shards\n",
    "def total_samples_in_shards(shard_list):\n",
    "    total = 0\n",
    "    for sp in shard_list:\n",
    "        d = torch.load(sp)\n",
    "        total += int(d[\"prefix\"].size(0))\n",
    "    return total\n",
    "\n",
    "# Run training\n",
    "for epoch in range(start_epoch, start_epoch + EPOCHS):\n",
    "    loader = make_loader_for_shards(train_shards, batch_size=BATCH_SIZE)\n",
    "    t0 = time.time()\n",
    "    # Train: if you want to limit iterations, pass max_steps=MAX_STEPS_PER_EPOCH to train_one_epoch\n",
    "    avg_loss, avg_rec = train_one_epoch(model, loader, optimizer, scaler, epoch, max_steps=MAX_STEPS_PER_EPOCH)\n",
    "    t1 = time.time()\n",
    "    elapsed = t1 - t0\n",
    "\n",
    "    # Compute processed sample count and throughput:\n",
    "    if MAX_STEPS_PER_EPOCH:\n",
    "        # approximate processed samples from steps * batch * accum\n",
    "        processed_steps = MAX_STEPS_PER_EPOCH\n",
    "        processed_samples = processed_steps * BATCH_SIZE * ACCUM_STEPS\n",
    "    else:\n",
    "        # exact count is all samples across used shards\n",
    "        processed_samples = total_samples_in_shards(train_shards)\n",
    "\n",
    "    throughput = processed_samples / elapsed if elapsed > 0 else float(\"nan\")\n",
    "\n",
    "    print(f\"Epoch {epoch} done — time {elapsed:.1f}s, avg_loss {avg_loss:.4f}\")\n",
    "    print(f\"Samples processed (approx/exact): {processed_samples}, throughput: {throughput:.1f} samples/sec\")\n",
    "\n",
    "    # save checkpoint\n",
    "    if SAVE_EVERY_EPOCH:\n",
    "        ckpt_name = f\"sasrec_warmup_top{TOP_N_ITEMS}_epoch{epoch}.pt\"\n",
    "        save_checkpoint(model, optimizer, epoch, name=f\"sasrec_warmup_top{TOP_N_ITEMS}_epoch{epoch}\")\n",
    "        print(\"Saved checkpoint:\", CKPT_DIR / ckpt_name)\n",
    "\n",
    "    # optional small validation (if validation shards provided)\n",
    "    if VALIDATION_SHARDS:\n",
    "        val_shards = VALIDATION_SHARDS\n",
    "    else:\n",
    "        # choose two shards after the train shards for a quick validation set if available\n",
    "        val_shards = shard_paths[NUM_WARMUP_SHARDS:NUM_WARMUP_SHARDS+2] if len(shard_paths) > NUM_WARMUP_SHARDS else []\n",
    "\n",
    "    if val_shards:\n",
    "        print(\"Running quick validation on\", len(val_shards), \"shards...\")\n",
    "        val_loader = make_loader_for_shards(val_shards, batch_size=BATCH_SIZE)\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_steps = 0\n",
    "        with torch.no_grad():\n",
    "            for step, (X, L, y) in enumerate(val_loader):\n",
    "                with torch.cuda.amp.autocast(enabled=FP16):\n",
    "                    logits, final = model(X)\n",
    "                    if USE_SAMPLED:\n",
    "                        loss = sampled_softmax_loss(final, y, model.item_emb.weight, num_negatives=NUM_NEGATIVES)\n",
    "                    else:\n",
    "                        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "                val_loss_total += float(loss.item())\n",
    "                val_steps += 1\n",
    "                if step >= 200:  # limit validation steps for speed\n",
    "                    break\n",
    "        if val_steps > 0:\n",
    "            print(f\"Validation (approx) loss: {val_loss_total / val_steps:.4f} over {val_steps} steps\")\n",
    "        model.train()\n",
    "\n",
    "print(\"Warm-up training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a79937",
   "metadata": {},
   "source": [
    "#### Phase B: train on 50 shards (ready-to-run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acc2d68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 50 shards: shard_000.pt ... shard_049.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2588\\601031732.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(sp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in shards: 12500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2588\\559748316.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch = torch.load(p)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2588\\3293107640.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 200 avg_loss 2.2194\n",
      "epoch 0 step 400 avg_loss 2.1946\n",
      "epoch 0 step 600 avg_loss 2.2335\n",
      "epoch 0 step 800 avg_loss 2.2944\n",
      "epoch 0 step 1000 avg_loss 2.2708\n",
      "epoch 0 step 1200 avg_loss 2.2357\n",
      "epoch 0 step 1400 avg_loss 2.2369\n",
      "epoch 0 step 1600 avg_loss 2.2292\n",
      "epoch 0 step 1800 avg_loss 2.2193\n",
      "epoch 0 step 2000 avg_loss 2.2342\n",
      "epoch 0 step 2200 avg_loss 2.2316\n",
      "epoch 0 step 2400 avg_loss 2.2414\n",
      "epoch 0 step 2600 avg_loss 2.2305\n",
      "epoch 0 step 2800 avg_loss 2.2203\n",
      "epoch 0 step 3000 avg_loss 2.2194\n",
      "epoch 0 step 3200 avg_loss 2.2197\n",
      "epoch 0 step 3400 avg_loss 2.2272\n",
      "epoch 0 step 3600 avg_loss 2.2280\n",
      "epoch 0 step 3800 avg_loss 2.2242\n",
      "epoch 0 step 4000 avg_loss 2.2231\n",
      "epoch 0 step 4200 avg_loss 2.2260\n",
      "epoch 0 step 4400 avg_loss 2.2249\n",
      "epoch 0 step 4600 avg_loss 2.2172\n",
      "epoch 0 step 4800 avg_loss 2.2175\n",
      "epoch 0 step 5000 avg_loss 2.2179\n",
      "epoch 0 step 5200 avg_loss 2.2153\n",
      "epoch 0 step 5400 avg_loss 2.2187\n",
      "epoch 0 step 5600 avg_loss 2.2203\n",
      "epoch 0 step 5800 avg_loss 2.2228\n",
      "epoch 0 step 6000 avg_loss 2.2300\n",
      "epoch 0 step 6200 avg_loss 2.2272\n",
      "epoch 0 step 6400 avg_loss 2.2300\n",
      "epoch 0 step 6600 avg_loss 2.2237\n",
      "epoch 0 step 6800 avg_loss 2.2224\n",
      "epoch 0 step 7000 avg_loss 2.2211\n",
      "epoch 0 step 7200 avg_loss 2.2203\n",
      "epoch 0 step 7400 avg_loss 2.2193\n",
      "epoch 0 step 7600 avg_loss 2.2151\n",
      "epoch 0 step 7800 avg_loss 2.2147\n",
      "epoch 0 step 8000 avg_loss 2.2123\n",
      "epoch 0 step 8200 avg_loss 2.2085\n",
      "epoch 0 step 8400 avg_loss 2.2101\n",
      "epoch 0 step 8600 avg_loss 2.2059\n",
      "epoch 0 step 8800 avg_loss 2.2008\n",
      "epoch 0 step 9000 avg_loss 2.1994\n",
      "epoch 0 step 9200 avg_loss 2.1968\n",
      "epoch 0 step 9400 avg_loss 2.1940\n",
      "epoch 0 step 9600 avg_loss 2.1912\n",
      "epoch 0 step 9800 avg_loss 2.1908\n",
      "epoch 0 step 10000 avg_loss 2.1869\n",
      "epoch 0 step 10200 avg_loss 2.1894\n",
      "epoch 0 step 10400 avg_loss 2.1897\n",
      "epoch 0 step 10600 avg_loss 2.1866\n",
      "epoch 0 step 10800 avg_loss 2.1864\n",
      "epoch 0 step 11000 avg_loss 2.1851\n",
      "epoch 0 step 11200 avg_loss 2.1831\n",
      "epoch 0 step 11400 avg_loss 2.1823\n",
      "epoch 0 step 11600 avg_loss 2.1812\n",
      "epoch 0 step 11800 avg_loss 2.1766\n",
      "epoch 0 step 12000 avg_loss 2.1768\n",
      "epoch 0 step 12200 avg_loss 2.1761\n",
      "epoch 0 step 12400 avg_loss 2.1752\n",
      "epoch 0 step 12600 avg_loss 2.1738\n",
      "epoch 0 step 12800 avg_loss 2.1734\n",
      "epoch 0 step 13000 avg_loss 2.1740\n",
      "epoch 0 step 13200 avg_loss 2.1748\n",
      "epoch 0 step 13400 avg_loss 2.1783\n",
      "epoch 0 step 13600 avg_loss 2.1795\n",
      "epoch 0 step 13800 avg_loss 2.1807\n",
      "epoch 0 step 14000 avg_loss 2.1819\n",
      "epoch 0 step 14200 avg_loss 2.1812\n",
      "epoch 0 step 14400 avg_loss 2.1831\n",
      "epoch 0 step 14600 avg_loss 2.1835\n",
      "epoch 0 step 14800 avg_loss 2.1828\n",
      "epoch 0 step 15000 avg_loss 2.1852\n",
      "epoch 0 step 15200 avg_loss 2.1834\n",
      "epoch 0 step 15400 avg_loss 2.1816\n",
      "epoch 0 step 15600 avg_loss 2.1825\n",
      "epoch 0 step 15800 avg_loss 2.1817\n",
      "epoch 0 step 16000 avg_loss 2.1805\n",
      "epoch 0 step 16200 avg_loss 2.1775\n",
      "epoch 0 step 16400 avg_loss 2.1776\n",
      "epoch 0 step 16600 avg_loss 2.1766\n",
      "epoch 0 step 16800 avg_loss 2.1757\n",
      "epoch 0 step 17000 avg_loss 2.1763\n",
      "epoch 0 step 17200 avg_loss 2.1767\n",
      "epoch 0 step 17400 avg_loss 2.1761\n",
      "epoch 0 step 17600 avg_loss 2.1756\n",
      "epoch 0 step 17800 avg_loss 2.1752\n",
      "epoch 0 step 18000 avg_loss 2.1726\n",
      "epoch 0 step 18200 avg_loss 2.1711\n",
      "epoch 0 step 18400 avg_loss 2.1701\n",
      "epoch 0 step 18600 avg_loss 2.1701\n",
      "epoch 0 step 18800 avg_loss 2.1696\n",
      "epoch 0 step 19000 avg_loss 2.1683\n",
      "epoch 0 step 19200 avg_loss 2.1683\n",
      "epoch 0 step 19400 avg_loss 2.1673\n",
      "epoch 0 step 19600 avg_loss 2.1642\n",
      "epoch 0 step 19800 avg_loss 2.1626\n",
      "epoch 0 step 20000 avg_loss 2.1620\n",
      "epoch 0 step 20200 avg_loss 2.1604\n",
      "epoch 0 step 20400 avg_loss 2.1584\n",
      "epoch 0 step 20600 avg_loss 2.1542\n",
      "epoch 0 step 20800 avg_loss 2.1550\n",
      "epoch 0 step 21000 avg_loss 2.1539\n",
      "epoch 0 step 21200 avg_loss 2.1531\n",
      "epoch 0 step 21400 avg_loss 2.1526\n",
      "epoch 0 step 21600 avg_loss 2.1520\n",
      "epoch 0 step 21800 avg_loss 2.1512\n",
      "epoch 0 step 22000 avg_loss 2.1504\n",
      "epoch 0 step 22200 avg_loss 2.1485\n",
      "epoch 0 step 22400 avg_loss 2.1474\n",
      "epoch 0 step 22600 avg_loss 2.1464\n",
      "epoch 0 step 22800 avg_loss 2.1445\n",
      "epoch 0 step 23000 avg_loss 2.1445\n",
      "epoch 0 step 23200 avg_loss 2.1437\n",
      "epoch 0 step 23400 avg_loss 2.1428\n",
      "epoch 0 step 23600 avg_loss 2.1422\n",
      "epoch 0 step 23800 avg_loss 2.1419\n",
      "epoch 0 step 24000 avg_loss 2.1417\n",
      "epoch 0 step 24200 avg_loss 2.1415\n",
      "epoch 0 step 24400 avg_loss 2.1423\n",
      "epoch 0 step 24600 avg_loss 2.1407\n",
      "epoch 0 step 24800 avg_loss 2.1412\n",
      "epoch 0 step 25000 avg_loss 2.1392\n",
      "epoch 0 step 25200 avg_loss 2.1383\n",
      "epoch 0 step 25400 avg_loss 2.1378\n",
      "epoch 0 step 25600 avg_loss 2.1383\n",
      "epoch 0 step 25800 avg_loss 2.1385\n",
      "epoch 0 step 26000 avg_loss 2.1400\n",
      "epoch 0 step 26200 avg_loss 2.1392\n",
      "epoch 0 step 26400 avg_loss 2.1383\n",
      "epoch 0 step 26600 avg_loss 2.1377\n",
      "epoch 0 step 26800 avg_loss 2.1364\n",
      "epoch 0 step 27000 avg_loss 2.1361\n",
      "epoch 0 step 27200 avg_loss 2.1349\n",
      "epoch 0 step 27400 avg_loss 2.1350\n",
      "epoch 0 step 27600 avg_loss 2.1356\n",
      "epoch 0 step 27800 avg_loss 2.1352\n",
      "epoch 0 step 28000 avg_loss 2.1356\n",
      "epoch 0 step 28200 avg_loss 2.1363\n",
      "epoch 0 step 28400 avg_loss 2.1364\n",
      "epoch 0 step 28600 avg_loss 2.1372\n",
      "epoch 0 step 28800 avg_loss 2.1386\n",
      "epoch 0 step 29000 avg_loss 2.1392\n",
      "epoch 0 step 29200 avg_loss 2.1394\n",
      "epoch 0 step 29400 avg_loss 2.1391\n",
      "epoch 0 step 29600 avg_loss 2.1400\n",
      "epoch 0 step 29800 avg_loss 2.1396\n",
      "epoch 0 step 30000 avg_loss 2.1393\n",
      "epoch 0 step 30200 avg_loss 2.1403\n",
      "epoch 0 step 30400 avg_loss 2.1402\n",
      "epoch 0 step 30600 avg_loss 2.1402\n",
      "epoch 0 step 30800 avg_loss 2.1408\n",
      "epoch 0 step 31000 avg_loss 2.1417\n",
      "epoch 0 step 31200 avg_loss 2.1412\n",
      "epoch 0 step 31400 avg_loss 2.1408\n",
      "epoch 0 step 31600 avg_loss 2.1403\n",
      "epoch 0 step 31800 avg_loss 2.1418\n",
      "epoch 0 step 32000 avg_loss 2.1426\n",
      "epoch 0 step 32200 avg_loss 2.1425\n",
      "epoch 0 step 32400 avg_loss 2.1417\n",
      "epoch 0 step 32600 avg_loss 2.1429\n",
      "epoch 0 step 32800 avg_loss 2.1438\n",
      "epoch 0 step 33000 avg_loss 2.1441\n",
      "epoch 0 step 33200 avg_loss 2.1443\n",
      "epoch 0 step 33400 avg_loss 2.1447\n",
      "epoch 0 step 33600 avg_loss 2.1453\n",
      "epoch 0 step 33800 avg_loss 2.1452\n",
      "epoch 0 step 34000 avg_loss 2.1457\n",
      "epoch 0 step 34200 avg_loss 2.1454\n",
      "epoch 0 step 34400 avg_loss 2.1454\n",
      "epoch 0 step 34600 avg_loss 2.1453\n",
      "epoch 0 step 34800 avg_loss 2.1458\n",
      "epoch 0 step 35000 avg_loss 2.1456\n",
      "epoch 0 step 35200 avg_loss 2.1458\n",
      "epoch 0 step 35400 avg_loss 2.1466\n",
      "epoch 0 step 35600 avg_loss 2.1455\n",
      "epoch 0 step 35800 avg_loss 2.1455\n",
      "epoch 0 step 36000 avg_loss 2.1458\n",
      "epoch 0 step 36200 avg_loss 2.1467\n",
      "epoch 0 step 36400 avg_loss 2.1473\n",
      "epoch 0 step 36600 avg_loss 2.1480\n",
      "epoch 0 step 36800 avg_loss 2.1484\n",
      "epoch 0 step 37000 avg_loss 2.1478\n",
      "epoch 0 step 37200 avg_loss 2.1472\n",
      "epoch 0 step 37400 avg_loss 2.1475\n",
      "epoch 0 step 37600 avg_loss 2.1487\n",
      "epoch 0 step 37800 avg_loss 2.1487\n",
      "epoch 0 step 38000 avg_loss 2.1488\n",
      "epoch 0 step 38200 avg_loss 2.1488\n",
      "epoch 0 step 38400 avg_loss 2.1488\n",
      "epoch 0 step 38600 avg_loss 2.1491\n",
      "epoch 0 step 38800 avg_loss 2.1489\n",
      "epoch 0 step 39000 avg_loss 2.1499\n",
      "epoch 0 step 39200 avg_loss 2.1496\n",
      "epoch 0 step 39400 avg_loss 2.1498\n",
      "epoch 0 step 39600 avg_loss 2.1501\n",
      "epoch 0 step 39800 avg_loss 2.1499\n",
      "epoch 0 step 40000 avg_loss 2.1499\n",
      "epoch 0 step 40200 avg_loss 2.1500\n",
      "epoch 0 step 40400 avg_loss 2.1502\n",
      "epoch 0 step 40600 avg_loss 2.1503\n",
      "epoch 0 step 40800 avg_loss 2.1503\n",
      "epoch 0 step 41000 avg_loss 2.1514\n",
      "epoch 0 step 41200 avg_loss 2.1518\n",
      "epoch 0 step 41400 avg_loss 2.1524\n",
      "epoch 0 step 41600 avg_loss 2.1526\n",
      "epoch 0 step 41800 avg_loss 2.1527\n",
      "epoch 0 step 42000 avg_loss 2.1529\n",
      "epoch 0 step 42200 avg_loss 2.1526\n",
      "epoch 0 step 42400 avg_loss 2.1532\n",
      "epoch 0 step 42600 avg_loss 2.1532\n",
      "epoch 0 step 42800 avg_loss 2.1529\n",
      "epoch 0 step 43000 avg_loss 2.1530\n",
      "epoch 0 step 43200 avg_loss 2.1536\n",
      "epoch 0 step 43400 avg_loss 2.1539\n",
      "epoch 0 step 43600 avg_loss 2.1538\n",
      "epoch 0 step 43800 avg_loss 2.1535\n",
      "epoch 0 step 44000 avg_loss 2.1537\n",
      "epoch 0 step 44200 avg_loss 2.1540\n",
      "epoch 0 step 44400 avg_loss 2.1540\n",
      "epoch 0 step 44600 avg_loss 2.1536\n",
      "epoch 0 step 44800 avg_loss 2.1538\n",
      "epoch 0 step 45000 avg_loss 2.1541\n",
      "epoch 0 step 45200 avg_loss 2.1551\n",
      "epoch 0 step 45400 avg_loss 2.1556\n",
      "epoch 0 step 45600 avg_loss 2.1554\n",
      "epoch 0 step 45800 avg_loss 2.1550\n",
      "epoch 0 step 46000 avg_loss 2.1548\n",
      "epoch 0 step 46200 avg_loss 2.1549\n",
      "epoch 0 step 46400 avg_loss 2.1549\n",
      "epoch 0 step 46600 avg_loss 2.1549\n",
      "epoch 0 step 46800 avg_loss 2.1552\n",
      "epoch 0 step 47000 avg_loss 2.1552\n",
      "epoch 0 step 47200 avg_loss 2.1548\n",
      "epoch 0 step 47400 avg_loss 2.1552\n",
      "epoch 0 step 47600 avg_loss 2.1554\n",
      "epoch 0 step 47800 avg_loss 2.1556\n",
      "epoch 0 step 48000 avg_loss 2.1565\n",
      "epoch 0 step 48200 avg_loss 2.1574\n",
      "epoch 0 step 48400 avg_loss 2.1576\n",
      "epoch 0 step 48600 avg_loss 2.1584\n",
      "epoch 0 step 48800 avg_loss 2.1581\n",
      "epoch 0 step 49000 avg_loss 2.1581\n",
      "epoch 0 step 49200 avg_loss 2.1587\n",
      "epoch 0 step 49400 avg_loss 2.1601\n",
      "epoch 0 step 49600 avg_loss 2.1600\n",
      "epoch 0 step 49800 avg_loss 2.1598\n",
      "epoch 0 step 50000 avg_loss 2.1598\n",
      "epoch 0 step 50200 avg_loss 2.1596\n",
      "epoch 0 step 50400 avg_loss 2.1592\n",
      "epoch 0 step 50600 avg_loss 2.1589\n",
      "epoch 0 step 50800 avg_loss 2.1591\n",
      "epoch 0 step 51000 avg_loss 2.1590\n",
      "epoch 0 step 51200 avg_loss 2.1587\n",
      "epoch 0 step 51400 avg_loss 2.1586\n",
      "epoch 0 step 51600 avg_loss 2.1584\n",
      "epoch 0 step 51800 avg_loss 2.1581\n",
      "epoch 0 step 52000 avg_loss 2.1585\n",
      "epoch 0 step 52200 avg_loss 2.1584\n",
      "epoch 0 step 52400 avg_loss 2.1582\n",
      "epoch 0 step 52600 avg_loss 2.1582\n",
      "epoch 0 step 52800 avg_loss 2.1579\n",
      "epoch 0 step 53000 avg_loss 2.1583\n",
      "epoch 0 step 53200 avg_loss 2.1579\n",
      "epoch 0 step 53400 avg_loss 2.1582\n",
      "epoch 0 step 53600 avg_loss 2.1582\n",
      "epoch 0 step 53800 avg_loss 2.1584\n",
      "epoch 0 step 54000 avg_loss 2.1585\n",
      "epoch 0 step 54200 avg_loss 2.1586\n",
      "epoch 0 step 54400 avg_loss 2.1587\n",
      "epoch 0 step 54600 avg_loss 2.1583\n",
      "epoch 0 step 54800 avg_loss 2.1583\n",
      "epoch 0 step 55000 avg_loss 2.1581\n",
      "epoch 0 step 55200 avg_loss 2.1575\n",
      "epoch 0 step 55400 avg_loss 2.1581\n",
      "epoch 0 step 55600 avg_loss 2.1575\n",
      "epoch 0 step 55800 avg_loss 2.1575\n",
      "epoch 0 step 56000 avg_loss 2.1573\n",
      "epoch 0 step 56200 avg_loss 2.1574\n",
      "epoch 0 step 56400 avg_loss 2.1574\n",
      "epoch 0 step 56600 avg_loss 2.1569\n",
      "epoch 0 step 56800 avg_loss 2.1565\n",
      "epoch 0 step 57000 avg_loss 2.1563\n",
      "epoch 0 step 57200 avg_loss 2.1562\n",
      "epoch 0 step 57400 avg_loss 2.1561\n",
      "epoch 0 step 57600 avg_loss 2.1563\n",
      "epoch 0 step 57800 avg_loss 2.1556\n",
      "epoch 0 step 58000 avg_loss 2.1556\n",
      "epoch 0 step 58200 avg_loss 2.1557\n",
      "epoch 0 step 58400 avg_loss 2.1554\n",
      "epoch 0 step 58600 avg_loss 2.1554\n",
      "epoch 0 step 58800 avg_loss 2.1550\n",
      "epoch 0 step 59000 avg_loss 2.1549\n",
      "epoch 0 step 59200 avg_loss 2.1547\n",
      "epoch 0 step 59400 avg_loss 2.1544\n",
      "epoch 0 step 59600 avg_loss 2.1544\n",
      "epoch 0 step 59800 avg_loss 2.1547\n",
      "epoch 0 step 60000 avg_loss 2.1539\n",
      "epoch 0 step 60200 avg_loss 2.1538\n",
      "epoch 0 step 60400 avg_loss 2.1532\n",
      "epoch 0 step 60600 avg_loss 2.1531\n",
      "epoch 0 step 60800 avg_loss 2.1529\n",
      "epoch 0 step 61000 avg_loss 2.1525\n",
      "epoch 0 step 61200 avg_loss 2.1524\n",
      "epoch 0 step 61400 avg_loss 2.1519\n",
      "epoch 0 step 61600 avg_loss 2.1516\n",
      "epoch 0 step 61800 avg_loss 2.1509\n",
      "epoch 0 step 62000 avg_loss 2.1510\n",
      "epoch 0 step 62200 avg_loss 2.1505\n",
      "epoch 0 step 62400 avg_loss 2.1498\n",
      "epoch 0 step 62600 avg_loss 2.1497\n",
      "epoch 0 step 62800 avg_loss 2.1497\n",
      "epoch 0 step 63000 avg_loss 2.1491\n",
      "epoch 0 step 63200 avg_loss 2.1486\n",
      "epoch 0 step 63400 avg_loss 2.1484\n",
      "epoch 0 step 63600 avg_loss 2.1482\n",
      "epoch 0 step 63800 avg_loss 2.1479\n",
      "epoch 0 step 64000 avg_loss 2.1474\n",
      "epoch 0 step 64200 avg_loss 2.1469\n",
      "epoch 0 step 64400 avg_loss 2.1469\n",
      "epoch 0 step 64600 avg_loss 2.1466\n",
      "epoch 0 step 64800 avg_loss 2.1467\n",
      "epoch 0 step 65000 avg_loss 2.1467\n",
      "epoch 0 step 65200 avg_loss 2.1466\n",
      "epoch 0 step 65400 avg_loss 2.1461\n",
      "epoch 0 step 65600 avg_loss 2.1458\n",
      "epoch 0 step 65800 avg_loss 2.1458\n",
      "epoch 0 step 66000 avg_loss 2.1450\n",
      "epoch 0 step 66200 avg_loss 2.1442\n",
      "epoch 0 step 66400 avg_loss 2.1439\n",
      "epoch 0 step 66600 avg_loss 2.1442\n",
      "epoch 0 step 66800 avg_loss 2.1443\n",
      "epoch 0 step 67000 avg_loss 2.1444\n",
      "epoch 0 step 67200 avg_loss 2.1449\n",
      "epoch 0 step 67400 avg_loss 2.1455\n",
      "epoch 0 step 67600 avg_loss 2.1456\n",
      "epoch 0 step 67800 avg_loss 2.1457\n",
      "epoch 0 step 68000 avg_loss 2.1461\n",
      "epoch 0 step 68200 avg_loss 2.1459\n",
      "epoch 0 step 68400 avg_loss 2.1462\n",
      "epoch 0 step 68600 avg_loss 2.1459\n",
      "epoch 0 step 68800 avg_loss 2.1454\n",
      "epoch 0 step 69000 avg_loss 2.1455\n",
      "epoch 0 step 69200 avg_loss 2.1455\n",
      "epoch 0 step 69400 avg_loss 2.1457\n",
      "epoch 0 step 69600 avg_loss 2.1455\n",
      "epoch 0 step 69800 avg_loss 2.1455\n",
      "epoch 0 step 70000 avg_loss 2.1459\n",
      "epoch 0 step 70200 avg_loss 2.1460\n",
      "epoch 0 step 70400 avg_loss 2.1465\n",
      "epoch 0 step 70600 avg_loss 2.1463\n",
      "epoch 0 step 70800 avg_loss 2.1461\n",
      "epoch 0 step 71000 avg_loss 2.1461\n",
      "epoch 0 step 71200 avg_loss 2.1459\n",
      "epoch 0 step 71400 avg_loss 2.1464\n",
      "epoch 0 step 71600 avg_loss 2.1469\n",
      "epoch 0 step 71800 avg_loss 2.1468\n",
      "epoch 0 step 72000 avg_loss 2.1470\n",
      "epoch 0 step 72200 avg_loss 2.1470\n",
      "epoch 0 step 72400 avg_loss 2.1469\n",
      "epoch 0 step 72600 avg_loss 2.1470\n",
      "epoch 0 step 72800 avg_loss 2.1474\n",
      "epoch 0 step 73000 avg_loss 2.1477\n",
      "epoch 0 step 73200 avg_loss 2.1476\n",
      "epoch 0 step 73400 avg_loss 2.1473\n",
      "epoch 0 step 73600 avg_loss 2.1475\n",
      "epoch 0 step 73800 avg_loss 2.1477\n",
      "epoch 0 step 74000 avg_loss 2.1481\n",
      "epoch 0 step 74200 avg_loss 2.1482\n",
      "epoch 0 step 74400 avg_loss 2.1480\n",
      "epoch 0 step 74600 avg_loss 2.1481\n",
      "epoch 0 step 74800 avg_loss 2.1477\n",
      "epoch 0 step 75000 avg_loss 2.1475\n",
      "epoch 0 step 75200 avg_loss 2.1475\n",
      "epoch 0 step 75400 avg_loss 2.1477\n",
      "epoch 0 step 75600 avg_loss 2.1475\n",
      "epoch 0 step 75800 avg_loss 2.1473\n",
      "epoch 0 step 76000 avg_loss 2.1472\n",
      "epoch 0 step 76200 avg_loss 2.1471\n",
      "epoch 0 step 76400 avg_loss 2.1469\n",
      "epoch 0 step 76600 avg_loss 2.1472\n",
      "epoch 0 step 76800 avg_loss 2.1473\n",
      "epoch 0 step 77000 avg_loss 2.1476\n",
      "epoch 0 step 77200 avg_loss 2.1475\n",
      "epoch 0 step 77400 avg_loss 2.1474\n",
      "epoch 0 step 77600 avg_loss 2.1473\n",
      "epoch 0 step 77800 avg_loss 2.1476\n",
      "epoch 0 step 78000 avg_loss 2.1476\n",
      "epoch 0 step 78200 avg_loss 2.1470\n",
      "epoch 0 step 78400 avg_loss 2.1474\n",
      "epoch 0 step 78600 avg_loss 2.1475\n",
      "epoch 0 step 78800 avg_loss 2.1477\n",
      "epoch 0 step 79000 avg_loss 2.1478\n",
      "epoch 0 step 79200 avg_loss 2.1475\n",
      "epoch 0 step 79400 avg_loss 2.1474\n",
      "epoch 0 step 79600 avg_loss 2.1475\n",
      "epoch 0 step 79800 avg_loss 2.1479\n",
      "epoch 0 step 80000 avg_loss 2.1480\n",
      "epoch 0 step 80200 avg_loss 2.1479\n",
      "epoch 0 step 80400 avg_loss 2.1481\n",
      "epoch 0 step 80600 avg_loss 2.1478\n",
      "epoch 0 step 80800 avg_loss 2.1477\n",
      "epoch 0 step 81000 avg_loss 2.1475\n",
      "epoch 0 step 81200 avg_loss 2.1472\n",
      "epoch 0 step 81400 avg_loss 2.1474\n",
      "epoch 0 step 81600 avg_loss 2.1469\n",
      "epoch 0 step 81800 avg_loss 2.1467\n",
      "epoch 0 step 82000 avg_loss 2.1468\n",
      "epoch 0 step 82200 avg_loss 2.1465\n",
      "epoch 0 step 82400 avg_loss 2.1464\n",
      "epoch 0 step 82600 avg_loss 2.1464\n",
      "epoch 0 step 82800 avg_loss 2.1461\n",
      "epoch 0 step 83000 avg_loss 2.1461\n",
      "epoch 0 step 83200 avg_loss 2.1463\n",
      "epoch 0 step 83400 avg_loss 2.1464\n",
      "epoch 0 step 83600 avg_loss 2.1461\n",
      "epoch 0 step 83800 avg_loss 2.1459\n",
      "epoch 0 step 84000 avg_loss 2.1458\n",
      "epoch 0 step 84200 avg_loss 2.1460\n",
      "epoch 0 step 84400 avg_loss 2.1460\n",
      "epoch 0 step 84600 avg_loss 2.1464\n",
      "epoch 0 step 84800 avg_loss 2.1465\n",
      "epoch 0 step 85000 avg_loss 2.1465\n",
      "epoch 0 step 85200 avg_loss 2.1465\n",
      "epoch 0 step 85400 avg_loss 2.1464\n",
      "epoch 0 step 85600 avg_loss 2.1463\n",
      "epoch 0 step 85800 avg_loss 2.1464\n",
      "epoch 0 step 86000 avg_loss 2.1464\n",
      "epoch 0 step 86200 avg_loss 2.1464\n",
      "epoch 0 step 86400 avg_loss 2.1463\n",
      "epoch 0 step 86600 avg_loss 2.1463\n",
      "epoch 0 step 86800 avg_loss 2.1462\n",
      "epoch 0 step 87000 avg_loss 2.1461\n",
      "epoch 0 step 87200 avg_loss 2.1459\n",
      "epoch 0 step 87400 avg_loss 2.1457\n",
      "epoch 0 step 87600 avg_loss 2.1454\n",
      "epoch 0 step 87800 avg_loss 2.1454\n",
      "epoch 0 step 88000 avg_loss 2.1453\n",
      "epoch 0 step 88200 avg_loss 2.1454\n",
      "epoch 0 step 88400 avg_loss 2.1455\n",
      "epoch 0 step 88600 avg_loss 2.1460\n",
      "epoch 0 step 88800 avg_loss 2.1459\n",
      "epoch 0 step 89000 avg_loss 2.1459\n",
      "epoch 0 step 89200 avg_loss 2.1462\n",
      "epoch 0 step 89400 avg_loss 2.1463\n",
      "epoch 0 step 89600 avg_loss 2.1465\n",
      "epoch 0 step 89800 avg_loss 2.1464\n",
      "epoch 0 step 90000 avg_loss 2.1472\n",
      "epoch 0 step 90200 avg_loss 2.1473\n",
      "epoch 0 step 90400 avg_loss 2.1476\n",
      "epoch 0 step 90600 avg_loss 2.1475\n",
      "epoch 0 step 90800 avg_loss 2.1472\n",
      "epoch 0 step 91000 avg_loss 2.1470\n",
      "epoch 0 step 91200 avg_loss 2.1468\n",
      "epoch 0 step 91400 avg_loss 2.1467\n",
      "epoch 0 step 91600 avg_loss 2.1468\n",
      "epoch 0 step 91800 avg_loss 2.1471\n",
      "epoch 0 step 92000 avg_loss 2.1473\n",
      "epoch 0 step 92200 avg_loss 2.1472\n",
      "epoch 0 step 92400 avg_loss 2.1471\n",
      "epoch 0 step 92600 avg_loss 2.1468\n",
      "epoch 0 step 92800 avg_loss 2.1470\n",
      "epoch 0 step 93000 avg_loss 2.1471\n",
      "epoch 0 step 93200 avg_loss 2.1471\n",
      "epoch 0 step 93400 avg_loss 2.1472\n",
      "epoch 0 step 93600 avg_loss 2.1472\n",
      "epoch 0 step 93800 avg_loss 2.1471\n",
      "epoch 0 step 94000 avg_loss 2.1473\n",
      "epoch 0 step 94200 avg_loss 2.1474\n",
      "epoch 0 step 94400 avg_loss 2.1476\n",
      "epoch 0 step 94600 avg_loss 2.1473\n",
      "epoch 0 step 94800 avg_loss 2.1475\n",
      "epoch 0 step 95000 avg_loss 2.1475\n",
      "epoch 0 step 95200 avg_loss 2.1476\n",
      "epoch 0 step 95400 avg_loss 2.1479\n",
      "epoch 0 step 95600 avg_loss 2.1477\n",
      "epoch 0 step 95800 avg_loss 2.1476\n",
      "epoch 0 step 96000 avg_loss 2.1473\n",
      "epoch 0 step 96200 avg_loss 2.1469\n",
      "epoch 0 step 96400 avg_loss 2.1468\n",
      "epoch 0 step 96600 avg_loss 2.1465\n",
      "epoch 0 step 96800 avg_loss 2.1469\n",
      "epoch 0 step 97000 avg_loss 2.1469\n",
      "epoch 0 step 97200 avg_loss 2.1464\n",
      "epoch 0 step 97400 avg_loss 2.1462\n",
      "epoch 0 step 97600 avg_loss 2.1461\n",
      "epoch 0 step 97800 avg_loss 2.1463\n",
      "epoch 0 step 98000 avg_loss 2.1461\n",
      "epoch 0 step 98200 avg_loss 2.1458\n",
      "epoch 0 step 98400 avg_loss 2.1457\n",
      "epoch 0 step 98600 avg_loss 2.1459\n",
      "epoch 0 step 98800 avg_loss 2.1461\n",
      "epoch 0 step 99000 avg_loss 2.1461\n",
      "epoch 0 step 99200 avg_loss 2.1455\n",
      "epoch 0 step 99400 avg_loss 2.1450\n",
      "epoch 0 step 99600 avg_loss 2.1452\n",
      "epoch 0 step 99800 avg_loss 2.1451\n",
      "epoch 0 step 100000 avg_loss 2.1453\n",
      "epoch 0 step 100200 avg_loss 2.1453\n",
      "epoch 0 step 100400 avg_loss 2.1452\n",
      "epoch 0 step 100600 avg_loss 2.1451\n",
      "epoch 0 step 100800 avg_loss 2.1450\n",
      "epoch 0 step 101000 avg_loss 2.1450\n",
      "epoch 0 step 101200 avg_loss 2.1450\n",
      "epoch 0 step 101400 avg_loss 2.1451\n",
      "epoch 0 step 101600 avg_loss 2.1452\n",
      "epoch 0 step 101800 avg_loss 2.1452\n",
      "epoch 0 step 102000 avg_loss 2.1453\n",
      "epoch 0 step 102200 avg_loss 2.1450\n",
      "epoch 0 step 102400 avg_loss 2.1446\n",
      "epoch 0 step 102600 avg_loss 2.1446\n",
      "epoch 0 step 102800 avg_loss 2.1443\n",
      "epoch 0 step 103000 avg_loss 2.1442\n",
      "epoch 0 step 103200 avg_loss 2.1442\n",
      "epoch 0 step 103400 avg_loss 2.1441\n",
      "epoch 0 step 103600 avg_loss 2.1444\n",
      "epoch 0 step 103800 avg_loss 2.1443\n",
      "epoch 0 step 104000 avg_loss 2.1443\n",
      "epoch 0 step 104200 avg_loss 2.1442\n",
      "epoch 0 step 104400 avg_loss 2.1441\n",
      "epoch 0 step 104600 avg_loss 2.1440\n",
      "epoch 0 step 104800 avg_loss 2.1439\n",
      "epoch 0 step 105000 avg_loss 2.1440\n",
      "epoch 0 step 105200 avg_loss 2.1438\n",
      "epoch 0 step 105400 avg_loss 2.1440\n",
      "epoch 0 step 105600 avg_loss 2.1439\n",
      "epoch 0 step 105800 avg_loss 2.1439\n",
      "epoch 0 step 106000 avg_loss 2.1438\n",
      "epoch 0 step 106200 avg_loss 2.1438\n",
      "epoch 0 step 106400 avg_loss 2.1439\n",
      "epoch 0 step 106600 avg_loss 2.1438\n",
      "epoch 0 step 106800 avg_loss 2.1439\n",
      "epoch 0 step 107000 avg_loss 2.1441\n",
      "epoch 0 step 107200 avg_loss 2.1442\n",
      "epoch 0 step 107400 avg_loss 2.1437\n",
      "epoch 0 step 107600 avg_loss 2.1438\n",
      "epoch 0 step 107800 avg_loss 2.1439\n",
      "epoch 0 step 108000 avg_loss 2.1440\n",
      "epoch 0 step 108200 avg_loss 2.1438\n",
      "epoch 0 step 108400 avg_loss 2.1441\n",
      "epoch 0 step 108600 avg_loss 2.1438\n",
      "epoch 0 step 108800 avg_loss 2.1436\n",
      "epoch 0 step 109000 avg_loss 2.1437\n",
      "epoch 0 step 109200 avg_loss 2.1438\n",
      "epoch 0 step 109400 avg_loss 2.1433\n",
      "epoch 0 step 109600 avg_loss 2.1432\n",
      "epoch 0 step 109800 avg_loss 2.1434\n",
      "epoch 0 step 110000 avg_loss 2.1432\n",
      "epoch 0 step 110200 avg_loss 2.1433\n",
      "epoch 0 step 110400 avg_loss 2.1432\n",
      "epoch 0 step 110600 avg_loss 2.1431\n",
      "epoch 0 step 110800 avg_loss 2.1432\n",
      "epoch 0 step 111000 avg_loss 2.1432\n",
      "epoch 0 step 111200 avg_loss 2.1432\n",
      "epoch 0 step 111400 avg_loss 2.1431\n",
      "epoch 0 step 111600 avg_loss 2.1432\n",
      "epoch 0 step 111800 avg_loss 2.1431\n",
      "epoch 0 step 112000 avg_loss 2.1433\n",
      "epoch 0 step 112200 avg_loss 2.1434\n",
      "epoch 0 step 112400 avg_loss 2.1434\n",
      "epoch 0 step 112600 avg_loss 2.1434\n",
      "epoch 0 step 112800 avg_loss 2.1437\n",
      "epoch 0 step 113000 avg_loss 2.1438\n",
      "epoch 0 step 113200 avg_loss 2.1438\n",
      "epoch 0 step 113400 avg_loss 2.1439\n",
      "epoch 0 step 113600 avg_loss 2.1440\n",
      "epoch 0 step 113800 avg_loss 2.1438\n",
      "epoch 0 step 114000 avg_loss 2.1438\n",
      "epoch 0 step 114200 avg_loss 2.1436\n",
      "epoch 0 step 114400 avg_loss 2.1432\n",
      "epoch 0 step 114600 avg_loss 2.1434\n",
      "epoch 0 step 114800 avg_loss 2.1433\n",
      "epoch 0 step 115000 avg_loss 2.1433\n",
      "epoch 0 step 115200 avg_loss 2.1433\n",
      "epoch 0 step 115400 avg_loss 2.1432\n",
      "epoch 0 step 115600 avg_loss 2.1432\n",
      "epoch 0 step 115800 avg_loss 2.1430\n",
      "epoch 0 step 116000 avg_loss 2.1428\n",
      "epoch 0 step 116200 avg_loss 2.1426\n",
      "epoch 0 step 116400 avg_loss 2.1424\n",
      "epoch 0 step 116600 avg_loss 2.1422\n",
      "epoch 0 step 116800 avg_loss 2.1421\n",
      "epoch 0 step 117000 avg_loss 2.1422\n",
      "epoch 0 step 117200 avg_loss 2.1423\n",
      "epoch 0 step 117400 avg_loss 2.1425\n",
      "epoch 0 step 117600 avg_loss 2.1427\n",
      "epoch 0 step 117800 avg_loss 2.1428\n",
      "epoch 0 step 118000 avg_loss 2.1425\n",
      "epoch 0 step 118200 avg_loss 2.1428\n",
      "epoch 0 step 118400 avg_loss 2.1426\n",
      "epoch 0 step 118600 avg_loss 2.1428\n",
      "epoch 0 step 118800 avg_loss 2.1428\n",
      "epoch 0 step 119000 avg_loss 2.1429\n",
      "epoch 0 step 119200 avg_loss 2.1429\n",
      "epoch 0 step 119400 avg_loss 2.1427\n",
      "epoch 0 step 119600 avg_loss 2.1426\n",
      "epoch 0 step 119800 avg_loss 2.1426\n",
      "epoch 0 step 120000 avg_loss 2.1427\n",
      "epoch 0 step 120200 avg_loss 2.1426\n",
      "epoch 0 step 120400 avg_loss 2.1427\n",
      "epoch 0 step 120600 avg_loss 2.1426\n",
      "epoch 0 step 120800 avg_loss 2.1423\n",
      "epoch 0 step 121000 avg_loss 2.1422\n",
      "epoch 0 step 121200 avg_loss 2.1423\n",
      "epoch 0 step 121400 avg_loss 2.1424\n",
      "epoch 0 step 121600 avg_loss 2.1423\n",
      "epoch 0 step 121800 avg_loss 2.1424\n",
      "epoch 0 step 122000 avg_loss 2.1423\n",
      "epoch 0 step 122200 avg_loss 2.1422\n",
      "epoch 0 step 122400 avg_loss 2.1421\n",
      "epoch 0 step 122600 avg_loss 2.1421\n",
      "epoch 0 step 122800 avg_loss 2.1423\n",
      "epoch 0 step 123000 avg_loss 2.1423\n",
      "epoch 0 step 123200 avg_loss 2.1423\n",
      "epoch 0 step 123400 avg_loss 2.1422\n",
      "epoch 0 step 123600 avg_loss 2.1422\n",
      "epoch 0 step 123800 avg_loss 2.1420\n",
      "epoch 0 step 124000 avg_loss 2.1421\n",
      "epoch 0 step 124200 avg_loss 2.1419\n",
      "epoch 0 step 124400 avg_loss 2.1417\n",
      "epoch 0 step 124600 avg_loss 2.1417\n",
      "epoch 0 step 124800 avg_loss 2.1418\n",
      "epoch 0 step 125000 avg_loss 2.1425\n",
      "epoch 0 step 125200 avg_loss 2.1424\n",
      "epoch 0 step 125400 avg_loss 2.1425\n",
      "epoch 0 step 125600 avg_loss 2.1422\n",
      "epoch 0 step 125800 avg_loss 2.1421\n",
      "epoch 0 step 126000 avg_loss 2.1419\n",
      "epoch 0 step 126200 avg_loss 2.1417\n",
      "epoch 0 step 126400 avg_loss 2.1416\n",
      "epoch 0 step 126600 avg_loss 2.1416\n",
      "epoch 0 step 126800 avg_loss 2.1415\n",
      "epoch 0 step 127000 avg_loss 2.1416\n",
      "epoch 0 step 127200 avg_loss 2.1417\n",
      "epoch 0 step 127400 avg_loss 2.1414\n",
      "epoch 0 step 127600 avg_loss 2.1414\n",
      "epoch 0 step 127800 avg_loss 2.1416\n",
      "epoch 0 step 128000 avg_loss 2.1415\n",
      "epoch 0 step 128200 avg_loss 2.1414\n",
      "epoch 0 step 128400 avg_loss 2.1413\n",
      "epoch 0 step 128600 avg_loss 2.1414\n",
      "epoch 0 step 128800 avg_loss 2.1414\n",
      "epoch 0 step 129000 avg_loss 2.1417\n",
      "epoch 0 step 129200 avg_loss 2.1416\n",
      "epoch 0 step 129400 avg_loss 2.1417\n",
      "epoch 0 step 129600 avg_loss 2.1419\n",
      "epoch 0 step 129800 avg_loss 2.1418\n",
      "epoch 0 step 130000 avg_loss 2.1420\n",
      "epoch 0 step 130200 avg_loss 2.1418\n",
      "epoch 0 step 130400 avg_loss 2.1418\n",
      "epoch 0 step 130600 avg_loss 2.1417\n",
      "epoch 0 step 130800 avg_loss 2.1416\n",
      "epoch 0 step 131000 avg_loss 2.1415\n",
      "epoch 0 step 131200 avg_loss 2.1414\n",
      "epoch 0 step 131400 avg_loss 2.1415\n",
      "epoch 0 step 131600 avg_loss 2.1414\n",
      "epoch 0 step 131800 avg_loss 2.1415\n",
      "epoch 0 step 132000 avg_loss 2.1415\n",
      "epoch 0 step 132200 avg_loss 2.1416\n",
      "epoch 0 step 132400 avg_loss 2.1419\n",
      "epoch 0 step 132600 avg_loss 2.1419\n",
      "epoch 0 step 132800 avg_loss 2.1417\n",
      "epoch 0 step 133000 avg_loss 2.1416\n",
      "epoch 0 step 133200 avg_loss 2.1415\n",
      "epoch 0 step 133400 avg_loss 2.1411\n",
      "epoch 0 step 133600 avg_loss 2.1411\n",
      "epoch 0 step 133800 avg_loss 2.1412\n",
      "epoch 0 step 134000 avg_loss 2.1413\n",
      "epoch 0 step 134200 avg_loss 2.1413\n",
      "epoch 0 step 134400 avg_loss 2.1412\n",
      "epoch 0 step 134600 avg_loss 2.1411\n",
      "epoch 0 step 134800 avg_loss 2.1411\n",
      "epoch 0 step 135000 avg_loss 2.1412\n",
      "epoch 0 step 135200 avg_loss 2.1411\n",
      "epoch 0 step 135400 avg_loss 2.1410\n",
      "epoch 0 step 135600 avg_loss 2.1411\n",
      "epoch 0 step 135800 avg_loss 2.1410\n",
      "epoch 0 step 136000 avg_loss 2.1410\n",
      "epoch 0 step 136200 avg_loss 2.1409\n",
      "epoch 0 step 136400 avg_loss 2.1410\n",
      "epoch 0 step 136600 avg_loss 2.1410\n",
      "epoch 0 step 136800 avg_loss 2.1412\n",
      "epoch 0 step 137000 avg_loss 2.1411\n",
      "epoch 0 step 137200 avg_loss 2.1409\n",
      "epoch 0 step 137400 avg_loss 2.1408\n",
      "epoch 0 step 137600 avg_loss 2.1406\n",
      "epoch 0 step 137800 avg_loss 2.1405\n",
      "epoch 0 step 138000 avg_loss 2.1406\n",
      "epoch 0 step 138200 avg_loss 2.1405\n",
      "epoch 0 step 138400 avg_loss 2.1407\n",
      "epoch 0 step 138600 avg_loss 2.1405\n",
      "epoch 0 step 138800 avg_loss 2.1404\n",
      "epoch 0 step 139000 avg_loss 2.1408\n",
      "epoch 0 step 139200 avg_loss 2.1409\n",
      "epoch 0 step 139400 avg_loss 2.1410\n",
      "epoch 0 step 139600 avg_loss 2.1410\n",
      "epoch 0 step 139800 avg_loss 2.1411\n",
      "epoch 0 step 140000 avg_loss 2.1412\n",
      "epoch 0 step 140200 avg_loss 2.1410\n",
      "epoch 0 step 140400 avg_loss 2.1411\n",
      "epoch 0 step 140600 avg_loss 2.1413\n",
      "epoch 0 step 140800 avg_loss 2.1413\n",
      "epoch 0 step 141000 avg_loss 2.1411\n",
      "epoch 0 step 141200 avg_loss 2.1413\n",
      "epoch 0 step 141400 avg_loss 2.1412\n",
      "epoch 0 step 141600 avg_loss 2.1412\n",
      "epoch 0 step 141800 avg_loss 2.1413\n",
      "epoch 0 step 142000 avg_loss 2.1414\n",
      "epoch 0 step 142200 avg_loss 2.1412\n",
      "epoch 0 step 142400 avg_loss 2.1411\n",
      "epoch 0 step 142600 avg_loss 2.1410\n",
      "epoch 0 step 142800 avg_loss 2.1409\n",
      "epoch 0 step 143000 avg_loss 2.1409\n",
      "epoch 0 step 143200 avg_loss 2.1410\n",
      "epoch 0 step 143400 avg_loss 2.1410\n",
      "epoch 0 step 143600 avg_loss 2.1410\n",
      "epoch 0 step 143800 avg_loss 2.1409\n",
      "epoch 0 step 144000 avg_loss 2.1408\n",
      "epoch 0 step 144200 avg_loss 2.1407\n",
      "epoch 0 step 144400 avg_loss 2.1405\n",
      "epoch 0 step 144600 avg_loss 2.1405\n",
      "epoch 0 step 144800 avg_loss 2.1406\n",
      "epoch 0 step 145000 avg_loss 2.1406\n",
      "epoch 0 step 145200 avg_loss 2.1406\n",
      "epoch 0 step 145400 avg_loss 2.1407\n",
      "epoch 0 step 145600 avg_loss 2.1405\n",
      "epoch 0 step 145800 avg_loss 2.1402\n",
      "epoch 0 step 146000 avg_loss 2.1402\n",
      "epoch 0 step 146200 avg_loss 2.1405\n",
      "epoch 0 step 146400 avg_loss 2.1406\n",
      "epoch 0 step 146600 avg_loss 2.1407\n",
      "epoch 0 step 146800 avg_loss 2.1407\n",
      "epoch 0 step 147000 avg_loss 2.1407\n",
      "epoch 0 step 147200 avg_loss 2.1407\n",
      "epoch 0 step 147400 avg_loss 2.1408\n",
      "epoch 0 step 147600 avg_loss 2.1409\n",
      "epoch 0 step 147800 avg_loss 2.1407\n",
      "epoch 0 step 148000 avg_loss 2.1406\n",
      "epoch 0 step 148200 avg_loss 2.1406\n",
      "epoch 0 step 148400 avg_loss 2.1405\n",
      "epoch 0 step 148600 avg_loss 2.1407\n",
      "epoch 0 step 148800 avg_loss 2.1406\n",
      "epoch 0 step 149000 avg_loss 2.1406\n",
      "epoch 0 step 149200 avg_loss 2.1410\n",
      "epoch 0 step 149400 avg_loss 2.1410\n",
      "epoch 0 step 149600 avg_loss 2.1410\n",
      "epoch 0 step 149800 avg_loss 2.1412\n",
      "epoch 0 step 150000 avg_loss 2.1409\n",
      "epoch 0 step 150200 avg_loss 2.1408\n",
      "epoch 0 step 150400 avg_loss 2.1408\n",
      "epoch 0 step 150600 avg_loss 2.1406\n",
      "epoch 0 step 150800 avg_loss 2.1405\n",
      "epoch 0 step 151000 avg_loss 2.1405\n",
      "epoch 0 step 151200 avg_loss 2.1405\n",
      "epoch 0 step 151400 avg_loss 2.1404\n",
      "epoch 0 step 151600 avg_loss 2.1406\n",
      "epoch 0 step 151800 avg_loss 2.1406\n",
      "epoch 0 step 152000 avg_loss 2.1405\n",
      "epoch 0 step 152200 avg_loss 2.1405\n",
      "epoch 0 step 152400 avg_loss 2.1406\n",
      "epoch 0 step 152600 avg_loss 2.1405\n",
      "epoch 0 step 152800 avg_loss 2.1403\n",
      "epoch 0 step 153000 avg_loss 2.1403\n",
      "epoch 0 step 153200 avg_loss 2.1404\n",
      "epoch 0 step 153400 avg_loss 2.1406\n",
      "epoch 0 step 153600 avg_loss 2.1407\n",
      "epoch 0 step 153800 avg_loss 2.1408\n",
      "epoch 0 step 154000 avg_loss 2.1408\n",
      "epoch 0 step 154200 avg_loss 2.1407\n",
      "epoch 0 step 154400 avg_loss 2.1408\n",
      "epoch 0 step 154600 avg_loss 2.1410\n",
      "epoch 0 step 154800 avg_loss 2.1408\n",
      "epoch 0 step 155000 avg_loss 2.1406\n",
      "epoch 0 step 155200 avg_loss 2.1406\n",
      "epoch 0 step 155400 avg_loss 2.1405\n",
      "epoch 0 step 155600 avg_loss 2.1404\n",
      "epoch 0 step 155800 avg_loss 2.1404\n",
      "epoch 0 step 156000 avg_loss 2.1413\n",
      "epoch 0 step 156200 avg_loss 2.1416\n",
      "epoch 0 step 156400 avg_loss 2.1414\n",
      "epoch 0 step 156600 avg_loss 2.1414\n",
      "epoch 0 step 156800 avg_loss 2.1412\n",
      "epoch 0 step 157000 avg_loss 2.1412\n",
      "epoch 0 step 157200 avg_loss 2.1412\n",
      "epoch 0 step 157400 avg_loss 2.1409\n",
      "epoch 0 step 157600 avg_loss 2.1409\n",
      "epoch 0 step 157800 avg_loss 2.1409\n",
      "epoch 0 step 158000 avg_loss 2.1408\n",
      "epoch 0 step 158200 avg_loss 2.1407\n",
      "epoch 0 step 158400 avg_loss 2.1408\n",
      "epoch 0 step 158600 avg_loss 2.1407\n",
      "epoch 0 step 158800 avg_loss 2.1405\n",
      "epoch 0 step 159000 avg_loss 2.1404\n",
      "epoch 0 step 159200 avg_loss 2.1403\n",
      "epoch 0 step 159400 avg_loss 2.1403\n",
      "epoch 0 step 159600 avg_loss 2.1402\n",
      "epoch 0 step 159800 avg_loss 2.1399\n",
      "epoch 0 step 160000 avg_loss 2.1399\n",
      "epoch 0 step 160200 avg_loss 2.1399\n",
      "epoch 0 step 160400 avg_loss 2.1397\n",
      "epoch 0 step 160600 avg_loss 2.1396\n",
      "epoch 0 step 160800 avg_loss 2.1395\n",
      "epoch 0 step 161000 avg_loss 2.1395\n",
      "epoch 0 step 161200 avg_loss 2.1397\n",
      "epoch 0 step 161400 avg_loss 2.1396\n",
      "epoch 0 step 161600 avg_loss 2.1396\n",
      "epoch 0 step 161800 avg_loss 2.1397\n",
      "epoch 0 step 162000 avg_loss 2.1397\n",
      "epoch 0 step 162200 avg_loss 2.1397\n",
      "epoch 0 step 162400 avg_loss 2.1396\n",
      "epoch 0 step 162600 avg_loss 2.1396\n",
      "epoch 0 step 162800 avg_loss 2.1397\n",
      "epoch 0 step 163000 avg_loss 2.1396\n",
      "epoch 0 step 163200 avg_loss 2.1396\n",
      "epoch 0 step 163400 avg_loss 2.1396\n",
      "epoch 0 step 163600 avg_loss 2.1396\n",
      "epoch 0 step 163800 avg_loss 2.1395\n",
      "epoch 0 step 164000 avg_loss 2.1394\n",
      "epoch 0 step 164200 avg_loss 2.1395\n",
      "epoch 0 step 164400 avg_loss 2.1394\n",
      "epoch 0 step 164600 avg_loss 2.1391\n",
      "epoch 0 step 164800 avg_loss 2.1393\n",
      "epoch 0 step 165000 avg_loss 2.1392\n",
      "epoch 0 step 165200 avg_loss 2.1392\n",
      "epoch 0 step 165400 avg_loss 2.1390\n",
      "epoch 0 step 165600 avg_loss 2.1389\n",
      "epoch 0 step 165800 avg_loss 2.1388\n",
      "epoch 0 step 166000 avg_loss 2.1387\n",
      "epoch 0 step 166200 avg_loss 2.1385\n",
      "epoch 0 step 166400 avg_loss 2.1386\n",
      "epoch 0 step 166600 avg_loss 2.1383\n",
      "epoch 0 step 166800 avg_loss 2.1383\n",
      "epoch 0 step 167000 avg_loss 2.1382\n",
      "epoch 0 step 167200 avg_loss 2.1382\n",
      "epoch 0 step 167400 avg_loss 2.1381\n",
      "epoch 0 step 167600 avg_loss 2.1379\n",
      "epoch 0 step 167800 avg_loss 2.1378\n",
      "epoch 0 step 168000 avg_loss 2.1379\n",
      "epoch 0 step 168200 avg_loss 2.1380\n",
      "epoch 0 step 168400 avg_loss 2.1378\n",
      "epoch 0 step 168600 avg_loss 2.1378\n",
      "epoch 0 step 168800 avg_loss 2.1377\n",
      "epoch 0 step 169000 avg_loss 2.1378\n",
      "epoch 0 step 169200 avg_loss 2.1377\n",
      "epoch 0 step 169400 avg_loss 2.1377\n",
      "epoch 0 step 169600 avg_loss 2.1376\n",
      "epoch 0 step 169800 avg_loss 2.1376\n",
      "epoch 0 step 170000 avg_loss 2.1375\n",
      "epoch 0 step 170200 avg_loss 2.1375\n",
      "epoch 0 step 170400 avg_loss 2.1375\n",
      "epoch 0 step 170600 avg_loss 2.1375\n",
      "epoch 0 step 170800 avg_loss 2.1376\n",
      "epoch 0 step 171000 avg_loss 2.1376\n",
      "epoch 0 step 171200 avg_loss 2.1376\n",
      "epoch 0 step 171400 avg_loss 2.1376\n",
      "epoch 0 step 171600 avg_loss 2.1376\n",
      "epoch 0 step 171800 avg_loss 2.1375\n",
      "epoch 0 step 172000 avg_loss 2.1374\n",
      "epoch 0 step 172200 avg_loss 2.1374\n",
      "epoch 0 step 172400 avg_loss 2.1373\n",
      "epoch 0 step 172600 avg_loss 2.1373\n",
      "epoch 0 step 172800 avg_loss 2.1374\n",
      "epoch 0 step 173000 avg_loss 2.1377\n",
      "epoch 0 step 173200 avg_loss 2.1376\n",
      "epoch 0 step 173400 avg_loss 2.1377\n",
      "epoch 0 step 173600 avg_loss 2.1377\n",
      "epoch 0 step 173800 avg_loss 2.1376\n",
      "epoch 0 step 174000 avg_loss 2.1376\n",
      "epoch 0 step 174200 avg_loss 2.1373\n",
      "epoch 0 step 174400 avg_loss 2.1375\n",
      "epoch 0 step 174600 avg_loss 2.1377\n",
      "epoch 0 step 174800 avg_loss 2.1376\n",
      "epoch 0 step 175000 avg_loss 2.1377\n",
      "epoch 0 step 175200 avg_loss 2.1376\n",
      "epoch 0 step 175400 avg_loss 2.1377\n",
      "epoch 0 step 175600 avg_loss 2.1375\n",
      "epoch 0 step 175800 avg_loss 2.1375\n",
      "epoch 0 step 176000 avg_loss 2.1375\n",
      "epoch 0 step 176200 avg_loss 2.1377\n",
      "epoch 0 step 176400 avg_loss 2.1375\n",
      "epoch 0 step 176600 avg_loss 2.1374\n",
      "epoch 0 step 176800 avg_loss 2.1374\n",
      "epoch 0 step 177000 avg_loss 2.1376\n",
      "epoch 0 step 177200 avg_loss 2.1376\n",
      "epoch 0 step 177400 avg_loss 2.1376\n",
      "epoch 0 step 177600 avg_loss 2.1375\n",
      "epoch 0 step 177800 avg_loss 2.1376\n",
      "epoch 0 step 178000 avg_loss 2.1377\n",
      "epoch 0 step 178200 avg_loss 2.1375\n",
      "epoch 0 step 178400 avg_loss 2.1374\n",
      "epoch 0 step 178600 avg_loss 2.1373\n",
      "epoch 0 step 178800 avg_loss 2.1372\n",
      "epoch 0 step 179000 avg_loss 2.1371\n",
      "epoch 0 step 179200 avg_loss 2.1371\n",
      "epoch 0 step 179400 avg_loss 2.1370\n",
      "epoch 0 step 179600 avg_loss 2.1368\n",
      "epoch 0 step 179800 avg_loss 2.1366\n",
      "epoch 0 step 180000 avg_loss 2.1367\n",
      "epoch 0 step 180200 avg_loss 2.1366\n",
      "epoch 0 step 180400 avg_loss 2.1366\n",
      "epoch 0 step 180600 avg_loss 2.1366\n",
      "epoch 0 step 180800 avg_loss 2.1365\n",
      "epoch 0 step 181000 avg_loss 2.1364\n",
      "epoch 0 step 181200 avg_loss 2.1365\n",
      "epoch 0 step 181400 avg_loss 2.1363\n",
      "epoch 0 step 181600 avg_loss 2.1364\n",
      "epoch 0 step 181800 avg_loss 2.1362\n",
      "epoch 0 step 182000 avg_loss 2.1360\n",
      "epoch 0 step 182200 avg_loss 2.1360\n",
      "epoch 0 step 182400 avg_loss 2.1362\n",
      "epoch 0 step 182600 avg_loss 2.1364\n",
      "epoch 0 step 182800 avg_loss 2.1364\n",
      "epoch 0 step 183000 avg_loss 2.1363\n",
      "epoch 0 step 183200 avg_loss 2.1364\n",
      "epoch 0 step 183400 avg_loss 2.1364\n",
      "epoch 0 step 183600 avg_loss 2.1364\n",
      "epoch 0 step 183800 avg_loss 2.1362\n",
      "epoch 0 step 184000 avg_loss 2.1362\n",
      "epoch 0 step 184200 avg_loss 2.1361\n",
      "epoch 0 step 184400 avg_loss 2.1360\n",
      "epoch 0 step 184600 avg_loss 2.1359\n",
      "epoch 0 step 184800 avg_loss 2.1357\n",
      "epoch 0 step 185000 avg_loss 2.1357\n",
      "epoch 0 step 185200 avg_loss 2.1356\n",
      "epoch 0 step 185400 avg_loss 2.1356\n",
      "epoch 0 step 185600 avg_loss 2.1355\n",
      "epoch 0 step 185800 avg_loss 2.1354\n",
      "epoch 0 step 186000 avg_loss 2.1354\n",
      "epoch 0 step 186200 avg_loss 2.1354\n",
      "epoch 0 step 186400 avg_loss 2.1354\n",
      "epoch 0 step 186600 avg_loss 2.1352\n",
      "epoch 0 step 186800 avg_loss 2.1352\n",
      "epoch 0 step 187000 avg_loss 2.1351\n",
      "epoch 0 step 187200 avg_loss 2.1352\n",
      "epoch 0 step 187400 avg_loss 2.1353\n",
      "epoch 0 step 187600 avg_loss 2.1353\n",
      "epoch 0 step 187800 avg_loss 2.1354\n",
      "epoch 0 step 188000 avg_loss 2.1357\n",
      "epoch 0 step 188200 avg_loss 2.1357\n",
      "epoch 0 step 188400 avg_loss 2.1356\n",
      "epoch 0 step 188600 avg_loss 2.1356\n",
      "epoch 0 step 188800 avg_loss 2.1357\n",
      "epoch 0 step 189000 avg_loss 2.1356\n",
      "epoch 0 step 189200 avg_loss 2.1357\n",
      "epoch 0 step 189400 avg_loss 2.1355\n",
      "epoch 0 step 189600 avg_loss 2.1354\n",
      "epoch 0 step 189800 avg_loss 2.1354\n",
      "epoch 0 step 190000 avg_loss 2.1353\n",
      "epoch 0 step 190200 avg_loss 2.1353\n",
      "epoch 0 step 190400 avg_loss 2.1355\n",
      "epoch 0 step 190600 avg_loss 2.1355\n",
      "epoch 0 step 190800 avg_loss 2.1353\n",
      "epoch 0 step 191000 avg_loss 2.1353\n",
      "epoch 0 step 191200 avg_loss 2.1353\n",
      "epoch 0 step 191400 avg_loss 2.1352\n",
      "epoch 0 step 191600 avg_loss 2.1352\n",
      "epoch 0 step 191800 avg_loss 2.1351\n",
      "epoch 0 step 192000 avg_loss 2.1351\n",
      "epoch 0 step 192200 avg_loss 2.1352\n",
      "epoch 0 step 192400 avg_loss 2.1352\n",
      "epoch 0 step 192600 avg_loss 2.1352\n",
      "epoch 0 step 192800 avg_loss 2.1352\n",
      "epoch 0 step 193000 avg_loss 2.1352\n",
      "epoch 0 step 193200 avg_loss 2.1352\n",
      "epoch 0 step 193400 avg_loss 2.1355\n",
      "epoch 0 step 193600 avg_loss 2.1355\n",
      "epoch 0 step 193800 avg_loss 2.1355\n",
      "epoch 0 step 194000 avg_loss 2.1352\n",
      "epoch 0 step 194200 avg_loss 2.1353\n",
      "epoch 0 step 194400 avg_loss 2.1352\n",
      "epoch 0 step 194600 avg_loss 2.1352\n",
      "epoch 0 step 194800 avg_loss 2.1354\n",
      "epoch 0 step 195000 avg_loss 2.1353\n",
      "epoch 0 step 195200 avg_loss 2.1352\n",
      "Epoch 0 finished avg_loss 2.1351\n",
      "Epoch 0 done — time 5807.1s, avg_loss 2.1351, throughput 2152.6 samples/sec\n",
      "Saved checkpoint for epoch 0\n",
      "epoch 1 step 200 avg_loss 2.2024\n",
      "epoch 1 step 400 avg_loss 2.1974\n",
      "epoch 1 step 600 avg_loss 2.1897\n",
      "epoch 1 step 800 avg_loss 2.1417\n",
      "epoch 1 step 1000 avg_loss 2.1623\n",
      "epoch 1 step 1200 avg_loss 2.1313\n",
      "epoch 1 step 1400 avg_loss 2.1306\n",
      "epoch 1 step 1600 avg_loss 2.1370\n",
      "epoch 1 step 1800 avg_loss 2.1377\n",
      "epoch 1 step 2000 avg_loss 2.1394\n",
      "epoch 1 step 2200 avg_loss 2.1258\n",
      "epoch 1 step 2400 avg_loss 2.1160\n",
      "epoch 1 step 2600 avg_loss 2.1152\n",
      "epoch 1 step 2800 avg_loss 2.1185\n",
      "epoch 1 step 3000 avg_loss 2.1139\n",
      "epoch 1 step 3200 avg_loss 2.1109\n",
      "epoch 1 step 3400 avg_loss 2.1114\n",
      "epoch 1 step 3600 avg_loss 2.1026\n",
      "epoch 1 step 3800 avg_loss 2.0982\n",
      "epoch 1 step 4000 avg_loss 2.0974\n",
      "epoch 1 step 4200 avg_loss 2.0978\n",
      "epoch 1 step 4400 avg_loss 2.0957\n",
      "epoch 1 step 4600 avg_loss 2.0950\n",
      "epoch 1 step 4800 avg_loss 2.0944\n",
      "epoch 1 step 5000 avg_loss 2.0916\n",
      "epoch 1 step 5200 avg_loss 2.0878\n",
      "epoch 1 step 5400 avg_loss 2.0836\n",
      "epoch 1 step 5600 avg_loss 2.0820\n",
      "epoch 1 step 5800 avg_loss 2.0794\n",
      "epoch 1 step 6000 avg_loss 2.0796\n",
      "epoch 1 step 6200 avg_loss 2.0850\n",
      "epoch 1 step 6400 avg_loss 2.0844\n",
      "epoch 1 step 6600 avg_loss 2.0902\n",
      "epoch 1 step 6800 avg_loss 2.0885\n",
      "epoch 1 step 7000 avg_loss 2.0901\n",
      "epoch 1 step 7200 avg_loss 2.0927\n",
      "epoch 1 step 7400 avg_loss 2.0960\n",
      "epoch 1 step 7600 avg_loss 2.0970\n",
      "epoch 1 step 7800 avg_loss 2.0953\n",
      "epoch 1 step 8000 avg_loss 2.1011\n",
      "epoch 1 step 8200 avg_loss 2.0974\n",
      "epoch 1 step 8400 avg_loss 2.0919\n",
      "epoch 1 step 8600 avg_loss 2.0952\n",
      "epoch 1 step 8800 avg_loss 2.0949\n",
      "epoch 1 step 9000 avg_loss 2.0945\n",
      "epoch 1 step 9200 avg_loss 2.0916\n",
      "epoch 1 step 9400 avg_loss 2.0899\n",
      "epoch 1 step 9600 avg_loss 2.0886\n",
      "epoch 1 step 9800 avg_loss 2.0852\n",
      "epoch 1 step 10000 avg_loss 2.0855\n",
      "epoch 1 step 10200 avg_loss 2.0852\n",
      "epoch 1 step 10400 avg_loss 2.0851\n",
      "epoch 1 step 10600 avg_loss 2.0830\n",
      "epoch 1 step 10800 avg_loss 2.0832\n",
      "epoch 1 step 11000 avg_loss 2.0831\n",
      "epoch 1 step 11200 avg_loss 2.0818\n",
      "epoch 1 step 11400 avg_loss 2.0791\n",
      "epoch 1 step 11600 avg_loss 2.0791\n",
      "epoch 1 step 11800 avg_loss 2.0800\n",
      "epoch 1 step 12000 avg_loss 2.0803\n",
      "epoch 1 step 12200 avg_loss 2.0795\n",
      "epoch 1 step 12400 avg_loss 2.0802\n",
      "epoch 1 step 12600 avg_loss 2.0833\n",
      "epoch 1 step 12800 avg_loss 2.0814\n",
      "epoch 1 step 13000 avg_loss 2.0832\n",
      "epoch 1 step 13200 avg_loss 2.0842\n",
      "epoch 1 step 13400 avg_loss 2.0841\n",
      "epoch 1 step 13600 avg_loss 2.0855\n",
      "epoch 1 step 13800 avg_loss 2.0892\n",
      "epoch 1 step 14000 avg_loss 2.0907\n",
      "epoch 1 step 14200 avg_loss 2.0878\n",
      "epoch 1 step 14400 avg_loss 2.0861\n",
      "epoch 1 step 14600 avg_loss 2.0877\n",
      "epoch 1 step 14800 avg_loss 2.0871\n",
      "epoch 1 step 15000 avg_loss 2.0893\n",
      "epoch 1 step 15200 avg_loss 2.0886\n",
      "epoch 1 step 15400 avg_loss 2.0890\n",
      "epoch 1 step 15600 avg_loss 2.0877\n",
      "epoch 1 step 15800 avg_loss 2.0869\n",
      "epoch 1 step 16000 avg_loss 2.0857\n",
      "epoch 1 step 16200 avg_loss 2.0853\n",
      "epoch 1 step 16400 avg_loss 2.0857\n",
      "epoch 1 step 16600 avg_loss 2.0868\n",
      "epoch 1 step 16800 avg_loss 2.0877\n",
      "epoch 1 step 17000 avg_loss 2.0874\n",
      "epoch 1 step 17200 avg_loss 2.0870\n",
      "epoch 1 step 17400 avg_loss 2.0890\n",
      "epoch 1 step 17600 avg_loss 2.0883\n",
      "epoch 1 step 17800 avg_loss 2.0885\n",
      "epoch 1 step 18000 avg_loss 2.0884\n",
      "epoch 1 step 18200 avg_loss 2.0889\n",
      "epoch 1 step 18400 avg_loss 2.0888\n",
      "epoch 1 step 18600 avg_loss 2.0887\n",
      "epoch 1 step 18800 avg_loss 2.0893\n",
      "epoch 1 step 19000 avg_loss 2.0894\n",
      "epoch 1 step 19200 avg_loss 2.0892\n",
      "epoch 1 step 19400 avg_loss 2.0879\n",
      "epoch 1 step 19600 avg_loss 2.0882\n",
      "epoch 1 step 19800 avg_loss 2.0884\n",
      "epoch 1 step 20000 avg_loss 2.0883\n",
      "epoch 1 step 20200 avg_loss 2.0870\n",
      "epoch 1 step 20400 avg_loss 2.0858\n",
      "epoch 1 step 20600 avg_loss 2.0865\n",
      "epoch 1 step 20800 avg_loss 2.0851\n",
      "epoch 1 step 21000 avg_loss 2.0850\n",
      "epoch 1 step 21200 avg_loss 2.0853\n",
      "epoch 1 step 21400 avg_loss 2.0851\n",
      "epoch 1 step 21600 avg_loss 2.0868\n",
      "epoch 1 step 21800 avg_loss 2.0871\n",
      "epoch 1 step 22000 avg_loss 2.0866\n",
      "epoch 1 step 22200 avg_loss 2.0874\n",
      "epoch 1 step 22400 avg_loss 2.0865\n",
      "epoch 1 step 22600 avg_loss 2.0867\n",
      "epoch 1 step 22800 avg_loss 2.0864\n",
      "epoch 1 step 23000 avg_loss 2.0871\n",
      "epoch 1 step 23200 avg_loss 2.0867\n",
      "epoch 1 step 23400 avg_loss 2.0874\n",
      "epoch 1 step 23600 avg_loss 2.0873\n",
      "epoch 1 step 23800 avg_loss 2.0869\n",
      "epoch 1 step 24000 avg_loss 2.0880\n",
      "epoch 1 step 24200 avg_loss 2.0885\n",
      "epoch 1 step 24400 avg_loss 2.0894\n",
      "epoch 1 step 24600 avg_loss 2.0884\n",
      "epoch 1 step 24800 avg_loss 2.0888\n",
      "epoch 1 step 25000 avg_loss 2.0885\n",
      "epoch 1 step 25200 avg_loss 2.0885\n",
      "epoch 1 step 25400 avg_loss 2.0885\n",
      "epoch 1 step 25600 avg_loss 2.0890\n",
      "epoch 1 step 25800 avg_loss 2.0896\n",
      "epoch 1 step 26000 avg_loss 2.0891\n",
      "epoch 1 step 26200 avg_loss 2.0883\n",
      "epoch 1 step 26400 avg_loss 2.0877\n",
      "epoch 1 step 26600 avg_loss 2.0879\n",
      "epoch 1 step 26800 avg_loss 2.0890\n",
      "epoch 1 step 27000 avg_loss 2.0893\n",
      "epoch 1 step 27200 avg_loss 2.0892\n",
      "epoch 1 step 27400 avg_loss 2.0899\n",
      "epoch 1 step 27600 avg_loss 2.0910\n",
      "epoch 1 step 27800 avg_loss 2.0926\n",
      "epoch 1 step 28000 avg_loss 2.0923\n",
      "epoch 1 step 28200 avg_loss 2.0913\n",
      "epoch 1 step 28400 avg_loss 2.0913\n",
      "epoch 1 step 28600 avg_loss 2.0906\n",
      "epoch 1 step 28800 avg_loss 2.0897\n",
      "epoch 1 step 29000 avg_loss 2.0908\n",
      "epoch 1 step 29200 avg_loss 2.0907\n",
      "epoch 1 step 29400 avg_loss 2.0911\n",
      "epoch 1 step 29600 avg_loss 2.0918\n",
      "epoch 1 step 29800 avg_loss 2.0917\n",
      "epoch 1 step 30000 avg_loss 2.0921\n",
      "epoch 1 step 30200 avg_loss 2.0909\n",
      "epoch 1 step 30400 avg_loss 2.0916\n",
      "epoch 1 step 30600 avg_loss 2.0915\n",
      "epoch 1 step 30800 avg_loss 2.0922\n",
      "epoch 1 step 31000 avg_loss 2.0921\n",
      "epoch 1 step 31200 avg_loss 2.0919\n",
      "epoch 1 step 31400 avg_loss 2.0917\n",
      "epoch 1 step 31600 avg_loss 2.0912\n",
      "epoch 1 step 31800 avg_loss 2.0916\n",
      "epoch 1 step 32000 avg_loss 2.0914\n",
      "epoch 1 step 32200 avg_loss 2.0920\n",
      "epoch 1 step 32400 avg_loss 2.0916\n",
      "epoch 1 step 32600 avg_loss 2.0924\n",
      "epoch 1 step 32800 avg_loss 2.0917\n",
      "epoch 1 step 33000 avg_loss 2.0916\n",
      "epoch 1 step 33200 avg_loss 2.0911\n",
      "epoch 1 step 33400 avg_loss 2.0910\n",
      "epoch 1 step 33600 avg_loss 2.0910\n",
      "epoch 1 step 33800 avg_loss 2.0911\n",
      "epoch 1 step 34000 avg_loss 2.0913\n",
      "epoch 1 step 34200 avg_loss 2.0918\n",
      "epoch 1 step 34400 avg_loss 2.0912\n",
      "epoch 1 step 34600 avg_loss 2.0912\n",
      "epoch 1 step 34800 avg_loss 2.0906\n",
      "epoch 1 step 35000 avg_loss 2.0899\n",
      "epoch 1 step 35200 avg_loss 2.0900\n",
      "epoch 1 step 35400 avg_loss 2.0898\n",
      "epoch 1 step 35600 avg_loss 2.0891\n",
      "epoch 1 step 35800 avg_loss 2.0890\n",
      "epoch 1 step 36000 avg_loss 2.0883\n",
      "epoch 1 step 36200 avg_loss 2.0880\n",
      "epoch 1 step 36400 avg_loss 2.0879\n",
      "epoch 1 step 36600 avg_loss 2.0883\n",
      "epoch 1 step 36800 avg_loss 2.0885\n",
      "epoch 1 step 37000 avg_loss 2.0881\n",
      "epoch 1 step 37200 avg_loss 2.0878\n",
      "epoch 1 step 37400 avg_loss 2.0883\n",
      "epoch 1 step 37600 avg_loss 2.0901\n",
      "epoch 1 step 37800 avg_loss 2.0901\n",
      "epoch 1 step 38000 avg_loss 2.0899\n",
      "epoch 1 step 38200 avg_loss 2.0906\n",
      "epoch 1 step 38400 avg_loss 2.0909\n",
      "epoch 1 step 38600 avg_loss 2.0905\n",
      "epoch 1 step 38800 avg_loss 2.0907\n",
      "epoch 1 step 39000 avg_loss 2.0917\n",
      "epoch 1 step 39200 avg_loss 2.0918\n",
      "epoch 1 step 39400 avg_loss 2.0916\n",
      "epoch 1 step 39600 avg_loss 2.0907\n",
      "epoch 1 step 39800 avg_loss 2.0904\n",
      "epoch 1 step 40000 avg_loss 2.0901\n",
      "epoch 1 step 40200 avg_loss 2.0903\n",
      "epoch 1 step 40400 avg_loss 2.0908\n",
      "epoch 1 step 40600 avg_loss 2.0915\n",
      "epoch 1 step 40800 avg_loss 2.0916\n",
      "epoch 1 step 41000 avg_loss 2.0918\n",
      "epoch 1 step 41200 avg_loss 2.0917\n",
      "epoch 1 step 41400 avg_loss 2.0908\n",
      "epoch 1 step 41600 avg_loss 2.0905\n",
      "epoch 1 step 41800 avg_loss 2.0903\n",
      "epoch 1 step 42000 avg_loss 2.0906\n",
      "epoch 1 step 42200 avg_loss 2.0905\n",
      "epoch 1 step 42400 avg_loss 2.0902\n",
      "epoch 1 step 42600 avg_loss 2.0905\n",
      "epoch 1 step 42800 avg_loss 2.0902\n",
      "epoch 1 step 43000 avg_loss 2.0898\n",
      "epoch 1 step 43200 avg_loss 2.0903\n",
      "epoch 1 step 43400 avg_loss 2.0897\n",
      "epoch 1 step 43600 avg_loss 2.0899\n",
      "epoch 1 step 43800 avg_loss 2.0896\n",
      "epoch 1 step 44000 avg_loss 2.0899\n",
      "epoch 1 step 44200 avg_loss 2.0895\n",
      "epoch 1 step 44400 avg_loss 2.0896\n",
      "epoch 1 step 44600 avg_loss 2.0894\n",
      "epoch 1 step 44800 avg_loss 2.0894\n",
      "epoch 1 step 45000 avg_loss 2.0894\n",
      "epoch 1 step 45200 avg_loss 2.0893\n",
      "epoch 1 step 45400 avg_loss 2.0893\n",
      "epoch 1 step 45600 avg_loss 2.0896\n",
      "epoch 1 step 45800 avg_loss 2.0899\n",
      "epoch 1 step 46000 avg_loss 2.0902\n",
      "epoch 1 step 46200 avg_loss 2.0900\n",
      "epoch 1 step 46400 avg_loss 2.0902\n",
      "epoch 1 step 46600 avg_loss 2.0903\n",
      "epoch 1 step 46800 avg_loss 2.0900\n",
      "epoch 1 step 47000 avg_loss 2.0896\n",
      "epoch 1 step 47200 avg_loss 2.0897\n",
      "epoch 1 step 47400 avg_loss 2.0899\n",
      "epoch 1 step 47600 avg_loss 2.0897\n",
      "epoch 1 step 47800 avg_loss 2.0895\n",
      "epoch 1 step 48000 avg_loss 2.0894\n",
      "epoch 1 step 48200 avg_loss 2.0895\n",
      "epoch 1 step 48400 avg_loss 2.0895\n",
      "epoch 1 step 48600 avg_loss 2.0892\n",
      "epoch 1 step 48800 avg_loss 2.0899\n",
      "epoch 1 step 49000 avg_loss 2.0905\n",
      "epoch 1 step 49200 avg_loss 2.0910\n",
      "epoch 1 step 49400 avg_loss 2.0910\n",
      "epoch 1 step 49600 avg_loss 2.0909\n",
      "epoch 1 step 49800 avg_loss 2.0911\n",
      "epoch 1 step 50000 avg_loss 2.0907\n",
      "epoch 1 step 50200 avg_loss 2.0910\n",
      "epoch 1 step 50400 avg_loss 2.0910\n",
      "epoch 1 step 50600 avg_loss 2.0906\n",
      "epoch 1 step 50800 avg_loss 2.0906\n",
      "epoch 1 step 51000 avg_loss 2.0912\n",
      "epoch 1 step 51200 avg_loss 2.0912\n",
      "epoch 1 step 51400 avg_loss 2.0914\n",
      "epoch 1 step 51600 avg_loss 2.0918\n",
      "epoch 1 step 51800 avg_loss 2.0913\n",
      "epoch 1 step 52000 avg_loss 2.0907\n",
      "epoch 1 step 52200 avg_loss 2.0912\n",
      "epoch 1 step 52400 avg_loss 2.0920\n",
      "epoch 1 step 52600 avg_loss 2.0920\n",
      "epoch 1 step 52800 avg_loss 2.0916\n",
      "epoch 1 step 53000 avg_loss 2.0919\n",
      "epoch 1 step 53200 avg_loss 2.0917\n",
      "epoch 1 step 53400 avg_loss 2.0917\n",
      "epoch 1 step 53600 avg_loss 2.0910\n",
      "epoch 1 step 53800 avg_loss 2.0910\n",
      "epoch 1 step 54000 avg_loss 2.0909\n",
      "epoch 1 step 54200 avg_loss 2.0907\n",
      "epoch 1 step 54400 avg_loss 2.0900\n",
      "epoch 1 step 54600 avg_loss 2.0899\n",
      "epoch 1 step 54800 avg_loss 2.0901\n",
      "epoch 1 step 55000 avg_loss 2.0897\n",
      "epoch 1 step 55200 avg_loss 2.0895\n",
      "epoch 1 step 55400 avg_loss 2.0896\n",
      "epoch 1 step 55600 avg_loss 2.0893\n",
      "epoch 1 step 55800 avg_loss 2.0897\n",
      "epoch 1 step 56000 avg_loss 2.0904\n",
      "epoch 1 step 56200 avg_loss 2.0904\n",
      "epoch 1 step 56400 avg_loss 2.0906\n",
      "epoch 1 step 56600 avg_loss 2.0906\n",
      "epoch 1 step 56800 avg_loss 2.0907\n",
      "epoch 1 step 57000 avg_loss 2.0905\n",
      "epoch 1 step 57200 avg_loss 2.0908\n",
      "epoch 1 step 57400 avg_loss 2.0912\n",
      "epoch 1 step 57600 avg_loss 2.0910\n",
      "epoch 1 step 57800 avg_loss 2.0907\n",
      "epoch 1 step 58000 avg_loss 2.0908\n",
      "epoch 1 step 58200 avg_loss 2.0911\n",
      "epoch 1 step 58400 avg_loss 2.0916\n",
      "epoch 1 step 58600 avg_loss 2.0916\n",
      "epoch 1 step 58800 avg_loss 2.0911\n",
      "epoch 1 step 59000 avg_loss 2.0908\n",
      "epoch 1 step 59200 avg_loss 2.0904\n",
      "epoch 1 step 59400 avg_loss 2.0902\n",
      "epoch 1 step 59600 avg_loss 2.0888\n",
      "epoch 1 step 59800 avg_loss 2.0890\n",
      "epoch 1 step 60000 avg_loss 2.0887\n",
      "epoch 1 step 60200 avg_loss 2.0886\n",
      "epoch 1 step 60400 avg_loss 2.0887\n",
      "epoch 1 step 60600 avg_loss 2.0886\n",
      "epoch 1 step 60800 avg_loss 2.0885\n",
      "epoch 1 step 61000 avg_loss 2.0885\n",
      "epoch 1 step 61200 avg_loss 2.0877\n",
      "epoch 1 step 61400 avg_loss 2.0874\n",
      "epoch 1 step 61600 avg_loss 2.0873\n",
      "epoch 1 step 61800 avg_loss 2.0868\n",
      "epoch 1 step 62000 avg_loss 2.0868\n",
      "epoch 1 step 62200 avg_loss 2.0867\n",
      "epoch 1 step 62400 avg_loss 2.0866\n",
      "epoch 1 step 62600 avg_loss 2.0869\n",
      "epoch 1 step 62800 avg_loss 2.0863\n",
      "epoch 1 step 63000 avg_loss 2.0860\n",
      "epoch 1 step 63200 avg_loss 2.0861\n",
      "epoch 1 step 63400 avg_loss 2.0856\n",
      "epoch 1 step 63600 avg_loss 2.0858\n",
      "epoch 1 step 63800 avg_loss 2.0859\n",
      "epoch 1 step 64000 avg_loss 2.0860\n",
      "epoch 1 step 64200 avg_loss 2.0859\n",
      "epoch 1 step 64400 avg_loss 2.0858\n",
      "epoch 1 step 64600 avg_loss 2.0859\n",
      "epoch 1 step 64800 avg_loss 2.0857\n",
      "epoch 1 step 65000 avg_loss 2.0856\n",
      "epoch 1 step 65200 avg_loss 2.0859\n",
      "epoch 1 step 65400 avg_loss 2.0858\n",
      "epoch 1 step 65600 avg_loss 2.0857\n",
      "epoch 1 step 65800 avg_loss 2.0860\n",
      "epoch 1 step 66000 avg_loss 2.0862\n",
      "epoch 1 step 66200 avg_loss 2.0860\n",
      "epoch 1 step 66400 avg_loss 2.0857\n",
      "epoch 1 step 66600 avg_loss 2.0858\n",
      "epoch 1 step 66800 avg_loss 2.0856\n",
      "epoch 1 step 67000 avg_loss 2.0861\n",
      "epoch 1 step 67200 avg_loss 2.0858\n",
      "epoch 1 step 67400 avg_loss 2.0853\n",
      "epoch 1 step 67600 avg_loss 2.0854\n",
      "epoch 1 step 67800 avg_loss 2.0852\n",
      "epoch 1 step 68000 avg_loss 2.0850\n",
      "epoch 1 step 68200 avg_loss 2.0849\n",
      "epoch 1 step 68400 avg_loss 2.0850\n",
      "epoch 1 step 68600 avg_loss 2.0846\n",
      "epoch 1 step 68800 avg_loss 2.0851\n",
      "epoch 1 step 69000 avg_loss 2.0853\n",
      "epoch 1 step 69200 avg_loss 2.0850\n",
      "epoch 1 step 69400 avg_loss 2.0853\n",
      "epoch 1 step 69600 avg_loss 2.0851\n",
      "epoch 1 step 69800 avg_loss 2.0850\n",
      "epoch 1 step 70000 avg_loss 2.0850\n",
      "epoch 1 step 70200 avg_loss 2.0850\n",
      "epoch 1 step 70400 avg_loss 2.0847\n",
      "epoch 1 step 70600 avg_loss 2.0846\n",
      "epoch 1 step 70800 avg_loss 2.0844\n",
      "epoch 1 step 71000 avg_loss 2.0844\n",
      "epoch 1 step 71200 avg_loss 2.0848\n",
      "epoch 1 step 71400 avg_loss 2.0843\n",
      "epoch 1 step 71600 avg_loss 2.0840\n",
      "epoch 1 step 71800 avg_loss 2.0842\n",
      "epoch 1 step 72000 avg_loss 2.0842\n",
      "epoch 1 step 72200 avg_loss 2.0841\n",
      "epoch 1 step 72400 avg_loss 2.0847\n",
      "epoch 1 step 72600 avg_loss 2.0844\n",
      "epoch 1 step 72800 avg_loss 2.0842\n",
      "epoch 1 step 73000 avg_loss 2.0840\n",
      "epoch 1 step 73200 avg_loss 2.0838\n",
      "epoch 1 step 73400 avg_loss 2.0840\n",
      "epoch 1 step 73600 avg_loss 2.0838\n",
      "epoch 1 step 73800 avg_loss 2.0833\n",
      "epoch 1 step 74000 avg_loss 2.0834\n",
      "epoch 1 step 74200 avg_loss 2.0834\n",
      "epoch 1 step 74400 avg_loss 2.0833\n",
      "epoch 1 step 74600 avg_loss 2.0829\n",
      "epoch 1 step 74800 avg_loss 2.0835\n",
      "epoch 1 step 75000 avg_loss 2.0836\n",
      "epoch 1 step 75200 avg_loss 2.0834\n",
      "epoch 1 step 75400 avg_loss 2.0831\n",
      "epoch 1 step 75600 avg_loss 2.0836\n",
      "epoch 1 step 75800 avg_loss 2.0838\n",
      "epoch 1 step 76000 avg_loss 2.0840\n",
      "epoch 1 step 76200 avg_loss 2.0839\n",
      "epoch 1 step 76400 avg_loss 2.0841\n",
      "epoch 1 step 76600 avg_loss 2.0841\n",
      "epoch 1 step 76800 avg_loss 2.0839\n",
      "epoch 1 step 77000 avg_loss 2.0839\n",
      "epoch 1 step 77200 avg_loss 2.0838\n",
      "epoch 1 step 77400 avg_loss 2.0837\n",
      "epoch 1 step 77600 avg_loss 2.0836\n",
      "epoch 1 step 77800 avg_loss 2.0837\n",
      "epoch 1 step 78000 avg_loss 2.0836\n",
      "epoch 1 step 78200 avg_loss 2.0842\n",
      "epoch 1 step 78400 avg_loss 2.0841\n",
      "epoch 1 step 78600 avg_loss 2.0843\n",
      "epoch 1 step 78800 avg_loss 2.0840\n",
      "epoch 1 step 79000 avg_loss 2.0838\n",
      "epoch 1 step 79200 avg_loss 2.0841\n",
      "epoch 1 step 79400 avg_loss 2.0844\n",
      "epoch 1 step 79600 avg_loss 2.0845\n",
      "epoch 1 step 79800 avg_loss 2.0845\n",
      "epoch 1 step 80000 avg_loss 2.0844\n",
      "epoch 1 step 80200 avg_loss 2.0849\n",
      "epoch 1 step 80400 avg_loss 2.0849\n",
      "epoch 1 step 80600 avg_loss 2.0846\n",
      "epoch 1 step 80800 avg_loss 2.0842\n",
      "epoch 1 step 81000 avg_loss 2.0844\n",
      "epoch 1 step 81200 avg_loss 2.0841\n",
      "epoch 1 step 81400 avg_loss 2.0840\n",
      "epoch 1 step 81600 avg_loss 2.0839\n",
      "epoch 1 step 81800 avg_loss 2.0839\n",
      "epoch 1 step 82000 avg_loss 2.0834\n",
      "epoch 1 step 82200 avg_loss 2.0833\n",
      "epoch 1 step 82400 avg_loss 2.0833\n",
      "epoch 1 step 82600 avg_loss 2.0827\n",
      "epoch 1 step 82800 avg_loss 2.0828\n",
      "epoch 1 step 83000 avg_loss 2.0829\n",
      "epoch 1 step 83200 avg_loss 2.0832\n",
      "epoch 1 step 83400 avg_loss 2.0832\n",
      "epoch 1 step 83600 avg_loss 2.0831\n",
      "epoch 1 step 83800 avg_loss 2.0830\n",
      "epoch 1 step 84000 avg_loss 2.0830\n",
      "epoch 1 step 84200 avg_loss 2.0832\n",
      "epoch 1 step 84400 avg_loss 2.0831\n",
      "epoch 1 step 84600 avg_loss 2.0831\n",
      "epoch 1 step 84800 avg_loss 2.0832\n",
      "epoch 1 step 85000 avg_loss 2.0831\n",
      "epoch 1 step 85200 avg_loss 2.0832\n",
      "epoch 1 step 85400 avg_loss 2.0830\n",
      "epoch 1 step 85600 avg_loss 2.0833\n",
      "epoch 1 step 85800 avg_loss 2.0833\n",
      "epoch 1 step 86000 avg_loss 2.0839\n",
      "epoch 1 step 86200 avg_loss 2.0840\n",
      "epoch 1 step 86400 avg_loss 2.0839\n",
      "epoch 1 step 86600 avg_loss 2.0839\n",
      "epoch 1 step 86800 avg_loss 2.0836\n",
      "epoch 1 step 87000 avg_loss 2.0834\n",
      "epoch 1 step 87200 avg_loss 2.0835\n",
      "epoch 1 step 87400 avg_loss 2.0834\n",
      "epoch 1 step 87600 avg_loss 2.0835\n",
      "epoch 1 step 87800 avg_loss 2.0834\n",
      "epoch 1 step 88000 avg_loss 2.0835\n",
      "epoch 1 step 88200 avg_loss 2.0834\n",
      "epoch 1 step 88400 avg_loss 2.0831\n",
      "epoch 1 step 88600 avg_loss 2.0830\n",
      "epoch 1 step 88800 avg_loss 2.0828\n",
      "epoch 1 step 89000 avg_loss 2.0827\n",
      "epoch 1 step 89200 avg_loss 2.0825\n",
      "epoch 1 step 89400 avg_loss 2.0823\n",
      "epoch 1 step 89600 avg_loss 2.0824\n",
      "epoch 1 step 89800 avg_loss 2.0827\n",
      "epoch 1 step 90000 avg_loss 2.0824\n",
      "epoch 1 step 90200 avg_loss 2.0824\n",
      "epoch 1 step 90400 avg_loss 2.0821\n",
      "epoch 1 step 90600 avg_loss 2.0821\n",
      "epoch 1 step 90800 avg_loss 2.0819\n",
      "epoch 1 step 91000 avg_loss 2.0818\n",
      "epoch 1 step 91200 avg_loss 2.0814\n",
      "epoch 1 step 91400 avg_loss 2.0814\n",
      "epoch 1 step 91600 avg_loss 2.0817\n",
      "epoch 1 step 91800 avg_loss 2.0815\n",
      "epoch 1 step 92000 avg_loss 2.0814\n",
      "epoch 1 step 92200 avg_loss 2.0815\n",
      "epoch 1 step 92400 avg_loss 2.0816\n",
      "epoch 1 step 92600 avg_loss 2.0817\n",
      "epoch 1 step 92800 avg_loss 2.0813\n",
      "epoch 1 step 93000 avg_loss 2.0813\n",
      "epoch 1 step 93200 avg_loss 2.0811\n",
      "epoch 1 step 93400 avg_loss 2.0815\n",
      "epoch 1 step 93600 avg_loss 2.0816\n",
      "epoch 1 step 93800 avg_loss 2.0821\n",
      "epoch 1 step 94000 avg_loss 2.0818\n",
      "epoch 1 step 94200 avg_loss 2.0815\n",
      "epoch 1 step 94400 avg_loss 2.0816\n",
      "epoch 1 step 94600 avg_loss 2.0819\n",
      "epoch 1 step 94800 avg_loss 2.0821\n",
      "epoch 1 step 95000 avg_loss 2.0825\n",
      "epoch 1 step 95200 avg_loss 2.0827\n",
      "epoch 1 step 95400 avg_loss 2.0827\n",
      "epoch 1 step 95600 avg_loss 2.0827\n",
      "epoch 1 step 95800 avg_loss 2.0828\n",
      "epoch 1 step 96000 avg_loss 2.0831\n",
      "epoch 1 step 96200 avg_loss 2.0829\n",
      "epoch 1 step 96400 avg_loss 2.0826\n",
      "epoch 1 step 96600 avg_loss 2.0826\n",
      "epoch 1 step 96800 avg_loss 2.0826\n",
      "epoch 1 step 97000 avg_loss 2.0825\n",
      "epoch 1 step 97200 avg_loss 2.0825\n",
      "epoch 1 step 97400 avg_loss 2.0840\n",
      "epoch 1 step 97600 avg_loss 2.0845\n",
      "epoch 1 step 97800 avg_loss 2.0845\n",
      "epoch 1 step 98000 avg_loss 2.0844\n",
      "epoch 1 step 98200 avg_loss 2.0845\n",
      "epoch 1 step 98400 avg_loss 2.0849\n",
      "epoch 1 step 98600 avg_loss 2.0852\n",
      "epoch 1 step 98800 avg_loss 2.0852\n",
      "epoch 1 step 99000 avg_loss 2.0856\n",
      "epoch 1 step 99200 avg_loss 2.0852\n",
      "epoch 1 step 99400 avg_loss 2.0851\n",
      "epoch 1 step 99600 avg_loss 2.0851\n",
      "epoch 1 step 99800 avg_loss 2.0849\n",
      "epoch 1 step 100000 avg_loss 2.0846\n",
      "epoch 1 step 100200 avg_loss 2.0848\n",
      "epoch 1 step 100400 avg_loss 2.0847\n",
      "epoch 1 step 100600 avg_loss 2.0847\n",
      "epoch 1 step 100800 avg_loss 2.0850\n",
      "epoch 1 step 101000 avg_loss 2.0851\n",
      "epoch 1 step 101200 avg_loss 2.0851\n",
      "epoch 1 step 101400 avg_loss 2.0850\n",
      "epoch 1 step 101600 avg_loss 2.0851\n",
      "epoch 1 step 101800 avg_loss 2.0851\n",
      "epoch 1 step 102000 avg_loss 2.0850\n",
      "epoch 1 step 102200 avg_loss 2.0850\n",
      "epoch 1 step 102400 avg_loss 2.0850\n",
      "epoch 1 step 102600 avg_loss 2.0853\n",
      "epoch 1 step 102800 avg_loss 2.0851\n",
      "epoch 1 step 103000 avg_loss 2.0850\n",
      "epoch 1 step 103200 avg_loss 2.0849\n",
      "epoch 1 step 103400 avg_loss 2.0846\n",
      "epoch 1 step 103600 avg_loss 2.0848\n",
      "epoch 1 step 103800 avg_loss 2.0848\n",
      "epoch 1 step 104000 avg_loss 2.0846\n",
      "epoch 1 step 104200 avg_loss 2.0846\n",
      "epoch 1 step 104400 avg_loss 2.0844\n",
      "epoch 1 step 104600 avg_loss 2.0844\n",
      "epoch 1 step 104800 avg_loss 2.0840\n",
      "epoch 1 step 105000 avg_loss 2.0841\n",
      "epoch 1 step 105200 avg_loss 2.0838\n",
      "epoch 1 step 105400 avg_loss 2.0836\n",
      "epoch 1 step 105600 avg_loss 2.0834\n",
      "epoch 1 step 105800 avg_loss 2.0839\n",
      "epoch 1 step 106000 avg_loss 2.0838\n",
      "epoch 1 step 106200 avg_loss 2.0837\n",
      "epoch 1 step 106400 avg_loss 2.0836\n",
      "epoch 1 step 106600 avg_loss 2.0838\n",
      "epoch 1 step 106800 avg_loss 2.0837\n",
      "epoch 1 step 107000 avg_loss 2.0836\n",
      "epoch 1 step 107200 avg_loss 2.0835\n",
      "epoch 1 step 107400 avg_loss 2.0836\n",
      "epoch 1 step 107600 avg_loss 2.0840\n",
      "epoch 1 step 107800 avg_loss 2.0842\n",
      "epoch 1 step 108000 avg_loss 2.0841\n",
      "epoch 1 step 108200 avg_loss 2.0843\n",
      "epoch 1 step 108400 avg_loss 2.0839\n",
      "epoch 1 step 108600 avg_loss 2.0840\n",
      "epoch 1 step 108800 avg_loss 2.0839\n",
      "epoch 1 step 109000 avg_loss 2.0839\n",
      "epoch 1 step 109200 avg_loss 2.0839\n",
      "epoch 1 step 109400 avg_loss 2.0839\n",
      "epoch 1 step 109600 avg_loss 2.0838\n",
      "epoch 1 step 109800 avg_loss 2.0842\n",
      "epoch 1 step 110000 avg_loss 2.0843\n",
      "epoch 1 step 110200 avg_loss 2.0842\n",
      "epoch 1 step 110400 avg_loss 2.0843\n",
      "epoch 1 step 110600 avg_loss 2.0844\n",
      "epoch 1 step 110800 avg_loss 2.0844\n",
      "epoch 1 step 111000 avg_loss 2.0847\n",
      "epoch 1 step 111200 avg_loss 2.0846\n",
      "epoch 1 step 111400 avg_loss 2.0845\n",
      "epoch 1 step 111600 avg_loss 2.0843\n",
      "epoch 1 step 111800 avg_loss 2.0840\n",
      "epoch 1 step 112000 avg_loss 2.0839\n",
      "epoch 1 step 112200 avg_loss 2.0837\n",
      "epoch 1 step 112400 avg_loss 2.0840\n",
      "epoch 1 step 112600 avg_loss 2.0842\n",
      "epoch 1 step 112800 avg_loss 2.0837\n",
      "epoch 1 step 113000 avg_loss 2.0835\n",
      "epoch 1 step 113200 avg_loss 2.0833\n",
      "epoch 1 step 113400 avg_loss 2.0832\n",
      "epoch 1 step 113600 avg_loss 2.0834\n",
      "epoch 1 step 113800 avg_loss 2.0835\n",
      "epoch 1 step 114000 avg_loss 2.0831\n",
      "epoch 1 step 114200 avg_loss 2.0832\n",
      "epoch 1 step 114400 avg_loss 2.0829\n",
      "epoch 1 step 114600 avg_loss 2.0828\n",
      "epoch 1 step 114800 avg_loss 2.0826\n",
      "epoch 1 step 115000 avg_loss 2.0826\n",
      "epoch 1 step 115200 avg_loss 2.0827\n",
      "epoch 1 step 115400 avg_loss 2.0828\n",
      "epoch 1 step 115600 avg_loss 2.0828\n",
      "epoch 1 step 115800 avg_loss 2.0826\n",
      "epoch 1 step 116000 avg_loss 2.0828\n",
      "epoch 1 step 116200 avg_loss 2.0827\n",
      "epoch 1 step 116400 avg_loss 2.0827\n",
      "epoch 1 step 116600 avg_loss 2.0828\n",
      "epoch 1 step 116800 avg_loss 2.0827\n",
      "epoch 1 step 117000 avg_loss 2.0828\n",
      "epoch 1 step 117200 avg_loss 2.0828\n",
      "epoch 1 step 117400 avg_loss 2.0828\n",
      "epoch 1 step 117600 avg_loss 2.0828\n",
      "epoch 1 step 117800 avg_loss 2.0828\n",
      "epoch 1 step 118000 avg_loss 2.0829\n",
      "epoch 1 step 118200 avg_loss 2.0829\n",
      "epoch 1 step 118400 avg_loss 2.0829\n",
      "epoch 1 step 118600 avg_loss 2.0830\n",
      "epoch 1 step 118800 avg_loss 2.0832\n",
      "epoch 1 step 119000 avg_loss 2.0830\n",
      "epoch 1 step 119200 avg_loss 2.0830\n",
      "epoch 1 step 119400 avg_loss 2.0829\n",
      "epoch 1 step 119600 avg_loss 2.0831\n",
      "epoch 1 step 119800 avg_loss 2.0831\n",
      "epoch 1 step 120000 avg_loss 2.0831\n",
      "epoch 1 step 120200 avg_loss 2.0832\n",
      "epoch 1 step 120400 avg_loss 2.0830\n",
      "epoch 1 step 120600 avg_loss 2.0829\n",
      "epoch 1 step 120800 avg_loss 2.0832\n",
      "epoch 1 step 121000 avg_loss 2.0830\n",
      "epoch 1 step 121200 avg_loss 2.0828\n",
      "epoch 1 step 121400 avg_loss 2.0827\n",
      "epoch 1 step 121600 avg_loss 2.0825\n",
      "epoch 1 step 121800 avg_loss 2.0827\n",
      "epoch 1 step 122000 avg_loss 2.0824\n",
      "epoch 1 step 122200 avg_loss 2.0825\n",
      "epoch 1 step 122400 avg_loss 2.0824\n",
      "epoch 1 step 122600 avg_loss 2.0824\n",
      "epoch 1 step 122800 avg_loss 2.0824\n",
      "epoch 1 step 123000 avg_loss 2.0822\n",
      "epoch 1 step 123200 avg_loss 2.0819\n",
      "epoch 1 step 123400 avg_loss 2.0819\n",
      "epoch 1 step 123600 avg_loss 2.0819\n",
      "epoch 1 step 123800 avg_loss 2.0818\n",
      "epoch 1 step 124000 avg_loss 2.0819\n",
      "epoch 1 step 124200 avg_loss 2.0816\n",
      "epoch 1 step 124400 avg_loss 2.0816\n",
      "epoch 1 step 124600 avg_loss 2.0816\n",
      "epoch 1 step 124800 avg_loss 2.0814\n",
      "epoch 1 step 125000 avg_loss 2.0814\n",
      "epoch 1 step 125200 avg_loss 2.0815\n",
      "epoch 1 step 125400 avg_loss 2.0812\n",
      "epoch 1 step 125600 avg_loss 2.0810\n",
      "epoch 1 step 125800 avg_loss 2.0811\n",
      "epoch 1 step 126000 avg_loss 2.0813\n",
      "epoch 1 step 126200 avg_loss 2.0814\n",
      "epoch 1 step 126400 avg_loss 2.0817\n",
      "epoch 1 step 126600 avg_loss 2.0816\n",
      "epoch 1 step 126800 avg_loss 2.0815\n",
      "epoch 1 step 127000 avg_loss 2.0812\n",
      "epoch 1 step 127200 avg_loss 2.0813\n",
      "epoch 1 step 127400 avg_loss 2.0815\n",
      "epoch 1 step 127600 avg_loss 2.0814\n",
      "epoch 1 step 127800 avg_loss 2.0815\n",
      "epoch 1 step 128000 avg_loss 2.0814\n",
      "epoch 1 step 128200 avg_loss 2.0814\n",
      "epoch 1 step 128400 avg_loss 2.0814\n",
      "epoch 1 step 128600 avg_loss 2.0815\n",
      "epoch 1 step 128800 avg_loss 2.0816\n",
      "epoch 1 step 129000 avg_loss 2.0816\n",
      "epoch 1 step 129200 avg_loss 2.0818\n",
      "epoch 1 step 129400 avg_loss 2.0817\n",
      "epoch 1 step 129600 avg_loss 2.0818\n",
      "epoch 1 step 129800 avg_loss 2.0818\n",
      "epoch 1 step 130000 avg_loss 2.0817\n",
      "epoch 1 step 130200 avg_loss 2.0817\n",
      "epoch 1 step 130400 avg_loss 2.0816\n",
      "epoch 1 step 130600 avg_loss 2.0819\n",
      "epoch 1 step 130800 avg_loss 2.0819\n",
      "epoch 1 step 131000 avg_loss 2.0819\n",
      "epoch 1 step 131200 avg_loss 2.0818\n",
      "epoch 1 step 131400 avg_loss 2.0819\n",
      "epoch 1 step 131600 avg_loss 2.0817\n",
      "epoch 1 step 131800 avg_loss 2.0818\n",
      "epoch 1 step 132000 avg_loss 2.0817\n",
      "epoch 1 step 132200 avg_loss 2.0816\n",
      "epoch 1 step 132400 avg_loss 2.0817\n",
      "epoch 1 step 132600 avg_loss 2.0818\n",
      "epoch 1 step 132800 avg_loss 2.0822\n",
      "epoch 1 step 133000 avg_loss 2.0824\n",
      "epoch 1 step 133200 avg_loss 2.0826\n",
      "epoch 1 step 133400 avg_loss 2.0827\n",
      "epoch 1 step 133600 avg_loss 2.0828\n",
      "epoch 1 step 133800 avg_loss 2.0827\n",
      "epoch 1 step 134000 avg_loss 2.0830\n",
      "epoch 1 step 134200 avg_loss 2.0828\n",
      "epoch 1 step 134400 avg_loss 2.0828\n",
      "epoch 1 step 134600 avg_loss 2.0828\n",
      "epoch 1 step 134800 avg_loss 2.0827\n",
      "epoch 1 step 135000 avg_loss 2.0827\n",
      "epoch 1 step 135200 avg_loss 2.0826\n",
      "epoch 1 step 135400 avg_loss 2.0827\n",
      "epoch 1 step 135600 avg_loss 2.0826\n",
      "epoch 1 step 135800 avg_loss 2.0829\n",
      "epoch 1 step 136000 avg_loss 2.0827\n",
      "epoch 1 step 136200 avg_loss 2.0833\n",
      "epoch 1 step 136400 avg_loss 2.0832\n",
      "epoch 1 step 136600 avg_loss 2.0831\n",
      "epoch 1 step 136800 avg_loss 2.0831\n",
      "epoch 1 step 137000 avg_loss 2.0833\n",
      "epoch 1 step 137200 avg_loss 2.0833\n",
      "epoch 1 step 137400 avg_loss 2.0830\n",
      "epoch 1 step 137600 avg_loss 2.0830\n",
      "epoch 1 step 137800 avg_loss 2.0830\n",
      "epoch 1 step 138000 avg_loss 2.0829\n",
      "epoch 1 step 138200 avg_loss 2.0831\n",
      "epoch 1 step 138400 avg_loss 2.0832\n",
      "epoch 1 step 138600 avg_loss 2.0833\n",
      "epoch 1 step 138800 avg_loss 2.0837\n",
      "epoch 1 step 139000 avg_loss 2.0835\n",
      "epoch 1 step 139200 avg_loss 2.0837\n",
      "epoch 1 step 139400 avg_loss 2.0834\n",
      "epoch 1 step 139600 avg_loss 2.0834\n",
      "epoch 1 step 139800 avg_loss 2.0834\n",
      "epoch 1 step 140000 avg_loss 2.0833\n",
      "epoch 1 step 140200 avg_loss 2.0833\n",
      "epoch 1 step 140400 avg_loss 2.0831\n",
      "epoch 1 step 140600 avg_loss 2.0831\n",
      "epoch 1 step 140800 avg_loss 2.0832\n",
      "epoch 1 step 141000 avg_loss 2.0835\n",
      "epoch 1 step 141200 avg_loss 2.0838\n",
      "epoch 1 step 141400 avg_loss 2.0837\n",
      "epoch 1 step 141600 avg_loss 2.0836\n",
      "epoch 1 step 141800 avg_loss 2.0837\n",
      "epoch 1 step 142000 avg_loss 2.0838\n",
      "epoch 1 step 142200 avg_loss 2.0837\n",
      "epoch 1 step 142400 avg_loss 2.0836\n",
      "epoch 1 step 142600 avg_loss 2.0836\n",
      "epoch 1 step 142800 avg_loss 2.0836\n",
      "epoch 1 step 143000 avg_loss 2.0836\n",
      "epoch 1 step 143200 avg_loss 2.0834\n",
      "epoch 1 step 143400 avg_loss 2.0837\n",
      "epoch 1 step 143600 avg_loss 2.0839\n",
      "epoch 1 step 143800 avg_loss 2.0838\n",
      "epoch 1 step 144000 avg_loss 2.0836\n",
      "epoch 1 step 144200 avg_loss 2.0838\n",
      "epoch 1 step 144400 avg_loss 2.0836\n",
      "epoch 1 step 144600 avg_loss 2.0835\n",
      "epoch 1 step 144800 avg_loss 2.0834\n",
      "epoch 1 step 145000 avg_loss 2.0834\n",
      "epoch 1 step 145200 avg_loss 2.0835\n",
      "epoch 1 step 145400 avg_loss 2.0836\n",
      "epoch 1 step 145600 avg_loss 2.0838\n",
      "epoch 1 step 145800 avg_loss 2.0840\n",
      "epoch 1 step 146000 avg_loss 2.0841\n",
      "epoch 1 step 146200 avg_loss 2.0843\n",
      "epoch 1 step 146400 avg_loss 2.0843\n",
      "epoch 1 step 146600 avg_loss 2.0842\n",
      "epoch 1 step 146800 avg_loss 2.0843\n",
      "epoch 1 step 147000 avg_loss 2.0846\n",
      "epoch 1 step 147200 avg_loss 2.0849\n",
      "epoch 1 step 147400 avg_loss 2.0848\n",
      "epoch 1 step 147600 avg_loss 2.0849\n",
      "epoch 1 step 147800 avg_loss 2.0849\n",
      "epoch 1 step 148000 avg_loss 2.0846\n",
      "epoch 1 step 148200 avg_loss 2.0846\n",
      "epoch 1 step 148400 avg_loss 2.0845\n",
      "epoch 1 step 148600 avg_loss 2.0845\n",
      "epoch 1 step 148800 avg_loss 2.0844\n",
      "epoch 1 step 149000 avg_loss 2.0844\n",
      "epoch 1 step 149200 avg_loss 2.0844\n",
      "epoch 1 step 149400 avg_loss 2.0846\n",
      "epoch 1 step 149600 avg_loss 2.0844\n",
      "epoch 1 step 149800 avg_loss 2.0845\n",
      "epoch 1 step 150000 avg_loss 2.0842\n",
      "epoch 1 step 150200 avg_loss 2.0841\n",
      "epoch 1 step 150400 avg_loss 2.0840\n",
      "epoch 1 step 150600 avg_loss 2.0842\n",
      "epoch 1 step 150800 avg_loss 2.0842\n",
      "epoch 1 step 151000 avg_loss 2.0845\n",
      "epoch 1 step 151200 avg_loss 2.0844\n",
      "epoch 1 step 151400 avg_loss 2.0843\n",
      "epoch 1 step 151600 avg_loss 2.0842\n",
      "epoch 1 step 151800 avg_loss 2.0840\n",
      "epoch 1 step 152000 avg_loss 2.0840\n",
      "epoch 1 step 152200 avg_loss 2.0838\n",
      "epoch 1 step 152400 avg_loss 2.0838\n",
      "epoch 1 step 152600 avg_loss 2.0838\n",
      "epoch 1 step 152800 avg_loss 2.0837\n",
      "epoch 1 step 153000 avg_loss 2.0836\n",
      "epoch 1 step 153200 avg_loss 2.0838\n",
      "epoch 1 step 153400 avg_loss 2.0838\n",
      "epoch 1 step 153600 avg_loss 2.0838\n",
      "epoch 1 step 153800 avg_loss 2.0835\n",
      "epoch 1 step 154000 avg_loss 2.0833\n",
      "epoch 1 step 154200 avg_loss 2.0832\n",
      "epoch 1 step 154400 avg_loss 2.0833\n",
      "epoch 1 step 154600 avg_loss 2.0834\n",
      "epoch 1 step 154800 avg_loss 2.0833\n",
      "epoch 1 step 155000 avg_loss 2.0834\n",
      "epoch 1 step 155200 avg_loss 2.0832\n",
      "epoch 1 step 155400 avg_loss 2.0832\n",
      "epoch 1 step 155600 avg_loss 2.0833\n",
      "epoch 1 step 155800 avg_loss 2.0833\n",
      "epoch 1 step 156000 avg_loss 2.0833\n",
      "epoch 1 step 156200 avg_loss 2.0834\n",
      "epoch 1 step 156400 avg_loss 2.0835\n",
      "epoch 1 step 156600 avg_loss 2.0835\n",
      "epoch 1 step 156800 avg_loss 2.0833\n",
      "epoch 1 step 157000 avg_loss 2.0833\n",
      "epoch 1 step 157200 avg_loss 2.0832\n",
      "epoch 1 step 157400 avg_loss 2.0832\n",
      "epoch 1 step 157600 avg_loss 2.0830\n",
      "epoch 1 step 157800 avg_loss 2.0829\n",
      "epoch 1 step 158000 avg_loss 2.0829\n",
      "epoch 1 step 158200 avg_loss 2.0829\n",
      "epoch 1 step 158400 avg_loss 2.0829\n",
      "epoch 1 step 158600 avg_loss 2.0829\n",
      "epoch 1 step 158800 avg_loss 2.0829\n",
      "epoch 1 step 159000 avg_loss 2.0829\n",
      "epoch 1 step 159200 avg_loss 2.0828\n",
      "epoch 1 step 159400 avg_loss 2.0829\n",
      "epoch 1 step 159600 avg_loss 2.0829\n",
      "epoch 1 step 159800 avg_loss 2.0826\n",
      "epoch 1 step 160000 avg_loss 2.0824\n",
      "epoch 1 step 160200 avg_loss 2.0822\n",
      "epoch 1 step 160400 avg_loss 2.0822\n",
      "epoch 1 step 160600 avg_loss 2.0821\n",
      "epoch 1 step 160800 avg_loss 2.0821\n",
      "epoch 1 step 161000 avg_loss 2.0823\n",
      "epoch 1 step 161200 avg_loss 2.0827\n",
      "epoch 1 step 161400 avg_loss 2.0827\n",
      "epoch 1 step 161600 avg_loss 2.0828\n",
      "epoch 1 step 161800 avg_loss 2.0827\n",
      "epoch 1 step 162000 avg_loss 2.0827\n",
      "epoch 1 step 162200 avg_loss 2.0826\n",
      "epoch 1 step 162400 avg_loss 2.0825\n",
      "epoch 1 step 162600 avg_loss 2.0827\n",
      "epoch 1 step 162800 avg_loss 2.0829\n",
      "epoch 1 step 163000 avg_loss 2.0828\n",
      "epoch 1 step 163200 avg_loss 2.0830\n",
      "epoch 1 step 163400 avg_loss 2.0829\n",
      "epoch 1 step 163600 avg_loss 2.0830\n",
      "epoch 1 step 163800 avg_loss 2.0829\n",
      "epoch 1 step 164000 avg_loss 2.0829\n",
      "epoch 1 step 164200 avg_loss 2.0828\n",
      "epoch 1 step 164400 avg_loss 2.0829\n",
      "epoch 1 step 164600 avg_loss 2.0827\n",
      "epoch 1 step 164800 avg_loss 2.0827\n",
      "epoch 1 step 165000 avg_loss 2.0827\n",
      "epoch 1 step 165200 avg_loss 2.0828\n",
      "epoch 1 step 165400 avg_loss 2.0828\n",
      "epoch 1 step 165600 avg_loss 2.0831\n",
      "epoch 1 step 165800 avg_loss 2.0830\n",
      "epoch 1 step 166000 avg_loss 2.0830\n",
      "epoch 1 step 166200 avg_loss 2.0829\n",
      "epoch 1 step 166400 avg_loss 2.0829\n",
      "epoch 1 step 166600 avg_loss 2.0828\n",
      "epoch 1 step 166800 avg_loss 2.0828\n",
      "epoch 1 step 167000 avg_loss 2.0828\n",
      "epoch 1 step 167200 avg_loss 2.0829\n",
      "epoch 1 step 167400 avg_loss 2.0827\n",
      "epoch 1 step 167600 avg_loss 2.0828\n",
      "epoch 1 step 167800 avg_loss 2.0829\n",
      "epoch 1 step 168000 avg_loss 2.0827\n",
      "epoch 1 step 168200 avg_loss 2.0826\n",
      "epoch 1 step 168400 avg_loss 2.0828\n",
      "epoch 1 step 168600 avg_loss 2.0826\n",
      "epoch 1 step 168800 avg_loss 2.0827\n",
      "epoch 1 step 169000 avg_loss 2.0827\n",
      "epoch 1 step 169200 avg_loss 2.0826\n",
      "epoch 1 step 169400 avg_loss 2.0827\n",
      "epoch 1 step 169600 avg_loss 2.0827\n",
      "epoch 1 step 169800 avg_loss 2.0827\n",
      "epoch 1 step 170000 avg_loss 2.0827\n",
      "epoch 1 step 170200 avg_loss 2.0827\n",
      "epoch 1 step 170400 avg_loss 2.0827\n",
      "epoch 1 step 170600 avg_loss 2.0828\n",
      "epoch 1 step 170800 avg_loss 2.0829\n",
      "epoch 1 step 171000 avg_loss 2.0830\n",
      "epoch 1 step 171200 avg_loss 2.0829\n",
      "epoch 1 step 171400 avg_loss 2.0831\n",
      "epoch 1 step 171600 avg_loss 2.0832\n",
      "epoch 1 step 171800 avg_loss 2.0832\n",
      "epoch 1 step 172000 avg_loss 2.0831\n",
      "epoch 1 step 172200 avg_loss 2.0831\n",
      "epoch 1 step 172400 avg_loss 2.0831\n",
      "epoch 1 step 172600 avg_loss 2.0831\n",
      "epoch 1 step 172800 avg_loss 2.0831\n",
      "epoch 1 step 173000 avg_loss 2.0831\n",
      "epoch 1 step 173200 avg_loss 2.0830\n",
      "epoch 1 step 173400 avg_loss 2.0831\n",
      "epoch 1 step 173600 avg_loss 2.0830\n",
      "epoch 1 step 173800 avg_loss 2.0831\n",
      "epoch 1 step 174000 avg_loss 2.0829\n",
      "epoch 1 step 174200 avg_loss 2.0828\n",
      "epoch 1 step 174400 avg_loss 2.0828\n",
      "epoch 1 step 174600 avg_loss 2.0830\n",
      "epoch 1 step 174800 avg_loss 2.0832\n",
      "epoch 1 step 175000 avg_loss 2.0832\n",
      "epoch 1 step 175200 avg_loss 2.0831\n",
      "epoch 1 step 175400 avg_loss 2.0832\n",
      "epoch 1 step 175600 avg_loss 2.0833\n",
      "epoch 1 step 175800 avg_loss 2.0833\n",
      "epoch 1 step 176000 avg_loss 2.0832\n",
      "epoch 1 step 176200 avg_loss 2.0833\n",
      "epoch 1 step 176400 avg_loss 2.0832\n",
      "epoch 1 step 176600 avg_loss 2.0831\n",
      "epoch 1 step 176800 avg_loss 2.0832\n",
      "epoch 1 step 177000 avg_loss 2.0832\n",
      "epoch 1 step 177200 avg_loss 2.0832\n",
      "epoch 1 step 177400 avg_loss 2.0830\n",
      "epoch 1 step 177600 avg_loss 2.0830\n",
      "epoch 1 step 177800 avg_loss 2.0830\n",
      "epoch 1 step 178000 avg_loss 2.0831\n",
      "epoch 1 step 178200 avg_loss 2.0830\n",
      "epoch 1 step 178400 avg_loss 2.0832\n",
      "epoch 1 step 178600 avg_loss 2.0833\n",
      "epoch 1 step 178800 avg_loss 2.0832\n",
      "epoch 1 step 179000 avg_loss 2.0832\n",
      "epoch 1 step 179200 avg_loss 2.0832\n",
      "epoch 1 step 179400 avg_loss 2.0831\n",
      "epoch 1 step 179600 avg_loss 2.0832\n",
      "epoch 1 step 179800 avg_loss 2.0833\n",
      "epoch 1 step 180000 avg_loss 2.0834\n",
      "epoch 1 step 180200 avg_loss 2.0834\n",
      "epoch 1 step 180400 avg_loss 2.0835\n",
      "epoch 1 step 180600 avg_loss 2.0838\n",
      "epoch 1 step 180800 avg_loss 2.0838\n",
      "epoch 1 step 181000 avg_loss 2.0838\n",
      "epoch 1 step 181200 avg_loss 2.0839\n",
      "epoch 1 step 181400 avg_loss 2.0840\n",
      "epoch 1 step 181600 avg_loss 2.0839\n",
      "epoch 1 step 181800 avg_loss 2.0839\n",
      "epoch 1 step 182000 avg_loss 2.0837\n",
      "epoch 1 step 182200 avg_loss 2.0837\n",
      "epoch 1 step 182400 avg_loss 2.0837\n",
      "epoch 1 step 182600 avg_loss 2.0837\n",
      "epoch 1 step 182800 avg_loss 2.0837\n",
      "epoch 1 step 183000 avg_loss 2.0837\n",
      "epoch 1 step 183200 avg_loss 2.0839\n",
      "epoch 1 step 183400 avg_loss 2.0838\n",
      "epoch 1 step 183600 avg_loss 2.0838\n",
      "epoch 1 step 183800 avg_loss 2.0838\n",
      "epoch 1 step 184000 avg_loss 2.0837\n",
      "epoch 1 step 184200 avg_loss 2.0837\n",
      "epoch 1 step 184400 avg_loss 2.0835\n",
      "epoch 1 step 184600 avg_loss 2.0834\n",
      "epoch 1 step 184800 avg_loss 2.0834\n",
      "epoch 1 step 185000 avg_loss 2.0834\n",
      "epoch 1 step 185200 avg_loss 2.0836\n",
      "epoch 1 step 185400 avg_loss 2.0837\n",
      "epoch 1 step 185600 avg_loss 2.0838\n",
      "epoch 1 step 185800 avg_loss 2.0839\n",
      "epoch 1 step 186000 avg_loss 2.0838\n",
      "epoch 1 step 186200 avg_loss 2.0839\n",
      "epoch 1 step 186400 avg_loss 2.0839\n",
      "epoch 1 step 186600 avg_loss 2.0839\n",
      "epoch 1 step 186800 avg_loss 2.0841\n",
      "epoch 1 step 187000 avg_loss 2.0839\n",
      "epoch 1 step 187200 avg_loss 2.0837\n",
      "epoch 1 step 187400 avg_loss 2.0837\n",
      "epoch 1 step 187600 avg_loss 2.0837\n",
      "epoch 1 step 187800 avg_loss 2.0838\n",
      "epoch 1 step 188000 avg_loss 2.0838\n",
      "epoch 1 step 188200 avg_loss 2.0839\n",
      "epoch 1 step 188400 avg_loss 2.0840\n",
      "epoch 1 step 188600 avg_loss 2.0838\n",
      "epoch 1 step 188800 avg_loss 2.0836\n",
      "epoch 1 step 189000 avg_loss 2.0837\n",
      "epoch 1 step 189200 avg_loss 2.0839\n",
      "epoch 1 step 189400 avg_loss 2.0840\n",
      "epoch 1 step 189600 avg_loss 2.0840\n",
      "epoch 1 step 189800 avg_loss 2.0840\n",
      "epoch 1 step 190000 avg_loss 2.0841\n",
      "epoch 1 step 190200 avg_loss 2.0841\n",
      "epoch 1 step 190400 avg_loss 2.0843\n",
      "epoch 1 step 190600 avg_loss 2.0843\n",
      "epoch 1 step 190800 avg_loss 2.0842\n",
      "epoch 1 step 191000 avg_loss 2.0841\n",
      "epoch 1 step 191200 avg_loss 2.0841\n",
      "epoch 1 step 191400 avg_loss 2.0842\n",
      "epoch 1 step 191600 avg_loss 2.0840\n",
      "epoch 1 step 191800 avg_loss 2.0841\n",
      "epoch 1 step 192000 avg_loss 2.0839\n",
      "epoch 1 step 192200 avg_loss 2.0838\n",
      "epoch 1 step 192400 avg_loss 2.0838\n",
      "epoch 1 step 192600 avg_loss 2.0839\n",
      "epoch 1 step 192800 avg_loss 2.0838\n",
      "epoch 1 step 193000 avg_loss 2.0837\n",
      "epoch 1 step 193200 avg_loss 2.0836\n",
      "epoch 1 step 193400 avg_loss 2.0836\n",
      "epoch 1 step 193600 avg_loss 2.0836\n",
      "epoch 1 step 193800 avg_loss 2.0837\n",
      "epoch 1 step 194000 avg_loss 2.0837\n",
      "epoch 1 step 194200 avg_loss 2.0838\n",
      "epoch 1 step 194400 avg_loss 2.0837\n",
      "epoch 1 step 194600 avg_loss 2.0837\n",
      "epoch 1 step 194800 avg_loss 2.0836\n",
      "epoch 1 step 195000 avg_loss 2.0838\n",
      "epoch 1 step 195200 avg_loss 2.0837\n",
      "Epoch 1 finished avg_loss 2.0836\n",
      "Epoch 1 done — time 5786.2s, avg_loss 2.0836, throughput 2160.3 samples/sec\n",
      "Saved checkpoint for epoch 1\n",
      "Phase B finished.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Phase B training cell ----------\n",
    "NUM_SHARDS_B = 50\n",
    "EPOCHS_B = 2          # run 1-2 epochs\n",
    "SAVE_EVERY_EPOCH = True\n",
    "MAX_STEPS_PER_EPOCH = None  # or set a limit for debugging\n",
    "\n",
    "shard_paths = sorted(SHARD_DIR.glob(\"shard_*.pt\"))\n",
    "assert len(shard_paths) >= NUM_SHARDS_B, f\"Need >= {NUM_SHARDS_B} shards.\"\n",
    "\n",
    "train_shards = shard_paths[:NUM_SHARDS_B]\n",
    "print(f\"Training on {len(train_shards)} shards: {train_shards[0].name} ... {train_shards[-1].name}\")\n",
    "\n",
    "def make_loader(shards, batch_size=BATCH_SIZE):\n",
    "    ds = TensorShardDataset(shards, shuffle_shards=True)\n",
    "    return DataLoader(ds, batch_size=batch_size, collate_fn=collate_shard_batch, num_workers=0)\n",
    "\n",
    "def total_samples(shard_list):\n",
    "    tot = 0\n",
    "    for sp in shard_list:\n",
    "        d = torch.load(sp)\n",
    "        tot += int(d[\"prefix\"].size(0))\n",
    "    return tot\n",
    "\n",
    "samples_in_shards = total_samples(train_shards)\n",
    "print(\"Samples in shards:\", samples_in_shards)\n",
    "\n",
    "for epoch in range(EPOCHS_B):\n",
    "    loader = make_loader(train_shards, batch_size=BATCH_SIZE)\n",
    "    t0 = time.time()\n",
    "    avg_loss, avg_rec = train_one_epoch(model, loader, optimizer, scaler, epoch, max_steps=MAX_STEPS_PER_EPOCH)\n",
    "    t1 = time.time()\n",
    "    elapsed = t1 - t0\n",
    "    throughput = samples_in_shards / elapsed if elapsed>0 else float(\"nan\")\n",
    "    print(f\"Epoch {epoch} done — time {elapsed:.1f}s, avg_loss {avg_loss:.4f}, throughput {throughput:.1f} samples/sec\")\n",
    "    if SAVE_EVERY_EPOCH:\n",
    "        save_checkpoint(model, optimizer, epoch, name=f\"sasrec_phaseB_top{TOP_N_ITEMS}_epoch{epoch}\")\n",
    "        print(\"Saved checkpoint for epoch\", epoch)\n",
    "print(\"Phase B finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52475868",
   "metadata": {},
   "source": [
    "### Phase C — Full pretrain (90 shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a80c39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase C: Full Pretraining ===\n",
      "Training on 90 shards\n",
      "Example: shard_000.pt ... shard_089.pt\n",
      "No valid RESUME_CKPT found — training from current model state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2588\\4241680917.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(sp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples across shards: 22290168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2588\\559748316.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch = torch.load(p)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2588\\3293107640.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 200 avg_loss 2.0669\n",
      "epoch 0 step 400 avg_loss 2.0267\n",
      "epoch 0 step 600 avg_loss 2.0092\n",
      "epoch 0 step 800 avg_loss 2.0683\n",
      "epoch 0 step 1000 avg_loss 2.0875\n",
      "epoch 0 step 1200 avg_loss 2.0881\n",
      "epoch 0 step 1400 avg_loss 2.1173\n",
      "epoch 0 step 1600 avg_loss 2.1167\n",
      "epoch 0 step 1800 avg_loss 2.1101\n",
      "epoch 0 step 2000 avg_loss 2.0975\n",
      "epoch 0 step 2200 avg_loss 2.1147\n",
      "epoch 0 step 2400 avg_loss 2.1128\n",
      "epoch 0 step 2600 avg_loss 2.0951\n",
      "epoch 0 step 2800 avg_loss 2.0949\n",
      "epoch 0 step 3000 avg_loss 2.0872\n",
      "epoch 0 step 3200 avg_loss 2.0827\n",
      "epoch 0 step 3400 avg_loss 2.0797\n",
      "epoch 0 step 3600 avg_loss 2.1121\n",
      "epoch 0 step 3800 avg_loss 2.1277\n",
      "epoch 0 step 4000 avg_loss 2.1197\n",
      "epoch 0 step 4200 avg_loss 2.1192\n",
      "epoch 0 step 4400 avg_loss 2.1163\n",
      "epoch 0 step 4600 avg_loss 2.1124\n",
      "epoch 0 step 4800 avg_loss 2.1114\n",
      "epoch 0 step 5000 avg_loss 2.1138\n",
      "epoch 0 step 5200 avg_loss 2.1107\n",
      "epoch 0 step 5400 avg_loss 2.1069\n",
      "epoch 0 step 5600 avg_loss 2.1033\n",
      "epoch 0 step 5800 avg_loss 2.1010\n",
      "epoch 0 step 6000 avg_loss 2.1059\n",
      "epoch 0 step 6200 avg_loss 2.1019\n",
      "epoch 0 step 6400 avg_loss 2.1028\n",
      "epoch 0 step 6600 avg_loss 2.1057\n",
      "epoch 0 step 6800 avg_loss 2.1066\n",
      "epoch 0 step 7000 avg_loss 2.1048\n",
      "epoch 0 step 7200 avg_loss 2.1017\n",
      "epoch 0 step 7400 avg_loss 2.1037\n",
      "epoch 0 step 7600 avg_loss 2.0991\n",
      "epoch 0 step 7800 avg_loss 2.1014\n",
      "epoch 0 step 8000 avg_loss 2.1020\n",
      "epoch 0 step 8200 avg_loss 2.1015\n",
      "epoch 0 step 8400 avg_loss 2.0969\n",
      "epoch 0 step 8600 avg_loss 2.1001\n",
      "epoch 0 step 8800 avg_loss 2.0997\n",
      "epoch 0 step 9000 avg_loss 2.0948\n",
      "epoch 0 step 9200 avg_loss 2.0949\n",
      "epoch 0 step 9400 avg_loss 2.0924\n",
      "epoch 0 step 9600 avg_loss 2.0931\n",
      "epoch 0 step 9800 avg_loss 2.0927\n",
      "epoch 0 step 10000 avg_loss 2.0955\n",
      "epoch 0 step 10200 avg_loss 2.0926\n",
      "epoch 0 step 10400 avg_loss 2.0903\n",
      "epoch 0 step 10600 avg_loss 2.0895\n",
      "epoch 0 step 10800 avg_loss 2.0885\n",
      "epoch 0 step 11000 avg_loss 2.0887\n",
      "epoch 0 step 11200 avg_loss 2.0878\n",
      "epoch 0 step 11400 avg_loss 2.0826\n",
      "epoch 0 step 11600 avg_loss 2.0829\n",
      "epoch 0 step 11800 avg_loss 2.0834\n",
      "epoch 0 step 12000 avg_loss 2.0819\n",
      "epoch 0 step 12200 avg_loss 2.0800\n",
      "epoch 0 step 12400 avg_loss 2.0792\n",
      "epoch 0 step 12600 avg_loss 2.0773\n",
      "epoch 0 step 12800 avg_loss 2.0765\n",
      "epoch 0 step 13000 avg_loss 2.0767\n",
      "epoch 0 step 13200 avg_loss 2.0766\n",
      "epoch 0 step 13400 avg_loss 2.0781\n",
      "epoch 0 step 13600 avg_loss 2.0761\n",
      "epoch 0 step 13800 avg_loss 2.0753\n",
      "epoch 0 step 14000 avg_loss 2.0788\n",
      "epoch 0 step 14200 avg_loss 2.0808\n",
      "epoch 0 step 14400 avg_loss 2.0816\n",
      "epoch 0 step 14600 avg_loss 2.0809\n",
      "epoch 0 step 14800 avg_loss 2.0813\n",
      "epoch 0 step 15000 avg_loss 2.0823\n",
      "epoch 0 step 15200 avg_loss 2.0810\n",
      "epoch 0 step 15400 avg_loss 2.0817\n",
      "epoch 0 step 15600 avg_loss 2.0838\n",
      "epoch 0 step 15800 avg_loss 2.0837\n",
      "epoch 0 step 16000 avg_loss 2.0834\n",
      "epoch 0 step 16200 avg_loss 2.0820\n",
      "epoch 0 step 16400 avg_loss 2.0830\n",
      "epoch 0 step 16600 avg_loss 2.0836\n",
      "epoch 0 step 16800 avg_loss 2.0834\n",
      "epoch 0 step 17000 avg_loss 2.0820\n",
      "epoch 0 step 17200 avg_loss 2.0835\n",
      "epoch 0 step 17400 avg_loss 2.0827\n",
      "epoch 0 step 17600 avg_loss 2.0840\n",
      "epoch 0 step 17800 avg_loss 2.0846\n",
      "epoch 0 step 18000 avg_loss 2.0839\n",
      "epoch 0 step 18200 avg_loss 2.0857\n",
      "epoch 0 step 18400 avg_loss 2.0853\n",
      "epoch 0 step 18600 avg_loss 2.0861\n",
      "epoch 0 step 18800 avg_loss 2.0861\n",
      "epoch 0 step 19000 avg_loss 2.0879\n",
      "epoch 0 step 19200 avg_loss 2.0839\n",
      "epoch 0 step 19400 avg_loss 2.0833\n",
      "epoch 0 step 19600 avg_loss 2.0820\n",
      "epoch 0 step 19800 avg_loss 2.0824\n",
      "epoch 0 step 20000 avg_loss 2.0825\n",
      "epoch 0 step 20200 avg_loss 2.0821\n",
      "epoch 0 step 20400 avg_loss 2.0817\n",
      "epoch 0 step 20600 avg_loss 2.0807\n",
      "epoch 0 step 20800 avg_loss 2.0800\n",
      "epoch 0 step 21000 avg_loss 2.0796\n",
      "epoch 0 step 21200 avg_loss 2.0801\n",
      "epoch 0 step 21400 avg_loss 2.0799\n",
      "epoch 0 step 21600 avg_loss 2.0796\n",
      "epoch 0 step 21800 avg_loss 2.0811\n",
      "epoch 0 step 22000 avg_loss 2.0825\n",
      "epoch 0 step 22200 avg_loss 2.0826\n",
      "epoch 0 step 22400 avg_loss 2.0819\n",
      "epoch 0 step 22600 avg_loss 2.0829\n",
      "epoch 0 step 22800 avg_loss 2.0838\n",
      "epoch 0 step 23000 avg_loss 2.0842\n",
      "epoch 0 step 23200 avg_loss 2.0844\n",
      "epoch 0 step 23400 avg_loss 2.0849\n",
      "epoch 0 step 23600 avg_loss 2.0844\n",
      "epoch 0 step 23800 avg_loss 2.0857\n",
      "epoch 0 step 24000 avg_loss 2.0846\n",
      "epoch 0 step 24200 avg_loss 2.0840\n",
      "epoch 0 step 24400 avg_loss 2.0841\n",
      "epoch 0 step 24600 avg_loss 2.0843\n",
      "epoch 0 step 24800 avg_loss 2.0838\n",
      "epoch 0 step 25000 avg_loss 2.0831\n",
      "epoch 0 step 25200 avg_loss 2.0823\n",
      "epoch 0 step 25400 avg_loss 2.0828\n",
      "epoch 0 step 25600 avg_loss 2.0842\n",
      "epoch 0 step 25800 avg_loss 2.0852\n",
      "epoch 0 step 26000 avg_loss 2.0840\n",
      "epoch 0 step 26200 avg_loss 2.0840\n",
      "epoch 0 step 26400 avg_loss 2.0830\n",
      "epoch 0 step 26600 avg_loss 2.0829\n",
      "epoch 0 step 26800 avg_loss 2.0825\n",
      "epoch 0 step 27000 avg_loss 2.0824\n",
      "epoch 0 step 27200 avg_loss 2.0827\n",
      "epoch 0 step 27400 avg_loss 2.0821\n",
      "epoch 0 step 27600 avg_loss 2.0827\n",
      "epoch 0 step 27800 avg_loss 2.0831\n",
      "epoch 0 step 28000 avg_loss 2.0831\n",
      "epoch 0 step 28200 avg_loss 2.0836\n",
      "epoch 0 step 28400 avg_loss 2.0826\n",
      "epoch 0 step 28600 avg_loss 2.0820\n",
      "epoch 0 step 28800 avg_loss 2.0824\n",
      "epoch 0 step 29000 avg_loss 2.0833\n",
      "epoch 0 step 29200 avg_loss 2.0834\n",
      "epoch 0 step 29400 avg_loss 2.0828\n",
      "epoch 0 step 29600 avg_loss 2.0834\n",
      "epoch 0 step 29800 avg_loss 2.0826\n",
      "epoch 0 step 30000 avg_loss 2.0824\n",
      "epoch 0 step 30200 avg_loss 2.0813\n",
      "epoch 0 step 30400 avg_loss 2.0811\n",
      "epoch 0 step 30600 avg_loss 2.0810\n",
      "epoch 0 step 30800 avg_loss 2.0800\n",
      "epoch 0 step 31000 avg_loss 2.0791\n",
      "epoch 0 step 31200 avg_loss 2.0793\n",
      "epoch 0 step 31400 avg_loss 2.0788\n",
      "epoch 0 step 31600 avg_loss 2.0797\n",
      "epoch 0 step 31800 avg_loss 2.0794\n",
      "epoch 0 step 32000 avg_loss 2.0787\n",
      "epoch 0 step 32200 avg_loss 2.0787\n",
      "epoch 0 step 32400 avg_loss 2.0781\n",
      "epoch 0 step 32600 avg_loss 2.0781\n",
      "epoch 0 step 32800 avg_loss 2.0784\n",
      "epoch 0 step 33000 avg_loss 2.0787\n",
      "epoch 0 step 33200 avg_loss 2.0786\n",
      "epoch 0 step 33400 avg_loss 2.0786\n",
      "epoch 0 step 33600 avg_loss 2.0792\n",
      "epoch 0 step 33800 avg_loss 2.0792\n",
      "epoch 0 step 34000 avg_loss 2.0792\n",
      "epoch 0 step 34200 avg_loss 2.0795\n",
      "epoch 0 step 34400 avg_loss 2.0788\n",
      "epoch 0 step 34600 avg_loss 2.0786\n",
      "epoch 0 step 34800 avg_loss 2.0782\n",
      "epoch 0 step 35000 avg_loss 2.0786\n",
      "epoch 0 step 35200 avg_loss 2.0786\n",
      "epoch 0 step 35400 avg_loss 2.0796\n",
      "epoch 0 step 35600 avg_loss 2.0793\n",
      "epoch 0 step 35800 avg_loss 2.0792\n",
      "epoch 0 step 36000 avg_loss 2.0784\n",
      "epoch 0 step 36200 avg_loss 2.0781\n",
      "epoch 0 step 36400 avg_loss 2.0782\n",
      "epoch 0 step 36600 avg_loss 2.0789\n",
      "epoch 0 step 36800 avg_loss 2.0792\n",
      "epoch 0 step 37000 avg_loss 2.0794\n",
      "epoch 0 step 37200 avg_loss 2.0787\n",
      "epoch 0 step 37400 avg_loss 2.0785\n",
      "epoch 0 step 37600 avg_loss 2.0787\n",
      "epoch 0 step 37800 avg_loss 2.0791\n",
      "epoch 0 step 38000 avg_loss 2.0787\n",
      "epoch 0 step 38200 avg_loss 2.0792\n",
      "epoch 0 step 38400 avg_loss 2.0785\n",
      "epoch 0 step 38600 avg_loss 2.0788\n",
      "epoch 0 step 38800 avg_loss 2.0782\n",
      "epoch 0 step 39000 avg_loss 2.0785\n",
      "epoch 0 step 39200 avg_loss 2.0757\n",
      "epoch 0 step 39400 avg_loss 2.0748\n",
      "epoch 0 step 39600 avg_loss 2.0742\n",
      "epoch 0 step 39800 avg_loss 2.0740\n",
      "epoch 0 step 40000 avg_loss 2.0721\n",
      "epoch 0 step 40200 avg_loss 2.0703\n",
      "epoch 0 step 40400 avg_loss 2.0703\n",
      "epoch 0 step 40600 avg_loss 2.0704\n",
      "epoch 0 step 40800 avg_loss 2.0705\n",
      "epoch 0 step 41000 avg_loss 2.0705\n",
      "epoch 0 step 41200 avg_loss 2.0710\n",
      "epoch 0 step 41400 avg_loss 2.0715\n",
      "epoch 0 step 41600 avg_loss 2.0713\n",
      "epoch 0 step 41800 avg_loss 2.0718\n",
      "epoch 0 step 42000 avg_loss 2.0718\n",
      "epoch 0 step 42200 avg_loss 2.0720\n",
      "epoch 0 step 42400 avg_loss 2.0719\n",
      "epoch 0 step 42600 avg_loss 2.0720\n",
      "epoch 0 step 42800 avg_loss 2.0724\n",
      "epoch 0 step 43000 avg_loss 2.0726\n",
      "epoch 0 step 43200 avg_loss 2.0726\n",
      "epoch 0 step 43400 avg_loss 2.0721\n",
      "epoch 0 step 43600 avg_loss 2.0720\n",
      "epoch 0 step 43800 avg_loss 2.0717\n",
      "epoch 0 step 44000 avg_loss 2.0715\n",
      "epoch 0 step 44200 avg_loss 2.0717\n",
      "epoch 0 step 44400 avg_loss 2.0718\n",
      "epoch 0 step 44600 avg_loss 2.0714\n",
      "epoch 0 step 44800 avg_loss 2.0712\n",
      "epoch 0 step 45000 avg_loss 2.0710\n",
      "epoch 0 step 45200 avg_loss 2.0712\n",
      "epoch 0 step 45400 avg_loss 2.0708\n",
      "epoch 0 step 45600 avg_loss 2.0711\n",
      "epoch 0 step 45800 avg_loss 2.0711\n",
      "epoch 0 step 46000 avg_loss 2.0711\n",
      "epoch 0 step 46200 avg_loss 2.0712\n",
      "epoch 0 step 46400 avg_loss 2.0714\n",
      "epoch 0 step 46600 avg_loss 2.0715\n",
      "epoch 0 step 46800 avg_loss 2.0708\n",
      "epoch 0 step 47000 avg_loss 2.0709\n",
      "epoch 0 step 47200 avg_loss 2.0705\n",
      "epoch 0 step 47400 avg_loss 2.0705\n",
      "epoch 0 step 47600 avg_loss 2.0699\n",
      "epoch 0 step 47800 avg_loss 2.0699\n",
      "epoch 0 step 48000 avg_loss 2.0702\n",
      "epoch 0 step 48200 avg_loss 2.0703\n",
      "epoch 0 step 48400 avg_loss 2.0698\n",
      "epoch 0 step 48600 avg_loss 2.0693\n",
      "epoch 0 step 48800 avg_loss 2.0693\n",
      "epoch 0 step 49000 avg_loss 2.0692\n",
      "epoch 0 step 49200 avg_loss 2.0693\n",
      "epoch 0 step 49400 avg_loss 2.0699\n",
      "epoch 0 step 49600 avg_loss 2.0696\n",
      "epoch 0 step 49800 avg_loss 2.0701\n",
      "epoch 0 step 50000 avg_loss 2.0699\n",
      "epoch 0 step 50200 avg_loss 2.0697\n",
      "epoch 0 step 50400 avg_loss 2.0698\n",
      "epoch 0 step 50600 avg_loss 2.0694\n",
      "epoch 0 step 50800 avg_loss 2.0699\n",
      "epoch 0 step 51000 avg_loss 2.0698\n",
      "epoch 0 step 51200 avg_loss 2.0700\n",
      "epoch 0 step 51400 avg_loss 2.0697\n",
      "epoch 0 step 51600 avg_loss 2.0701\n",
      "epoch 0 step 51800 avg_loss 2.0699\n",
      "epoch 0 step 52000 avg_loss 2.0700\n",
      "epoch 0 step 52200 avg_loss 2.0702\n",
      "epoch 0 step 52400 avg_loss 2.0704\n",
      "epoch 0 step 52600 avg_loss 2.0706\n",
      "epoch 0 step 52800 avg_loss 2.0705\n",
      "epoch 0 step 53000 avg_loss 2.0706\n",
      "epoch 0 step 53200 avg_loss 2.0704\n",
      "epoch 0 step 53400 avg_loss 2.0709\n",
      "epoch 0 step 53600 avg_loss 2.0717\n",
      "epoch 0 step 53800 avg_loss 2.0713\n",
      "epoch 0 step 54000 avg_loss 2.0711\n",
      "epoch 0 step 54200 avg_loss 2.0715\n",
      "epoch 0 step 54400 avg_loss 2.0711\n",
      "epoch 0 step 54600 avg_loss 2.0712\n",
      "epoch 0 step 54800 avg_loss 2.0716\n",
      "epoch 0 step 55000 avg_loss 2.0717\n",
      "epoch 0 step 55200 avg_loss 2.0714\n",
      "epoch 0 step 55400 avg_loss 2.0714\n",
      "epoch 0 step 55600 avg_loss 2.0716\n",
      "epoch 0 step 55800 avg_loss 2.0718\n",
      "epoch 0 step 56000 avg_loss 2.0718\n",
      "epoch 0 step 56200 avg_loss 2.0720\n",
      "epoch 0 step 56400 avg_loss 2.0717\n",
      "epoch 0 step 56600 avg_loss 2.0722\n",
      "epoch 0 step 56800 avg_loss 2.0721\n",
      "epoch 0 step 57000 avg_loss 2.0716\n",
      "epoch 0 step 57200 avg_loss 2.0719\n",
      "epoch 0 step 57400 avg_loss 2.0726\n",
      "epoch 0 step 57600 avg_loss 2.0729\n",
      "epoch 0 step 57800 avg_loss 2.0727\n",
      "epoch 0 step 58000 avg_loss 2.0728\n",
      "epoch 0 step 58200 avg_loss 2.0725\n",
      "epoch 0 step 58400 avg_loss 2.0726\n",
      "epoch 0 step 58600 avg_loss 2.0724\n",
      "epoch 0 step 58800 avg_loss 2.0728\n",
      "epoch 0 step 59000 avg_loss 2.0732\n",
      "epoch 0 step 59200 avg_loss 2.0734\n",
      "epoch 0 step 59400 avg_loss 2.0735\n",
      "epoch 0 step 59600 avg_loss 2.0733\n",
      "epoch 0 step 59800 avg_loss 2.0738\n",
      "epoch 0 step 60000 avg_loss 2.0734\n",
      "epoch 0 step 60200 avg_loss 2.0733\n",
      "epoch 0 step 60400 avg_loss 2.0732\n",
      "epoch 0 step 60600 avg_loss 2.0729\n",
      "epoch 0 step 60800 avg_loss 2.0728\n",
      "epoch 0 step 61000 avg_loss 2.0725\n",
      "epoch 0 step 61200 avg_loss 2.0728\n",
      "epoch 0 step 61400 avg_loss 2.0726\n",
      "epoch 0 step 61600 avg_loss 2.0732\n",
      "epoch 0 step 61800 avg_loss 2.0726\n",
      "epoch 0 step 62000 avg_loss 2.0740\n",
      "epoch 0 step 62200 avg_loss 2.0736\n",
      "epoch 0 step 62400 avg_loss 2.0734\n",
      "epoch 0 step 62600 avg_loss 2.0734\n",
      "epoch 0 step 62800 avg_loss 2.0732\n",
      "epoch 0 step 63000 avg_loss 2.0734\n",
      "epoch 0 step 63200 avg_loss 2.0736\n",
      "epoch 0 step 63400 avg_loss 2.0744\n",
      "epoch 0 step 63600 avg_loss 2.0741\n",
      "epoch 0 step 63800 avg_loss 2.0745\n",
      "epoch 0 step 64000 avg_loss 2.0744\n",
      "epoch 0 step 64200 avg_loss 2.0739\n",
      "epoch 0 step 64400 avg_loss 2.0739\n",
      "epoch 0 step 64600 avg_loss 2.0736\n",
      "epoch 0 step 64800 avg_loss 2.0732\n",
      "epoch 0 step 65000 avg_loss 2.0730\n",
      "epoch 0 step 65200 avg_loss 2.0730\n",
      "epoch 0 step 65400 avg_loss 2.0732\n",
      "epoch 0 step 65600 avg_loss 2.0735\n",
      "epoch 0 step 65800 avg_loss 2.0737\n",
      "epoch 0 step 66000 avg_loss 2.0736\n",
      "epoch 0 step 66200 avg_loss 2.0733\n",
      "epoch 0 step 66400 avg_loss 2.0735\n",
      "epoch 0 step 66600 avg_loss 2.0736\n",
      "epoch 0 step 66800 avg_loss 2.0747\n",
      "epoch 0 step 67000 avg_loss 2.0749\n",
      "epoch 0 step 67200 avg_loss 2.0747\n",
      "epoch 0 step 67400 avg_loss 2.0746\n",
      "epoch 0 step 67600 avg_loss 2.0745\n",
      "epoch 0 step 67800 avg_loss 2.0747\n",
      "epoch 0 step 68000 avg_loss 2.0744\n",
      "epoch 0 step 68200 avg_loss 2.0743\n",
      "epoch 0 step 68400 avg_loss 2.0743\n",
      "epoch 0 step 68600 avg_loss 2.0742\n",
      "epoch 0 step 68800 avg_loss 2.0740\n",
      "epoch 0 step 69000 avg_loss 2.0738\n",
      "epoch 0 step 69200 avg_loss 2.0743\n",
      "epoch 0 step 69400 avg_loss 2.0746\n",
      "epoch 0 step 69600 avg_loss 2.0743\n",
      "epoch 0 step 69800 avg_loss 2.0739\n",
      "epoch 0 step 70000 avg_loss 2.0744\n",
      "epoch 0 step 70200 avg_loss 2.0739\n",
      "epoch 0 step 70400 avg_loss 2.0736\n",
      "epoch 0 step 70600 avg_loss 2.0737\n",
      "epoch 0 step 70800 avg_loss 2.0736\n",
      "epoch 0 step 71000 avg_loss 2.0738\n",
      "epoch 0 step 71200 avg_loss 2.0740\n",
      "epoch 0 step 71400 avg_loss 2.0739\n",
      "epoch 0 step 71600 avg_loss 2.0743\n",
      "epoch 0 step 71800 avg_loss 2.0740\n",
      "epoch 0 step 72000 avg_loss 2.0743\n",
      "epoch 0 step 72200 avg_loss 2.0743\n",
      "epoch 0 step 72400 avg_loss 2.0741\n",
      "epoch 0 step 72600 avg_loss 2.0740\n",
      "epoch 0 step 72800 avg_loss 2.0739\n",
      "epoch 0 step 73000 avg_loss 2.0738\n",
      "epoch 0 step 73200 avg_loss 2.0736\n",
      "epoch 0 step 73400 avg_loss 2.0733\n",
      "epoch 0 step 73600 avg_loss 2.0737\n",
      "epoch 0 step 73800 avg_loss 2.0741\n",
      "epoch 0 step 74000 avg_loss 2.0741\n",
      "epoch 0 step 74200 avg_loss 2.0740\n",
      "epoch 0 step 74400 avg_loss 2.0740\n",
      "epoch 0 step 74600 avg_loss 2.0739\n",
      "epoch 0 step 74800 avg_loss 2.0734\n",
      "epoch 0 step 75000 avg_loss 2.0736\n",
      "epoch 0 step 75200 avg_loss 2.0735\n",
      "epoch 0 step 75400 avg_loss 2.0734\n",
      "epoch 0 step 75600 avg_loss 2.0738\n",
      "epoch 0 step 75800 avg_loss 2.0741\n",
      "epoch 0 step 76000 avg_loss 2.0740\n",
      "epoch 0 step 76200 avg_loss 2.0741\n",
      "epoch 0 step 76400 avg_loss 2.0740\n",
      "epoch 0 step 76600 avg_loss 2.0736\n",
      "epoch 0 step 76800 avg_loss 2.0734\n",
      "epoch 0 step 77000 avg_loss 2.0733\n",
      "epoch 0 step 77200 avg_loss 2.0735\n",
      "epoch 0 step 77400 avg_loss 2.0735\n",
      "epoch 0 step 77600 avg_loss 2.0734\n",
      "epoch 0 step 77800 avg_loss 2.0734\n",
      "epoch 0 step 78000 avg_loss 2.0733\n",
      "epoch 0 step 78200 avg_loss 2.0736\n",
      "epoch 0 step 78400 avg_loss 2.0739\n",
      "epoch 0 step 78600 avg_loss 2.0744\n",
      "epoch 0 step 78800 avg_loss 2.0741\n",
      "epoch 0 step 79000 avg_loss 2.0739\n",
      "epoch 0 step 79200 avg_loss 2.0738\n",
      "epoch 0 step 79400 avg_loss 2.0736\n",
      "epoch 0 step 79600 avg_loss 2.0733\n",
      "epoch 0 step 79800 avg_loss 2.0737\n",
      "epoch 0 step 80000 avg_loss 2.0738\n",
      "epoch 0 step 80200 avg_loss 2.0739\n",
      "epoch 0 step 80400 avg_loss 2.0739\n",
      "epoch 0 step 80600 avg_loss 2.0739\n",
      "epoch 0 step 80800 avg_loss 2.0736\n",
      "epoch 0 step 81000 avg_loss 2.0734\n",
      "epoch 0 step 81200 avg_loss 2.0737\n",
      "epoch 0 step 81400 avg_loss 2.0736\n",
      "epoch 0 step 81600 avg_loss 2.0738\n",
      "epoch 0 step 81800 avg_loss 2.0738\n",
      "epoch 0 step 82000 avg_loss 2.0737\n",
      "epoch 0 step 82200 avg_loss 2.0738\n",
      "epoch 0 step 82400 avg_loss 2.0737\n",
      "epoch 0 step 82600 avg_loss 2.0735\n",
      "epoch 0 step 82800 avg_loss 2.0733\n",
      "epoch 0 step 83000 avg_loss 2.0736\n",
      "epoch 0 step 83200 avg_loss 2.0736\n",
      "epoch 0 step 83400 avg_loss 2.0737\n",
      "epoch 0 step 83600 avg_loss 2.0738\n",
      "epoch 0 step 83800 avg_loss 2.0743\n",
      "epoch 0 step 84000 avg_loss 2.0743\n",
      "epoch 0 step 84200 avg_loss 2.0743\n",
      "epoch 0 step 84400 avg_loss 2.0741\n",
      "epoch 0 step 84600 avg_loss 2.0743\n",
      "epoch 0 step 84800 avg_loss 2.0744\n",
      "epoch 0 step 85000 avg_loss 2.0743\n",
      "epoch 0 step 85200 avg_loss 2.0743\n",
      "epoch 0 step 85400 avg_loss 2.0744\n",
      "epoch 0 step 85600 avg_loss 2.0745\n",
      "epoch 0 step 85800 avg_loss 2.0747\n",
      "epoch 0 step 86000 avg_loss 2.0750\n",
      "epoch 0 step 86200 avg_loss 2.0749\n",
      "epoch 0 step 86400 avg_loss 2.0753\n",
      "epoch 0 step 86600 avg_loss 2.0752\n",
      "epoch 0 step 86800 avg_loss 2.0753\n",
      "epoch 0 step 87000 avg_loss 2.0758\n",
      "epoch 0 step 87200 avg_loss 2.0755\n",
      "epoch 0 step 87400 avg_loss 2.0757\n",
      "epoch 0 step 87600 avg_loss 2.0760\n",
      "epoch 0 step 87800 avg_loss 2.0760\n",
      "epoch 0 step 88000 avg_loss 2.0764\n",
      "epoch 0 step 88200 avg_loss 2.0766\n",
      "epoch 0 step 88400 avg_loss 2.0770\n",
      "epoch 0 step 88600 avg_loss 2.0771\n",
      "epoch 0 step 88800 avg_loss 2.0774\n",
      "epoch 0 step 89000 avg_loss 2.0773\n",
      "epoch 0 step 89200 avg_loss 2.0781\n",
      "epoch 0 step 89400 avg_loss 2.0783\n",
      "epoch 0 step 89600 avg_loss 2.0784\n",
      "epoch 0 step 89800 avg_loss 2.0786\n",
      "epoch 0 step 90000 avg_loss 2.0787\n",
      "epoch 0 step 90200 avg_loss 2.0788\n",
      "epoch 0 step 90400 avg_loss 2.0787\n",
      "epoch 0 step 90600 avg_loss 2.0787\n",
      "epoch 0 step 90800 avg_loss 2.0790\n",
      "epoch 0 step 91000 avg_loss 2.0790\n",
      "epoch 0 step 91200 avg_loss 2.0788\n",
      "epoch 0 step 91400 avg_loss 2.0786\n",
      "epoch 0 step 91600 avg_loss 2.0783\n",
      "epoch 0 step 91800 avg_loss 2.0779\n",
      "epoch 0 step 92000 avg_loss 2.0782\n",
      "epoch 0 step 92200 avg_loss 2.0784\n",
      "epoch 0 step 92400 avg_loss 2.0786\n",
      "epoch 0 step 92600 avg_loss 2.0786\n",
      "epoch 0 step 92800 avg_loss 2.0789\n",
      "epoch 0 step 93000 avg_loss 2.0791\n",
      "epoch 0 step 93200 avg_loss 2.0790\n",
      "epoch 0 step 93400 avg_loss 2.0789\n",
      "epoch 0 step 93600 avg_loss 2.0789\n",
      "epoch 0 step 93800 avg_loss 2.0789\n",
      "epoch 0 step 94000 avg_loss 2.0789\n",
      "epoch 0 step 94200 avg_loss 2.0788\n",
      "epoch 0 step 94400 avg_loss 2.0786\n",
      "epoch 0 step 94600 avg_loss 2.0785\n",
      "epoch 0 step 94800 avg_loss 2.0789\n",
      "epoch 0 step 95000 avg_loss 2.0792\n",
      "epoch 0 step 95200 avg_loss 2.0789\n",
      "epoch 0 step 95400 avg_loss 2.0789\n",
      "epoch 0 step 95600 avg_loss 2.0787\n",
      "epoch 0 step 95800 avg_loss 2.0785\n",
      "epoch 0 step 96000 avg_loss 2.0788\n",
      "epoch 0 step 96200 avg_loss 2.0789\n",
      "epoch 0 step 96400 avg_loss 2.0791\n",
      "epoch 0 step 96600 avg_loss 2.0796\n",
      "epoch 0 step 96800 avg_loss 2.0796\n",
      "epoch 0 step 97000 avg_loss 2.0797\n",
      "epoch 0 step 97200 avg_loss 2.0799\n",
      "epoch 0 step 97400 avg_loss 2.0808\n",
      "epoch 0 step 97600 avg_loss 2.0810\n",
      "epoch 0 step 97800 avg_loss 2.0808\n",
      "epoch 0 step 98000 avg_loss 2.0807\n",
      "epoch 0 step 98200 avg_loss 2.0810\n",
      "epoch 0 step 98400 avg_loss 2.0807\n",
      "epoch 0 step 98600 avg_loss 2.0812\n",
      "epoch 0 step 98800 avg_loss 2.0809\n",
      "epoch 0 step 99000 avg_loss 2.0809\n",
      "epoch 0 step 99200 avg_loss 2.0807\n",
      "epoch 0 step 99400 avg_loss 2.0808\n",
      "epoch 0 step 99600 avg_loss 2.0807\n",
      "epoch 0 step 99800 avg_loss 2.0808\n",
      "epoch 0 step 100000 avg_loss 2.0807\n",
      "epoch 0 step 100200 avg_loss 2.0807\n",
      "epoch 0 step 100400 avg_loss 2.0803\n",
      "epoch 0 step 100600 avg_loss 2.0802\n",
      "epoch 0 step 100800 avg_loss 2.0802\n",
      "epoch 0 step 101000 avg_loss 2.0804\n",
      "epoch 0 step 101200 avg_loss 2.0805\n",
      "epoch 0 step 101400 avg_loss 2.0805\n",
      "epoch 0 step 101600 avg_loss 2.0802\n",
      "epoch 0 step 101800 avg_loss 2.0803\n",
      "epoch 0 step 102000 avg_loss 2.0803\n",
      "epoch 0 step 102200 avg_loss 2.0801\n",
      "epoch 0 step 102400 avg_loss 2.0803\n",
      "epoch 0 step 102600 avg_loss 2.0801\n",
      "epoch 0 step 102800 avg_loss 2.0800\n",
      "epoch 0 step 103000 avg_loss 2.0799\n",
      "epoch 0 step 103200 avg_loss 2.0799\n",
      "epoch 0 step 103400 avg_loss 2.0802\n",
      "epoch 0 step 103600 avg_loss 2.0801\n",
      "epoch 0 step 103800 avg_loss 2.0800\n",
      "epoch 0 step 104000 avg_loss 2.0800\n",
      "epoch 0 step 104200 avg_loss 2.0798\n",
      "epoch 0 step 104400 avg_loss 2.0797\n",
      "epoch 0 step 104600 avg_loss 2.0797\n",
      "epoch 0 step 104800 avg_loss 2.0795\n",
      "epoch 0 step 105000 avg_loss 2.0793\n",
      "epoch 0 step 105200 avg_loss 2.0795\n",
      "epoch 0 step 105400 avg_loss 2.0798\n",
      "epoch 0 step 105600 avg_loss 2.0799\n",
      "epoch 0 step 105800 avg_loss 2.0799\n",
      "epoch 0 step 106000 avg_loss 2.0796\n",
      "epoch 0 step 106200 avg_loss 2.0799\n",
      "epoch 0 step 106400 avg_loss 2.0798\n",
      "epoch 0 step 106600 avg_loss 2.0799\n",
      "epoch 0 step 106800 avg_loss 2.0797\n",
      "epoch 0 step 107000 avg_loss 2.0799\n",
      "epoch 0 step 107200 avg_loss 2.0797\n",
      "epoch 0 step 107400 avg_loss 2.0796\n",
      "epoch 0 step 107600 avg_loss 2.0798\n",
      "epoch 0 step 107800 avg_loss 2.0797\n",
      "epoch 0 step 108000 avg_loss 2.0794\n",
      "epoch 0 step 108200 avg_loss 2.0794\n",
      "epoch 0 step 108400 avg_loss 2.0793\n",
      "epoch 0 step 108600 avg_loss 2.0793\n",
      "epoch 0 step 108800 avg_loss 2.0794\n",
      "epoch 0 step 109000 avg_loss 2.0794\n",
      "epoch 0 step 109200 avg_loss 2.0793\n",
      "epoch 0 step 109400 avg_loss 2.0792\n",
      "epoch 0 step 109600 avg_loss 2.0794\n",
      "epoch 0 step 109800 avg_loss 2.0796\n",
      "epoch 0 step 110000 avg_loss 2.0797\n",
      "epoch 0 step 110200 avg_loss 2.0793\n",
      "epoch 0 step 110400 avg_loss 2.0797\n",
      "epoch 0 step 110600 avg_loss 2.0794\n",
      "epoch 0 step 110800 avg_loss 2.0797\n",
      "epoch 0 step 111000 avg_loss 2.0796\n",
      "epoch 0 step 111200 avg_loss 2.0798\n",
      "epoch 0 step 111400 avg_loss 2.0797\n",
      "epoch 0 step 111600 avg_loss 2.0796\n",
      "epoch 0 step 111800 avg_loss 2.0795\n",
      "epoch 0 step 112000 avg_loss 2.0795\n",
      "epoch 0 step 112200 avg_loss 2.0794\n",
      "epoch 0 step 112400 avg_loss 2.0794\n",
      "epoch 0 step 112600 avg_loss 2.0795\n",
      "epoch 0 step 112800 avg_loss 2.0793\n",
      "epoch 0 step 113000 avg_loss 2.0791\n",
      "epoch 0 step 113200 avg_loss 2.0790\n",
      "epoch 0 step 113400 avg_loss 2.0789\n",
      "epoch 0 step 113600 avg_loss 2.0790\n",
      "epoch 0 step 113800 avg_loss 2.0791\n",
      "epoch 0 step 114000 avg_loss 2.0793\n",
      "epoch 0 step 114200 avg_loss 2.0792\n",
      "epoch 0 step 114400 avg_loss 2.0790\n",
      "epoch 0 step 114600 avg_loss 2.0793\n",
      "epoch 0 step 114800 avg_loss 2.0797\n",
      "epoch 0 step 115000 avg_loss 2.0798\n",
      "epoch 0 step 115200 avg_loss 2.0802\n",
      "epoch 0 step 115400 avg_loss 2.0800\n",
      "epoch 0 step 115600 avg_loss 2.0800\n",
      "epoch 0 step 115800 avg_loss 2.0799\n",
      "epoch 0 step 116000 avg_loss 2.0795\n",
      "epoch 0 step 116200 avg_loss 2.0793\n",
      "epoch 0 step 116400 avg_loss 2.0794\n",
      "epoch 0 step 116600 avg_loss 2.0796\n",
      "epoch 0 step 116800 avg_loss 2.0797\n",
      "epoch 0 step 117000 avg_loss 2.0797\n",
      "epoch 0 step 117200 avg_loss 2.0798\n",
      "epoch 0 step 117400 avg_loss 2.0798\n",
      "epoch 0 step 117600 avg_loss 2.0798\n",
      "epoch 0 step 117800 avg_loss 2.0798\n",
      "epoch 0 step 118000 avg_loss 2.0796\n",
      "epoch 0 step 118200 avg_loss 2.0794\n",
      "epoch 0 step 118400 avg_loss 2.0793\n",
      "epoch 0 step 118600 avg_loss 2.0797\n",
      "epoch 0 step 118800 avg_loss 2.0798\n",
      "epoch 0 step 119000 avg_loss 2.0798\n",
      "epoch 0 step 119200 avg_loss 2.0797\n",
      "epoch 0 step 119400 avg_loss 2.0798\n",
      "epoch 0 step 119600 avg_loss 2.0797\n",
      "epoch 0 step 119800 avg_loss 2.0796\n",
      "epoch 0 step 120000 avg_loss 2.0796\n",
      "epoch 0 step 120200 avg_loss 2.0796\n",
      "epoch 0 step 120400 avg_loss 2.0795\n",
      "epoch 0 step 120600 avg_loss 2.0796\n",
      "epoch 0 step 120800 avg_loss 2.0797\n",
      "epoch 0 step 121000 avg_loss 2.0797\n",
      "epoch 0 step 121200 avg_loss 2.0801\n",
      "epoch 0 step 121400 avg_loss 2.0799\n",
      "epoch 0 step 121600 avg_loss 2.0797\n",
      "epoch 0 step 121800 avg_loss 2.0797\n",
      "epoch 0 step 122000 avg_loss 2.0795\n",
      "epoch 0 step 122200 avg_loss 2.0797\n",
      "epoch 0 step 122400 avg_loss 2.0800\n",
      "epoch 0 step 122600 avg_loss 2.0799\n",
      "epoch 0 step 122800 avg_loss 2.0799\n",
      "epoch 0 step 123000 avg_loss 2.0800\n",
      "epoch 0 step 123200 avg_loss 2.0800\n",
      "epoch 0 step 123400 avg_loss 2.0798\n",
      "epoch 0 step 123600 avg_loss 2.0800\n",
      "epoch 0 step 123800 avg_loss 2.0801\n",
      "epoch 0 step 124000 avg_loss 2.0799\n",
      "epoch 0 step 124200 avg_loss 2.0798\n",
      "epoch 0 step 124400 avg_loss 2.0798\n",
      "epoch 0 step 124600 avg_loss 2.0799\n",
      "epoch 0 step 124800 avg_loss 2.0801\n",
      "epoch 0 step 125000 avg_loss 2.0799\n",
      "epoch 0 step 125200 avg_loss 2.0799\n",
      "epoch 0 step 125400 avg_loss 2.0797\n",
      "epoch 0 step 125600 avg_loss 2.0799\n",
      "epoch 0 step 125800 avg_loss 2.0797\n",
      "epoch 0 step 126000 avg_loss 2.0798\n",
      "epoch 0 step 126200 avg_loss 2.0799\n",
      "epoch 0 step 126400 avg_loss 2.0799\n",
      "epoch 0 step 126600 avg_loss 2.0798\n",
      "epoch 0 step 126800 avg_loss 2.0797\n",
      "epoch 0 step 127000 avg_loss 2.0796\n",
      "epoch 0 step 127200 avg_loss 2.0796\n",
      "epoch 0 step 127400 avg_loss 2.0793\n",
      "epoch 0 step 127600 avg_loss 2.0796\n",
      "epoch 0 step 127800 avg_loss 2.0796\n",
      "epoch 0 step 128000 avg_loss 2.0795\n",
      "epoch 0 step 128200 avg_loss 2.0794\n",
      "epoch 0 step 128400 avg_loss 2.0793\n",
      "epoch 0 step 128600 avg_loss 2.0791\n",
      "epoch 0 step 128800 avg_loss 2.0790\n",
      "epoch 0 step 129000 avg_loss 2.0791\n",
      "epoch 0 step 129200 avg_loss 2.0791\n",
      "epoch 0 step 129400 avg_loss 2.0790\n",
      "epoch 0 step 129600 avg_loss 2.0790\n",
      "epoch 0 step 129800 avg_loss 2.0792\n",
      "epoch 0 step 130000 avg_loss 2.0791\n",
      "epoch 0 step 130200 avg_loss 2.0791\n",
      "epoch 0 step 130400 avg_loss 2.0791\n",
      "epoch 0 step 130600 avg_loss 2.0791\n",
      "epoch 0 step 130800 avg_loss 2.0792\n",
      "epoch 0 step 131000 avg_loss 2.0796\n",
      "epoch 0 step 131200 avg_loss 2.0797\n",
      "epoch 0 step 131400 avg_loss 2.0793\n",
      "epoch 0 step 131600 avg_loss 2.0791\n",
      "epoch 0 step 131800 avg_loss 2.0792\n",
      "epoch 0 step 132000 avg_loss 2.0791\n",
      "epoch 0 step 132200 avg_loss 2.0794\n",
      "epoch 0 step 132400 avg_loss 2.0793\n",
      "epoch 0 step 132600 avg_loss 2.0793\n",
      "epoch 0 step 132800 avg_loss 2.0791\n",
      "epoch 0 step 133000 avg_loss 2.0791\n",
      "epoch 0 step 133200 avg_loss 2.0789\n",
      "epoch 0 step 133400 avg_loss 2.0787\n",
      "epoch 0 step 133600 avg_loss 2.0787\n",
      "epoch 0 step 133800 avg_loss 2.0788\n",
      "epoch 0 step 134000 avg_loss 2.0789\n",
      "epoch 0 step 134200 avg_loss 2.0791\n",
      "epoch 0 step 134400 avg_loss 2.0789\n",
      "epoch 0 step 134600 avg_loss 2.0788\n",
      "epoch 0 step 134800 avg_loss 2.0785\n",
      "epoch 0 step 135000 avg_loss 2.0786\n",
      "epoch 0 step 135200 avg_loss 2.0787\n",
      "epoch 0 step 135400 avg_loss 2.0785\n",
      "epoch 0 step 135600 avg_loss 2.0787\n",
      "epoch 0 step 135800 avg_loss 2.0785\n",
      "epoch 0 step 136000 avg_loss 2.0785\n",
      "epoch 0 step 136200 avg_loss 2.0784\n",
      "epoch 0 step 136400 avg_loss 2.0784\n",
      "epoch 0 step 136600 avg_loss 2.0785\n",
      "epoch 0 step 136800 avg_loss 2.0783\n",
      "epoch 0 step 137000 avg_loss 2.0784\n",
      "epoch 0 step 137200 avg_loss 2.0784\n",
      "epoch 0 step 137400 avg_loss 2.0782\n",
      "epoch 0 step 137600 avg_loss 2.0782\n",
      "epoch 0 step 137800 avg_loss 2.0780\n",
      "epoch 0 step 138000 avg_loss 2.0781\n",
      "epoch 0 step 138200 avg_loss 2.0784\n",
      "epoch 0 step 138400 avg_loss 2.0785\n",
      "epoch 0 step 138600 avg_loss 2.0784\n",
      "epoch 0 step 138800 avg_loss 2.0784\n",
      "epoch 0 step 139000 avg_loss 2.0785\n",
      "epoch 0 step 139200 avg_loss 2.0785\n",
      "epoch 0 step 139400 avg_loss 2.0788\n",
      "epoch 0 step 139600 avg_loss 2.0787\n",
      "epoch 0 step 139800 avg_loss 2.0784\n",
      "epoch 0 step 140000 avg_loss 2.0784\n",
      "epoch 0 step 140200 avg_loss 2.0782\n",
      "epoch 0 step 140400 avg_loss 2.0782\n",
      "epoch 0 step 140600 avg_loss 2.0781\n",
      "epoch 0 step 140800 avg_loss 2.0780\n",
      "epoch 0 step 141000 avg_loss 2.0780\n",
      "epoch 0 step 141200 avg_loss 2.0782\n",
      "epoch 0 step 141400 avg_loss 2.0781\n",
      "epoch 0 step 141600 avg_loss 2.0778\n",
      "epoch 0 step 141800 avg_loss 2.0777\n",
      "epoch 0 step 142000 avg_loss 2.0776\n",
      "epoch 0 step 142200 avg_loss 2.0776\n",
      "epoch 0 step 142400 avg_loss 2.0774\n",
      "epoch 0 step 142600 avg_loss 2.0774\n",
      "epoch 0 step 142800 avg_loss 2.0773\n",
      "epoch 0 step 143000 avg_loss 2.0775\n",
      "epoch 0 step 143200 avg_loss 2.0775\n",
      "epoch 0 step 143400 avg_loss 2.0774\n",
      "epoch 0 step 143600 avg_loss 2.0775\n",
      "epoch 0 step 143800 avg_loss 2.0774\n",
      "epoch 0 step 144000 avg_loss 2.0773\n",
      "epoch 0 step 144200 avg_loss 2.0774\n",
      "epoch 0 step 144400 avg_loss 2.0773\n",
      "epoch 0 step 144600 avg_loss 2.0772\n",
      "epoch 0 step 144800 avg_loss 2.0772\n",
      "epoch 0 step 145000 avg_loss 2.0769\n",
      "epoch 0 step 145200 avg_loss 2.0769\n",
      "epoch 0 step 145400 avg_loss 2.0769\n",
      "epoch 0 step 145600 avg_loss 2.0765\n",
      "epoch 0 step 145800 avg_loss 2.0765\n",
      "epoch 0 step 146000 avg_loss 2.0764\n",
      "epoch 0 step 146200 avg_loss 2.0765\n",
      "epoch 0 step 146400 avg_loss 2.0766\n",
      "epoch 0 step 146600 avg_loss 2.0765\n",
      "epoch 0 step 146800 avg_loss 2.0765\n",
      "epoch 0 step 147000 avg_loss 2.0764\n",
      "epoch 0 step 147200 avg_loss 2.0764\n",
      "epoch 0 step 147400 avg_loss 2.0765\n",
      "epoch 0 step 147600 avg_loss 2.0764\n",
      "epoch 0 step 147800 avg_loss 2.0764\n",
      "epoch 0 step 148000 avg_loss 2.0764\n",
      "epoch 0 step 148200 avg_loss 2.0763\n",
      "epoch 0 step 148400 avg_loss 2.0762\n",
      "epoch 0 step 148600 avg_loss 2.0762\n",
      "epoch 0 step 148800 avg_loss 2.0762\n",
      "epoch 0 step 149000 avg_loss 2.0762\n",
      "epoch 0 step 149200 avg_loss 2.0763\n",
      "epoch 0 step 149400 avg_loss 2.0761\n",
      "epoch 0 step 149600 avg_loss 2.0763\n",
      "epoch 0 step 149800 avg_loss 2.0762\n",
      "epoch 0 step 150000 avg_loss 2.0762\n",
      "epoch 0 step 150200 avg_loss 2.0761\n",
      "epoch 0 step 150400 avg_loss 2.0759\n",
      "epoch 0 step 150600 avg_loss 2.0757\n",
      "epoch 0 step 150800 avg_loss 2.0757\n",
      "epoch 0 step 151000 avg_loss 2.0756\n",
      "epoch 0 step 151200 avg_loss 2.0756\n",
      "epoch 0 step 151400 avg_loss 2.0758\n",
      "epoch 0 step 151600 avg_loss 2.0759\n",
      "epoch 0 step 151800 avg_loss 2.0759\n",
      "epoch 0 step 152000 avg_loss 2.0758\n",
      "epoch 0 step 152200 avg_loss 2.0759\n",
      "epoch 0 step 152400 avg_loss 2.0760\n",
      "epoch 0 step 152600 avg_loss 2.0761\n",
      "epoch 0 step 152800 avg_loss 2.0761\n",
      "epoch 0 step 153000 avg_loss 2.0759\n",
      "epoch 0 step 153200 avg_loss 2.0758\n",
      "epoch 0 step 153400 avg_loss 2.0760\n",
      "epoch 0 step 153600 avg_loss 2.0758\n",
      "epoch 0 step 153800 avg_loss 2.0759\n",
      "epoch 0 step 154000 avg_loss 2.0758\n",
      "epoch 0 step 154200 avg_loss 2.0757\n",
      "epoch 0 step 154400 avg_loss 2.0757\n",
      "epoch 0 step 154600 avg_loss 2.0757\n",
      "epoch 0 step 154800 avg_loss 2.0757\n",
      "epoch 0 step 155000 avg_loss 2.0757\n",
      "epoch 0 step 155200 avg_loss 2.0757\n",
      "epoch 0 step 155400 avg_loss 2.0756\n",
      "epoch 0 step 155600 avg_loss 2.0757\n",
      "epoch 0 step 155800 avg_loss 2.0758\n",
      "epoch 0 step 156000 avg_loss 2.0758\n",
      "epoch 0 step 156200 avg_loss 2.0758\n",
      "epoch 0 step 156400 avg_loss 2.0760\n",
      "epoch 0 step 156600 avg_loss 2.0760\n",
      "epoch 0 step 156800 avg_loss 2.0760\n",
      "epoch 0 step 157000 avg_loss 2.0760\n",
      "epoch 0 step 157200 avg_loss 2.0760\n",
      "epoch 0 step 157400 avg_loss 2.0762\n",
      "epoch 0 step 157600 avg_loss 2.0761\n",
      "epoch 0 step 157800 avg_loss 2.0762\n",
      "epoch 0 step 158000 avg_loss 2.0763\n",
      "epoch 0 step 158200 avg_loss 2.0763\n",
      "epoch 0 step 158400 avg_loss 2.0763\n",
      "epoch 0 step 158600 avg_loss 2.0762\n",
      "epoch 0 step 158800 avg_loss 2.0762\n",
      "epoch 0 step 159000 avg_loss 2.0761\n",
      "epoch 0 step 159200 avg_loss 2.0761\n",
      "epoch 0 step 159400 avg_loss 2.0762\n",
      "epoch 0 step 159600 avg_loss 2.0761\n",
      "epoch 0 step 159800 avg_loss 2.0760\n",
      "epoch 0 step 160000 avg_loss 2.0759\n",
      "epoch 0 step 160200 avg_loss 2.0758\n",
      "epoch 0 step 160400 avg_loss 2.0758\n",
      "epoch 0 step 160600 avg_loss 2.0759\n",
      "epoch 0 step 160800 avg_loss 2.0758\n",
      "epoch 0 step 161000 avg_loss 2.0759\n",
      "epoch 0 step 161200 avg_loss 2.0759\n",
      "epoch 0 step 161400 avg_loss 2.0759\n",
      "epoch 0 step 161600 avg_loss 2.0762\n",
      "epoch 0 step 161800 avg_loss 2.0762\n",
      "epoch 0 step 162000 avg_loss 2.0762\n",
      "epoch 0 step 162200 avg_loss 2.0762\n",
      "epoch 0 step 162400 avg_loss 2.0764\n",
      "epoch 0 step 162600 avg_loss 2.0762\n",
      "epoch 0 step 162800 avg_loss 2.0763\n",
      "epoch 0 step 163000 avg_loss 2.0761\n",
      "epoch 0 step 163200 avg_loss 2.0759\n",
      "epoch 0 step 163400 avg_loss 2.0759\n",
      "epoch 0 step 163600 avg_loss 2.0759\n",
      "epoch 0 step 163800 avg_loss 2.0758\n",
      "epoch 0 step 164000 avg_loss 2.0758\n",
      "epoch 0 step 164200 avg_loss 2.0757\n",
      "epoch 0 step 164400 avg_loss 2.0759\n",
      "epoch 0 step 164600 avg_loss 2.0760\n",
      "epoch 0 step 164800 avg_loss 2.0758\n",
      "epoch 0 step 165000 avg_loss 2.0759\n",
      "epoch 0 step 165200 avg_loss 2.0759\n",
      "epoch 0 step 165400 avg_loss 2.0758\n",
      "epoch 0 step 165600 avg_loss 2.0757\n",
      "epoch 0 step 165800 avg_loss 2.0756\n",
      "epoch 0 step 166000 avg_loss 2.0757\n",
      "epoch 0 step 166200 avg_loss 2.0756\n",
      "epoch 0 step 166400 avg_loss 2.0754\n",
      "epoch 0 step 166600 avg_loss 2.0756\n",
      "epoch 0 step 166800 avg_loss 2.0758\n",
      "epoch 0 step 167000 avg_loss 2.0760\n",
      "epoch 0 step 167200 avg_loss 2.0759\n",
      "epoch 0 step 167400 avg_loss 2.0759\n",
      "epoch 0 step 167600 avg_loss 2.0759\n",
      "epoch 0 step 167800 avg_loss 2.0758\n",
      "epoch 0 step 168000 avg_loss 2.0758\n",
      "epoch 0 step 168200 avg_loss 2.0758\n",
      "epoch 0 step 168400 avg_loss 2.0757\n",
      "epoch 0 step 168600 avg_loss 2.0757\n",
      "epoch 0 step 168800 avg_loss 2.0758\n",
      "epoch 0 step 169000 avg_loss 2.0758\n",
      "epoch 0 step 169200 avg_loss 2.0758\n",
      "epoch 0 step 169400 avg_loss 2.0758\n",
      "epoch 0 step 169600 avg_loss 2.0756\n",
      "epoch 0 step 169800 avg_loss 2.0754\n",
      "epoch 0 step 170000 avg_loss 2.0753\n",
      "epoch 0 step 170200 avg_loss 2.0751\n",
      "epoch 0 step 170400 avg_loss 2.0751\n",
      "epoch 0 step 170600 avg_loss 2.0753\n",
      "epoch 0 step 170800 avg_loss 2.0753\n",
      "epoch 0 step 171000 avg_loss 2.0753\n",
      "epoch 0 step 171200 avg_loss 2.0750\n",
      "epoch 0 step 171400 avg_loss 2.0751\n",
      "epoch 0 step 171600 avg_loss 2.0751\n",
      "epoch 0 step 171800 avg_loss 2.0751\n",
      "epoch 0 step 172000 avg_loss 2.0753\n",
      "epoch 0 step 172200 avg_loss 2.0752\n",
      "epoch 0 step 172400 avg_loss 2.0753\n",
      "epoch 0 step 172600 avg_loss 2.0751\n",
      "epoch 0 step 172800 avg_loss 2.0751\n",
      "epoch 0 step 173000 avg_loss 2.0750\n",
      "epoch 0 step 173200 avg_loss 2.0749\n",
      "epoch 0 step 173400 avg_loss 2.0748\n",
      "epoch 0 step 173600 avg_loss 2.0747\n",
      "epoch 0 step 173800 avg_loss 2.0747\n",
      "epoch 0 step 174000 avg_loss 2.0747\n",
      "epoch 0 step 174200 avg_loss 2.0749\n",
      "epoch 0 step 174400 avg_loss 2.0749\n",
      "epoch 0 step 174600 avg_loss 2.0749\n",
      "epoch 0 step 174800 avg_loss 2.0750\n",
      "epoch 0 step 175000 avg_loss 2.0749\n",
      "epoch 0 step 175200 avg_loss 2.0750\n",
      "epoch 0 step 175400 avg_loss 2.0750\n",
      "epoch 0 step 175600 avg_loss 2.0749\n",
      "epoch 0 step 175800 avg_loss 2.0750\n",
      "epoch 0 step 176000 avg_loss 2.0748\n",
      "epoch 0 step 176200 avg_loss 2.0746\n",
      "epoch 0 step 176400 avg_loss 2.0746\n",
      "epoch 0 step 176600 avg_loss 2.0746\n",
      "epoch 0 step 176800 avg_loss 2.0747\n",
      "epoch 0 step 177000 avg_loss 2.0747\n",
      "epoch 0 step 177200 avg_loss 2.0746\n",
      "epoch 0 step 177400 avg_loss 2.0747\n",
      "epoch 0 step 177600 avg_loss 2.0749\n",
      "epoch 0 step 177800 avg_loss 2.0748\n",
      "epoch 0 step 178000 avg_loss 2.0747\n",
      "epoch 0 step 178200 avg_loss 2.0747\n",
      "epoch 0 step 178400 avg_loss 2.0746\n",
      "epoch 0 step 178600 avg_loss 2.0748\n",
      "epoch 0 step 178800 avg_loss 2.0748\n",
      "epoch 0 step 179000 avg_loss 2.0748\n",
      "epoch 0 step 179200 avg_loss 2.0746\n",
      "epoch 0 step 179400 avg_loss 2.0748\n",
      "epoch 0 step 179600 avg_loss 2.0747\n",
      "epoch 0 step 179800 avg_loss 2.0748\n",
      "epoch 0 step 180000 avg_loss 2.0746\n",
      "epoch 0 step 180200 avg_loss 2.0746\n",
      "epoch 0 step 180400 avg_loss 2.0745\n",
      "epoch 0 step 180600 avg_loss 2.0745\n",
      "epoch 0 step 180800 avg_loss 2.0745\n",
      "epoch 0 step 181000 avg_loss 2.0744\n",
      "epoch 0 step 181200 avg_loss 2.0744\n",
      "epoch 0 step 181400 avg_loss 2.0746\n",
      "epoch 0 step 181600 avg_loss 2.0745\n",
      "epoch 0 step 181800 avg_loss 2.0743\n",
      "epoch 0 step 182000 avg_loss 2.0743\n",
      "epoch 0 step 182200 avg_loss 2.0741\n",
      "epoch 0 step 182400 avg_loss 2.0742\n",
      "epoch 0 step 182600 avg_loss 2.0742\n",
      "epoch 0 step 182800 avg_loss 2.0741\n",
      "epoch 0 step 183000 avg_loss 2.0741\n",
      "epoch 0 step 183200 avg_loss 2.0740\n",
      "epoch 0 step 183400 avg_loss 2.0739\n",
      "epoch 0 step 183600 avg_loss 2.0738\n",
      "epoch 0 step 183800 avg_loss 2.0737\n",
      "epoch 0 step 184000 avg_loss 2.0735\n",
      "epoch 0 step 184200 avg_loss 2.0734\n",
      "epoch 0 step 184400 avg_loss 2.0733\n",
      "epoch 0 step 184600 avg_loss 2.0733\n",
      "epoch 0 step 184800 avg_loss 2.0732\n",
      "epoch 0 step 185000 avg_loss 2.0733\n",
      "epoch 0 step 185200 avg_loss 2.0733\n",
      "epoch 0 step 185400 avg_loss 2.0732\n",
      "epoch 0 step 185600 avg_loss 2.0730\n",
      "epoch 0 step 185800 avg_loss 2.0731\n",
      "epoch 0 step 186000 avg_loss 2.0731\n",
      "epoch 0 step 186200 avg_loss 2.0731\n",
      "epoch 0 step 186400 avg_loss 2.0729\n",
      "epoch 0 step 186600 avg_loss 2.0728\n",
      "epoch 0 step 186800 avg_loss 2.0729\n",
      "epoch 0 step 187000 avg_loss 2.0729\n",
      "epoch 0 step 187200 avg_loss 2.0731\n",
      "epoch 0 step 187400 avg_loss 2.0731\n",
      "epoch 0 step 187600 avg_loss 2.0731\n",
      "epoch 0 step 187800 avg_loss 2.0731\n",
      "epoch 0 step 188000 avg_loss 2.0731\n",
      "epoch 0 step 188200 avg_loss 2.0732\n",
      "epoch 0 step 188400 avg_loss 2.0733\n",
      "epoch 0 step 188600 avg_loss 2.0732\n",
      "epoch 0 step 188800 avg_loss 2.0731\n",
      "epoch 0 step 189000 avg_loss 2.0730\n",
      "epoch 0 step 189200 avg_loss 2.0730\n",
      "epoch 0 step 189400 avg_loss 2.0729\n",
      "epoch 0 step 189600 avg_loss 2.0729\n",
      "epoch 0 step 189800 avg_loss 2.0729\n",
      "epoch 0 step 190000 avg_loss 2.0729\n",
      "epoch 0 step 190200 avg_loss 2.0731\n",
      "epoch 0 step 190400 avg_loss 2.0731\n",
      "epoch 0 step 190600 avg_loss 2.0730\n",
      "epoch 0 step 190800 avg_loss 2.0731\n",
      "epoch 0 step 191000 avg_loss 2.0730\n",
      "epoch 0 step 191200 avg_loss 2.0730\n",
      "epoch 0 step 191400 avg_loss 2.0730\n",
      "epoch 0 step 191600 avg_loss 2.0731\n",
      "epoch 0 step 191800 avg_loss 2.0730\n",
      "epoch 0 step 192000 avg_loss 2.0731\n",
      "epoch 0 step 192200 avg_loss 2.0732\n",
      "epoch 0 step 192400 avg_loss 2.0733\n",
      "epoch 0 step 192600 avg_loss 2.0732\n",
      "epoch 0 step 192800 avg_loss 2.0732\n",
      "epoch 0 step 193000 avg_loss 2.0733\n",
      "epoch 0 step 193200 avg_loss 2.0733\n",
      "epoch 0 step 193400 avg_loss 2.0732\n",
      "epoch 0 step 193600 avg_loss 2.0731\n",
      "epoch 0 step 193800 avg_loss 2.0731\n",
      "epoch 0 step 194000 avg_loss 2.0730\n",
      "epoch 0 step 194200 avg_loss 2.0731\n",
      "epoch 0 step 194400 avg_loss 2.0731\n",
      "epoch 0 step 194600 avg_loss 2.0731\n",
      "epoch 0 step 194800 avg_loss 2.0731\n",
      "epoch 0 step 195000 avg_loss 2.0730\n",
      "epoch 0 step 195200 avg_loss 2.0730\n",
      "epoch 0 step 195400 avg_loss 2.0731\n",
      "epoch 0 step 195600 avg_loss 2.0731\n",
      "epoch 0 step 195800 avg_loss 2.0731\n",
      "epoch 0 step 196000 avg_loss 2.0730\n",
      "epoch 0 step 196200 avg_loss 2.0731\n",
      "epoch 0 step 196400 avg_loss 2.0732\n",
      "epoch 0 step 196600 avg_loss 2.0733\n",
      "epoch 0 step 196800 avg_loss 2.0733\n",
      "epoch 0 step 197000 avg_loss 2.0733\n",
      "epoch 0 step 197200 avg_loss 2.0734\n",
      "epoch 0 step 197400 avg_loss 2.0734\n",
      "epoch 0 step 197600 avg_loss 2.0734\n",
      "epoch 0 step 197800 avg_loss 2.0733\n",
      "epoch 0 step 198000 avg_loss 2.0735\n",
      "epoch 0 step 198200 avg_loss 2.0738\n",
      "epoch 0 step 198400 avg_loss 2.0737\n",
      "epoch 0 step 198600 avg_loss 2.0736\n",
      "epoch 0 step 198800 avg_loss 2.0738\n",
      "epoch 0 step 199000 avg_loss 2.0740\n",
      "epoch 0 step 199200 avg_loss 2.0741\n",
      "epoch 0 step 199400 avg_loss 2.0742\n",
      "epoch 0 step 199600 avg_loss 2.0743\n",
      "epoch 0 step 199800 avg_loss 2.0742\n",
      "epoch 0 step 200000 avg_loss 2.0741\n",
      "epoch 0 step 200200 avg_loss 2.0740\n",
      "epoch 0 step 200400 avg_loss 2.0739\n",
      "epoch 0 step 200600 avg_loss 2.0739\n",
      "epoch 0 step 200800 avg_loss 2.0740\n",
      "epoch 0 step 201000 avg_loss 2.0741\n",
      "epoch 0 step 201200 avg_loss 2.0740\n",
      "epoch 0 step 201400 avg_loss 2.0740\n",
      "epoch 0 step 201600 avg_loss 2.0741\n",
      "epoch 0 step 201800 avg_loss 2.0740\n",
      "epoch 0 step 202000 avg_loss 2.0740\n",
      "epoch 0 step 202200 avg_loss 2.0740\n",
      "epoch 0 step 202400 avg_loss 2.0740\n",
      "epoch 0 step 202600 avg_loss 2.0740\n",
      "epoch 0 step 202800 avg_loss 2.0740\n",
      "epoch 0 step 203000 avg_loss 2.0739\n",
      "epoch 0 step 203200 avg_loss 2.0740\n",
      "epoch 0 step 203400 avg_loss 2.0740\n",
      "epoch 0 step 203600 avg_loss 2.0738\n",
      "epoch 0 step 203800 avg_loss 2.0737\n",
      "epoch 0 step 204000 avg_loss 2.0737\n",
      "epoch 0 step 204200 avg_loss 2.0736\n",
      "epoch 0 step 204400 avg_loss 2.0736\n",
      "epoch 0 step 204600 avg_loss 2.0736\n",
      "epoch 0 step 204800 avg_loss 2.0736\n",
      "epoch 0 step 205000 avg_loss 2.0736\n",
      "epoch 0 step 205200 avg_loss 2.0737\n",
      "epoch 0 step 205400 avg_loss 2.0737\n",
      "epoch 0 step 205600 avg_loss 2.0736\n",
      "epoch 0 step 205800 avg_loss 2.0735\n",
      "epoch 0 step 206000 avg_loss 2.0735\n",
      "epoch 0 step 206200 avg_loss 2.0734\n",
      "epoch 0 step 206400 avg_loss 2.0733\n",
      "epoch 0 step 206600 avg_loss 2.0734\n",
      "epoch 0 step 206800 avg_loss 2.0733\n",
      "epoch 0 step 207000 avg_loss 2.0733\n",
      "epoch 0 step 207200 avg_loss 2.0733\n",
      "epoch 0 step 207400 avg_loss 2.0733\n",
      "epoch 0 step 207600 avg_loss 2.0732\n",
      "epoch 0 step 207800 avg_loss 2.0730\n",
      "epoch 0 step 208000 avg_loss 2.0731\n",
      "epoch 0 step 208200 avg_loss 2.0731\n",
      "epoch 0 step 208400 avg_loss 2.0729\n",
      "epoch 0 step 208600 avg_loss 2.0729\n",
      "epoch 0 step 208800 avg_loss 2.0727\n",
      "epoch 0 step 209000 avg_loss 2.0726\n",
      "epoch 0 step 209200 avg_loss 2.0725\n",
      "epoch 0 step 209400 avg_loss 2.0724\n",
      "epoch 0 step 209600 avg_loss 2.0726\n",
      "epoch 0 step 209800 avg_loss 2.0726\n",
      "epoch 0 step 210000 avg_loss 2.0724\n",
      "epoch 0 step 210200 avg_loss 2.0724\n",
      "epoch 0 step 210400 avg_loss 2.0725\n",
      "epoch 0 step 210600 avg_loss 2.0724\n",
      "epoch 0 step 210800 avg_loss 2.0724\n",
      "epoch 0 step 211000 avg_loss 2.0724\n",
      "epoch 0 step 211200 avg_loss 2.0723\n",
      "epoch 0 step 211400 avg_loss 2.0724\n",
      "epoch 0 step 211600 avg_loss 2.0723\n",
      "epoch 0 step 211800 avg_loss 2.0724\n",
      "epoch 0 step 212000 avg_loss 2.0724\n",
      "epoch 0 step 212200 avg_loss 2.0725\n",
      "epoch 0 step 212400 avg_loss 2.0725\n",
      "epoch 0 step 212600 avg_loss 2.0724\n",
      "epoch 0 step 212800 avg_loss 2.0725\n",
      "epoch 0 step 213000 avg_loss 2.0726\n",
      "epoch 0 step 213200 avg_loss 2.0726\n",
      "epoch 0 step 213400 avg_loss 2.0728\n",
      "epoch 0 step 213600 avg_loss 2.0729\n",
      "epoch 0 step 213800 avg_loss 2.0728\n",
      "epoch 0 step 214000 avg_loss 2.0728\n",
      "epoch 0 step 214200 avg_loss 2.0727\n",
      "epoch 0 step 214400 avg_loss 2.0726\n",
      "epoch 0 step 214600 avg_loss 2.0727\n",
      "epoch 0 step 214800 avg_loss 2.0726\n",
      "epoch 0 step 215000 avg_loss 2.0726\n",
      "epoch 0 step 215200 avg_loss 2.0727\n",
      "epoch 0 step 215400 avg_loss 2.0729\n",
      "epoch 0 step 215600 avg_loss 2.0730\n",
      "epoch 0 step 215800 avg_loss 2.0730\n",
      "epoch 0 step 216000 avg_loss 2.0729\n",
      "epoch 0 step 216200 avg_loss 2.0729\n",
      "epoch 0 step 216400 avg_loss 2.0728\n",
      "epoch 0 step 216600 avg_loss 2.0729\n",
      "epoch 0 step 216800 avg_loss 2.0728\n",
      "epoch 0 step 217000 avg_loss 2.0728\n",
      "epoch 0 step 217200 avg_loss 2.0727\n",
      "epoch 0 step 217400 avg_loss 2.0727\n",
      "epoch 0 step 217600 avg_loss 2.0727\n",
      "epoch 0 step 217800 avg_loss 2.0727\n",
      "epoch 0 step 218000 avg_loss 2.0726\n",
      "epoch 0 step 218200 avg_loss 2.0727\n",
      "epoch 0 step 218400 avg_loss 2.0728\n",
      "epoch 0 step 218600 avg_loss 2.0727\n",
      "epoch 0 step 218800 avg_loss 2.0727\n",
      "epoch 0 step 219000 avg_loss 2.0727\n",
      "epoch 0 step 219200 avg_loss 2.0727\n",
      "epoch 0 step 219400 avg_loss 2.0727\n",
      "epoch 0 step 219600 avg_loss 2.0727\n",
      "epoch 0 step 219800 avg_loss 2.0725\n",
      "epoch 0 step 220000 avg_loss 2.0725\n",
      "epoch 0 step 220200 avg_loss 2.0724\n",
      "epoch 0 step 220400 avg_loss 2.0724\n",
      "epoch 0 step 220600 avg_loss 2.0723\n",
      "epoch 0 step 220800 avg_loss 2.0722\n",
      "epoch 0 step 221000 avg_loss 2.0721\n",
      "epoch 0 step 221200 avg_loss 2.0721\n",
      "epoch 0 step 221400 avg_loss 2.0720\n",
      "epoch 0 step 221600 avg_loss 2.0721\n",
      "epoch 0 step 221800 avg_loss 2.0721\n",
      "epoch 0 step 222000 avg_loss 2.0721\n",
      "epoch 0 step 222200 avg_loss 2.0720\n",
      "epoch 0 step 222400 avg_loss 2.0719\n",
      "epoch 0 step 222600 avg_loss 2.0719\n",
      "epoch 0 step 222800 avg_loss 2.0718\n",
      "epoch 0 step 223000 avg_loss 2.0716\n",
      "epoch 0 step 223200 avg_loss 2.0715\n",
      "epoch 0 step 223400 avg_loss 2.0713\n",
      "epoch 0 step 223600 avg_loss 2.0713\n",
      "epoch 0 step 223800 avg_loss 2.0712\n",
      "epoch 0 step 224000 avg_loss 2.0711\n",
      "epoch 0 step 224200 avg_loss 2.0707\n",
      "epoch 0 step 224400 avg_loss 2.0707\n",
      "epoch 0 step 224600 avg_loss 2.0706\n",
      "epoch 0 step 224800 avg_loss 2.0707\n",
      "epoch 0 step 225000 avg_loss 2.0706\n",
      "epoch 0 step 225200 avg_loss 2.0706\n",
      "epoch 0 step 225400 avg_loss 2.0705\n",
      "epoch 0 step 225600 avg_loss 2.0705\n",
      "epoch 0 step 225800 avg_loss 2.0704\n",
      "epoch 0 step 226000 avg_loss 2.0703\n",
      "epoch 0 step 226200 avg_loss 2.0702\n",
      "epoch 0 step 226400 avg_loss 2.0700\n",
      "epoch 0 step 226600 avg_loss 2.0700\n",
      "epoch 0 step 226800 avg_loss 2.0700\n",
      "epoch 0 step 227000 avg_loss 2.0699\n",
      "epoch 0 step 227200 avg_loss 2.0699\n",
      "epoch 0 step 227400 avg_loss 2.0699\n",
      "epoch 0 step 227600 avg_loss 2.0699\n",
      "epoch 0 step 227800 avg_loss 2.0699\n",
      "epoch 0 step 228000 avg_loss 2.0700\n",
      "epoch 0 step 228200 avg_loss 2.0701\n",
      "epoch 0 step 228400 avg_loss 2.0700\n",
      "epoch 0 step 228600 avg_loss 2.0699\n",
      "epoch 0 step 228800 avg_loss 2.0698\n",
      "epoch 0 step 229000 avg_loss 2.0698\n",
      "epoch 0 step 229200 avg_loss 2.0698\n",
      "epoch 0 step 229400 avg_loss 2.0699\n",
      "epoch 0 step 229600 avg_loss 2.0698\n",
      "epoch 0 step 229800 avg_loss 2.0699\n",
      "epoch 0 step 230000 avg_loss 2.0699\n",
      "epoch 0 step 230200 avg_loss 2.0700\n",
      "epoch 0 step 230400 avg_loss 2.0700\n",
      "epoch 0 step 230600 avg_loss 2.0700\n",
      "epoch 0 step 230800 avg_loss 2.0699\n",
      "epoch 0 step 231000 avg_loss 2.0698\n",
      "epoch 0 step 231200 avg_loss 2.0699\n",
      "epoch 0 step 231400 avg_loss 2.0699\n",
      "epoch 0 step 231600 avg_loss 2.0699\n",
      "epoch 0 step 231800 avg_loss 2.0698\n",
      "epoch 0 step 232000 avg_loss 2.0697\n",
      "epoch 0 step 232200 avg_loss 2.0698\n",
      "epoch 0 step 232400 avg_loss 2.0698\n",
      "epoch 0 step 232600 avg_loss 2.0699\n",
      "epoch 0 step 232800 avg_loss 2.0698\n",
      "epoch 0 step 233000 avg_loss 2.0699\n",
      "epoch 0 step 233200 avg_loss 2.0699\n",
      "epoch 0 step 233400 avg_loss 2.0699\n",
      "epoch 0 step 233600 avg_loss 2.0698\n",
      "epoch 0 step 233800 avg_loss 2.0696\n",
      "epoch 0 step 234000 avg_loss 2.0697\n",
      "epoch 0 step 234200 avg_loss 2.0695\n",
      "epoch 0 step 234400 avg_loss 2.0695\n",
      "epoch 0 step 234600 avg_loss 2.0694\n",
      "epoch 0 step 234800 avg_loss 2.0694\n",
      "epoch 0 step 235000 avg_loss 2.0693\n",
      "epoch 0 step 235200 avg_loss 2.0692\n",
      "epoch 0 step 235400 avg_loss 2.0691\n",
      "epoch 0 step 235600 avg_loss 2.0692\n",
      "epoch 0 step 235800 avg_loss 2.0691\n",
      "epoch 0 step 236000 avg_loss 2.0693\n",
      "epoch 0 step 236200 avg_loss 2.0693\n",
      "epoch 0 step 236400 avg_loss 2.0695\n",
      "epoch 0 step 236600 avg_loss 2.0695\n",
      "epoch 0 step 236800 avg_loss 2.0696\n",
      "epoch 0 step 237000 avg_loss 2.0695\n",
      "epoch 0 step 237200 avg_loss 2.0695\n",
      "epoch 0 step 237400 avg_loss 2.0696\n",
      "epoch 0 step 237600 avg_loss 2.0698\n",
      "epoch 0 step 237800 avg_loss 2.0698\n",
      "epoch 0 step 238000 avg_loss 2.0698\n",
      "epoch 0 step 238200 avg_loss 2.0697\n",
      "epoch 0 step 238400 avg_loss 2.0696\n",
      "epoch 0 step 238600 avg_loss 2.0695\n",
      "epoch 0 step 238800 avg_loss 2.0694\n",
      "epoch 0 step 239000 avg_loss 2.0693\n",
      "epoch 0 step 239200 avg_loss 2.0693\n",
      "epoch 0 step 239400 avg_loss 2.0693\n",
      "epoch 0 step 239600 avg_loss 2.0691\n",
      "epoch 0 step 239800 avg_loss 2.0691\n",
      "epoch 0 step 240000 avg_loss 2.0690\n",
      "epoch 0 step 240200 avg_loss 2.0690\n",
      "epoch 0 step 240400 avg_loss 2.0690\n",
      "epoch 0 step 240600 avg_loss 2.0689\n",
      "epoch 0 step 240800 avg_loss 2.0689\n",
      "epoch 0 step 241000 avg_loss 2.0688\n",
      "epoch 0 step 241200 avg_loss 2.0687\n",
      "epoch 0 step 241400 avg_loss 2.0688\n",
      "epoch 0 step 241600 avg_loss 2.0688\n",
      "epoch 0 step 241800 avg_loss 2.0688\n",
      "epoch 0 step 242000 avg_loss 2.0688\n",
      "epoch 0 step 242200 avg_loss 2.0687\n",
      "epoch 0 step 242400 avg_loss 2.0688\n",
      "epoch 0 step 242600 avg_loss 2.0688\n",
      "epoch 0 step 242800 avg_loss 2.0686\n",
      "epoch 0 step 243000 avg_loss 2.0685\n",
      "epoch 0 step 243200 avg_loss 2.0682\n",
      "epoch 0 step 243400 avg_loss 2.0682\n",
      "epoch 0 step 243600 avg_loss 2.0681\n",
      "epoch 0 step 243800 avg_loss 2.0682\n",
      "epoch 0 step 244000 avg_loss 2.0682\n",
      "epoch 0 step 244200 avg_loss 2.0681\n",
      "epoch 0 step 244400 avg_loss 2.0680\n",
      "epoch 0 step 244600 avg_loss 2.0682\n",
      "epoch 0 step 244800 avg_loss 2.0683\n",
      "epoch 0 step 245000 avg_loss 2.0683\n",
      "epoch 0 step 245200 avg_loss 2.0684\n",
      "epoch 0 step 245400 avg_loss 2.0684\n",
      "epoch 0 step 245600 avg_loss 2.0684\n",
      "epoch 0 step 245800 avg_loss 2.0683\n",
      "epoch 0 step 246000 avg_loss 2.0683\n",
      "epoch 0 step 246200 avg_loss 2.0683\n",
      "epoch 0 step 246400 avg_loss 2.0682\n",
      "epoch 0 step 246600 avg_loss 2.0680\n",
      "epoch 0 step 246800 avg_loss 2.0680\n",
      "epoch 0 step 247000 avg_loss 2.0681\n",
      "epoch 0 step 247200 avg_loss 2.0681\n",
      "epoch 0 step 247400 avg_loss 2.0682\n",
      "epoch 0 step 247600 avg_loss 2.0683\n",
      "epoch 0 step 247800 avg_loss 2.0683\n",
      "epoch 0 step 248000 avg_loss 2.0682\n",
      "epoch 0 step 248200 avg_loss 2.0681\n",
      "epoch 0 step 248400 avg_loss 2.0681\n",
      "epoch 0 step 248600 avg_loss 2.0681\n",
      "epoch 0 step 248800 avg_loss 2.0683\n",
      "epoch 0 step 249000 avg_loss 2.0682\n",
      "epoch 0 step 249200 avg_loss 2.0681\n",
      "epoch 0 step 249400 avg_loss 2.0681\n",
      "epoch 0 step 249600 avg_loss 2.0681\n",
      "epoch 0 step 249800 avg_loss 2.0680\n",
      "epoch 0 step 250000 avg_loss 2.0679\n",
      "epoch 0 step 250200 avg_loss 2.0679\n",
      "epoch 0 step 250400 avg_loss 2.0679\n",
      "epoch 0 step 250600 avg_loss 2.0678\n",
      "epoch 0 step 250800 avg_loss 2.0679\n",
      "epoch 0 step 251000 avg_loss 2.0678\n",
      "epoch 0 step 251200 avg_loss 2.0676\n",
      "epoch 0 step 251400 avg_loss 2.0677\n",
      "epoch 0 step 251600 avg_loss 2.0677\n",
      "epoch 0 step 251800 avg_loss 2.0678\n",
      "epoch 0 step 252000 avg_loss 2.0677\n",
      "epoch 0 step 252200 avg_loss 2.0674\n",
      "epoch 0 step 252400 avg_loss 2.0673\n",
      "epoch 0 step 252600 avg_loss 2.0673\n",
      "epoch 0 step 252800 avg_loss 2.0673\n",
      "epoch 0 step 253000 avg_loss 2.0673\n",
      "epoch 0 step 253200 avg_loss 2.0673\n",
      "epoch 0 step 253400 avg_loss 2.0673\n",
      "epoch 0 step 253600 avg_loss 2.0672\n",
      "epoch 0 step 253800 avg_loss 2.0671\n",
      "epoch 0 step 254000 avg_loss 2.0671\n",
      "epoch 0 step 254200 avg_loss 2.0672\n",
      "epoch 0 step 254400 avg_loss 2.0671\n",
      "epoch 0 step 254600 avg_loss 2.0672\n",
      "epoch 0 step 254800 avg_loss 2.0672\n",
      "epoch 0 step 255000 avg_loss 2.0670\n",
      "epoch 0 step 255200 avg_loss 2.0671\n",
      "epoch 0 step 255400 avg_loss 2.0671\n",
      "epoch 0 step 255600 avg_loss 2.0671\n",
      "epoch 0 step 255800 avg_loss 2.0671\n",
      "epoch 0 step 256000 avg_loss 2.0671\n",
      "epoch 0 step 256200 avg_loss 2.0671\n",
      "epoch 0 step 256400 avg_loss 2.0670\n",
      "epoch 0 step 256600 avg_loss 2.0670\n",
      "epoch 0 step 256800 avg_loss 2.0670\n",
      "epoch 0 step 257000 avg_loss 2.0670\n",
      "epoch 0 step 257200 avg_loss 2.0670\n",
      "epoch 0 step 257400 avg_loss 2.0670\n",
      "epoch 0 step 257600 avg_loss 2.0670\n",
      "epoch 0 step 257800 avg_loss 2.0669\n",
      "epoch 0 step 258000 avg_loss 2.0668\n",
      "epoch 0 step 258200 avg_loss 2.0669\n",
      "epoch 0 step 258400 avg_loss 2.0668\n",
      "epoch 0 step 258600 avg_loss 2.0668\n",
      "epoch 0 step 258800 avg_loss 2.0668\n",
      "epoch 0 step 259000 avg_loss 2.0667\n",
      "epoch 0 step 259200 avg_loss 2.0666\n",
      "epoch 0 step 259400 avg_loss 2.0666\n",
      "epoch 0 step 259600 avg_loss 2.0666\n",
      "epoch 0 step 259800 avg_loss 2.0666\n",
      "epoch 0 step 260000 avg_loss 2.0666\n",
      "epoch 0 step 260200 avg_loss 2.0667\n",
      "epoch 0 step 260400 avg_loss 2.0667\n",
      "epoch 0 step 260600 avg_loss 2.0668\n",
      "epoch 0 step 260800 avg_loss 2.0668\n",
      "epoch 0 step 261000 avg_loss 2.0667\n",
      "epoch 0 step 261200 avg_loss 2.0667\n",
      "epoch 0 step 261400 avg_loss 2.0666\n",
      "epoch 0 step 261600 avg_loss 2.0666\n",
      "epoch 0 step 261800 avg_loss 2.0665\n",
      "epoch 0 step 262000 avg_loss 2.0665\n",
      "epoch 0 step 262200 avg_loss 2.0663\n",
      "epoch 0 step 262400 avg_loss 2.0663\n",
      "epoch 0 step 262600 avg_loss 2.0664\n",
      "epoch 0 step 262800 avg_loss 2.0664\n",
      "epoch 0 step 263000 avg_loss 2.0664\n",
      "epoch 0 step 263200 avg_loss 2.0664\n",
      "epoch 0 step 263400 avg_loss 2.0663\n",
      "epoch 0 step 263600 avg_loss 2.0661\n",
      "epoch 0 step 263800 avg_loss 2.0661\n",
      "epoch 0 step 264000 avg_loss 2.0663\n",
      "epoch 0 step 264200 avg_loss 2.0663\n",
      "epoch 0 step 264400 avg_loss 2.0663\n",
      "epoch 0 step 264600 avg_loss 2.0664\n",
      "epoch 0 step 264800 avg_loss 2.0664\n",
      "epoch 0 step 265000 avg_loss 2.0664\n",
      "epoch 0 step 265200 avg_loss 2.0664\n",
      "epoch 0 step 265400 avg_loss 2.0665\n",
      "epoch 0 step 265600 avg_loss 2.0664\n",
      "epoch 0 step 265800 avg_loss 2.0663\n",
      "epoch 0 step 266000 avg_loss 2.0663\n",
      "epoch 0 step 266200 avg_loss 2.0663\n",
      "epoch 0 step 266400 avg_loss 2.0663\n",
      "epoch 0 step 266600 avg_loss 2.0663\n",
      "epoch 0 step 266800 avg_loss 2.0663\n",
      "epoch 0 step 267000 avg_loss 2.0664\n",
      "epoch 0 step 267200 avg_loss 2.0664\n",
      "epoch 0 step 267400 avg_loss 2.0663\n",
      "epoch 0 step 267600 avg_loss 2.0663\n",
      "epoch 0 step 267800 avg_loss 2.0662\n",
      "epoch 0 step 268000 avg_loss 2.0662\n",
      "epoch 0 step 268200 avg_loss 2.0662\n",
      "epoch 0 step 268400 avg_loss 2.0662\n",
      "epoch 0 step 268600 avg_loss 2.0661\n",
      "epoch 0 step 268800 avg_loss 2.0660\n",
      "epoch 0 step 269000 avg_loss 2.0660\n",
      "epoch 0 step 269200 avg_loss 2.0661\n",
      "epoch 0 step 269400 avg_loss 2.0660\n",
      "epoch 0 step 269600 avg_loss 2.0661\n",
      "epoch 0 step 269800 avg_loss 2.0661\n",
      "epoch 0 step 270000 avg_loss 2.0660\n",
      "epoch 0 step 270200 avg_loss 2.0661\n",
      "epoch 0 step 270400 avg_loss 2.0661\n",
      "epoch 0 step 270600 avg_loss 2.0661\n",
      "epoch 0 step 270800 avg_loss 2.0661\n",
      "epoch 0 step 271000 avg_loss 2.0661\n",
      "epoch 0 step 271200 avg_loss 2.0659\n",
      "epoch 0 step 271400 avg_loss 2.0658\n",
      "epoch 0 step 271600 avg_loss 2.0658\n",
      "epoch 0 step 271800 avg_loss 2.0658\n",
      "epoch 0 step 272000 avg_loss 2.0658\n",
      "epoch 0 step 272200 avg_loss 2.0658\n",
      "epoch 0 step 272400 avg_loss 2.0658\n",
      "epoch 0 step 272600 avg_loss 2.0656\n",
      "epoch 0 step 272800 avg_loss 2.0656\n",
      "epoch 0 step 273000 avg_loss 2.0655\n",
      "epoch 0 step 273200 avg_loss 2.0655\n",
      "epoch 0 step 273400 avg_loss 2.0654\n",
      "epoch 0 step 273600 avg_loss 2.0653\n",
      "epoch 0 step 273800 avg_loss 2.0653\n",
      "epoch 0 step 274000 avg_loss 2.0654\n",
      "epoch 0 step 274200 avg_loss 2.0653\n",
      "epoch 0 step 274400 avg_loss 2.0653\n",
      "epoch 0 step 274600 avg_loss 2.0653\n",
      "epoch 0 step 274800 avg_loss 2.0652\n",
      "epoch 0 step 275000 avg_loss 2.0652\n",
      "epoch 0 step 275200 avg_loss 2.0652\n",
      "epoch 0 step 275400 avg_loss 2.0652\n",
      "epoch 0 step 275600 avg_loss 2.0650\n",
      "epoch 0 step 275800 avg_loss 2.0649\n",
      "epoch 0 step 276000 avg_loss 2.0649\n",
      "epoch 0 step 276200 avg_loss 2.0650\n",
      "epoch 0 step 276400 avg_loss 2.0650\n",
      "epoch 0 step 276600 avg_loss 2.0651\n",
      "epoch 0 step 276800 avg_loss 2.0650\n",
      "epoch 0 step 277000 avg_loss 2.0650\n",
      "epoch 0 step 277200 avg_loss 2.0649\n",
      "epoch 0 step 277400 avg_loss 2.0648\n",
      "epoch 0 step 277600 avg_loss 2.0648\n",
      "epoch 0 step 277800 avg_loss 2.0647\n",
      "epoch 0 step 278000 avg_loss 2.0646\n",
      "epoch 0 step 278200 avg_loss 2.0646\n",
      "epoch 0 step 278400 avg_loss 2.0646\n",
      "epoch 0 step 278600 avg_loss 2.0645\n",
      "epoch 0 step 278800 avg_loss 2.0643\n",
      "epoch 0 step 279000 avg_loss 2.0645\n",
      "epoch 0 step 279200 avg_loss 2.0643\n",
      "epoch 0 step 279400 avg_loss 2.0643\n",
      "epoch 0 step 279600 avg_loss 2.0643\n",
      "epoch 0 step 279800 avg_loss 2.0642\n",
      "epoch 0 step 280000 avg_loss 2.0644\n",
      "epoch 0 step 280200 avg_loss 2.0644\n",
      "epoch 0 step 280400 avg_loss 2.0642\n",
      "epoch 0 step 280600 avg_loss 2.0642\n",
      "epoch 0 step 280800 avg_loss 2.0642\n",
      "epoch 0 step 281000 avg_loss 2.0641\n",
      "epoch 0 step 281200 avg_loss 2.0641\n",
      "epoch 0 step 281400 avg_loss 2.0641\n",
      "epoch 0 step 281600 avg_loss 2.0641\n",
      "epoch 0 step 281800 avg_loss 2.0641\n",
      "epoch 0 step 282000 avg_loss 2.0642\n",
      "epoch 0 step 282200 avg_loss 2.0642\n",
      "epoch 0 step 282400 avg_loss 2.0641\n",
      "epoch 0 step 282600 avg_loss 2.0642\n",
      "epoch 0 step 282800 avg_loss 2.0641\n",
      "epoch 0 step 283000 avg_loss 2.0642\n",
      "epoch 0 step 283200 avg_loss 2.0641\n",
      "epoch 0 step 283400 avg_loss 2.0640\n",
      "epoch 0 step 283600 avg_loss 2.0640\n",
      "epoch 0 step 283800 avg_loss 2.0640\n",
      "epoch 0 step 284000 avg_loss 2.0639\n",
      "epoch 0 step 284200 avg_loss 2.0639\n",
      "epoch 0 step 284400 avg_loss 2.0638\n",
      "epoch 0 step 284600 avg_loss 2.0638\n",
      "epoch 0 step 284800 avg_loss 2.0638\n",
      "epoch 0 step 285000 avg_loss 2.0638\n",
      "epoch 0 step 285200 avg_loss 2.0638\n",
      "epoch 0 step 285400 avg_loss 2.0636\n",
      "epoch 0 step 285600 avg_loss 2.0636\n",
      "epoch 0 step 285800 avg_loss 2.0636\n",
      "epoch 0 step 286000 avg_loss 2.0636\n",
      "epoch 0 step 286200 avg_loss 2.0636\n",
      "epoch 0 step 286400 avg_loss 2.0636\n",
      "epoch 0 step 286600 avg_loss 2.0636\n",
      "epoch 0 step 286800 avg_loss 2.0636\n",
      "epoch 0 step 287000 avg_loss 2.0636\n",
      "epoch 0 step 287200 avg_loss 2.0636\n",
      "epoch 0 step 287400 avg_loss 2.0637\n",
      "epoch 0 step 287600 avg_loss 2.0638\n",
      "epoch 0 step 287800 avg_loss 2.0638\n",
      "epoch 0 step 288000 avg_loss 2.0637\n",
      "epoch 0 step 288200 avg_loss 2.0636\n",
      "epoch 0 step 288400 avg_loss 2.0635\n",
      "epoch 0 step 288600 avg_loss 2.0634\n",
      "epoch 0 step 288800 avg_loss 2.0634\n",
      "epoch 0 step 289000 avg_loss 2.0634\n",
      "epoch 0 step 289200 avg_loss 2.0634\n",
      "epoch 0 step 289400 avg_loss 2.0634\n",
      "epoch 0 step 289600 avg_loss 2.0634\n",
      "epoch 0 step 289800 avg_loss 2.0633\n",
      "epoch 0 step 290000 avg_loss 2.0634\n",
      "epoch 0 step 290200 avg_loss 2.0634\n",
      "epoch 0 step 290400 avg_loss 2.0634\n",
      "epoch 0 step 290600 avg_loss 2.0633\n",
      "epoch 0 step 290800 avg_loss 2.0634\n",
      "epoch 0 step 291000 avg_loss 2.0634\n",
      "epoch 0 step 291200 avg_loss 2.0634\n",
      "epoch 0 step 291400 avg_loss 2.0635\n",
      "epoch 0 step 291600 avg_loss 2.0634\n",
      "epoch 0 step 291800 avg_loss 2.0633\n",
      "epoch 0 step 292000 avg_loss 2.0632\n",
      "epoch 0 step 292200 avg_loss 2.0631\n",
      "epoch 0 step 292400 avg_loss 2.0630\n",
      "epoch 0 step 292600 avg_loss 2.0630\n",
      "epoch 0 step 292800 avg_loss 2.0631\n",
      "epoch 0 step 293000 avg_loss 2.0629\n",
      "epoch 0 step 293200 avg_loss 2.0629\n",
      "epoch 0 step 293400 avg_loss 2.0628\n",
      "epoch 0 step 293600 avg_loss 2.0628\n",
      "epoch 0 step 293800 avg_loss 2.0628\n",
      "epoch 0 step 294000 avg_loss 2.0628\n",
      "epoch 0 step 294200 avg_loss 2.0628\n",
      "epoch 0 step 294400 avg_loss 2.0629\n",
      "epoch 0 step 294600 avg_loss 2.0628\n",
      "epoch 0 step 294800 avg_loss 2.0626\n",
      "epoch 0 step 295000 avg_loss 2.0626\n",
      "epoch 0 step 295200 avg_loss 2.0626\n",
      "epoch 0 step 295400 avg_loss 2.0626\n",
      "epoch 0 step 295600 avg_loss 2.0627\n",
      "epoch 0 step 295800 avg_loss 2.0627\n",
      "epoch 0 step 296000 avg_loss 2.0627\n",
      "epoch 0 step 296200 avg_loss 2.0627\n",
      "epoch 0 step 296400 avg_loss 2.0628\n",
      "epoch 0 step 296600 avg_loss 2.0628\n",
      "epoch 0 step 296800 avg_loss 2.0628\n",
      "epoch 0 step 297000 avg_loss 2.0626\n",
      "epoch 0 step 297200 avg_loss 2.0626\n",
      "epoch 0 step 297400 avg_loss 2.0626\n",
      "epoch 0 step 297600 avg_loss 2.0625\n",
      "epoch 0 step 297800 avg_loss 2.0625\n",
      "epoch 0 step 298000 avg_loss 2.0624\n",
      "epoch 0 step 298200 avg_loss 2.0624\n",
      "epoch 0 step 298400 avg_loss 2.0623\n",
      "epoch 0 step 298600 avg_loss 2.0623\n",
      "epoch 0 step 298800 avg_loss 2.0621\n",
      "epoch 0 step 299000 avg_loss 2.0621\n",
      "epoch 0 step 299200 avg_loss 2.0622\n",
      "epoch 0 step 299400 avg_loss 2.0622\n",
      "epoch 0 step 299600 avg_loss 2.0621\n",
      "epoch 0 step 299800 avg_loss 2.0621\n",
      "epoch 0 step 300000 avg_loss 2.0622\n",
      "epoch 0 step 300200 avg_loss 2.0622\n",
      "epoch 0 step 300400 avg_loss 2.0621\n",
      "epoch 0 step 300600 avg_loss 2.0620\n",
      "epoch 0 step 300800 avg_loss 2.0620\n",
      "epoch 0 step 301000 avg_loss 2.0621\n",
      "epoch 0 step 301200 avg_loss 2.0621\n",
      "epoch 0 step 301400 avg_loss 2.0621\n",
      "epoch 0 step 301600 avg_loss 2.0623\n",
      "epoch 0 step 301800 avg_loss 2.0623\n",
      "epoch 0 step 302000 avg_loss 2.0624\n",
      "epoch 0 step 302200 avg_loss 2.0624\n",
      "epoch 0 step 302400 avg_loss 2.0624\n",
      "epoch 0 step 302600 avg_loss 2.0624\n",
      "epoch 0 step 302800 avg_loss 2.0625\n",
      "epoch 0 step 303000 avg_loss 2.0625\n",
      "epoch 0 step 303200 avg_loss 2.0625\n",
      "epoch 0 step 303400 avg_loss 2.0625\n",
      "epoch 0 step 303600 avg_loss 2.0625\n",
      "epoch 0 step 303800 avg_loss 2.0625\n",
      "epoch 0 step 304000 avg_loss 2.0625\n",
      "epoch 0 step 304200 avg_loss 2.0625\n",
      "epoch 0 step 304400 avg_loss 2.0624\n",
      "epoch 0 step 304600 avg_loss 2.0625\n",
      "epoch 0 step 304800 avg_loss 2.0627\n",
      "epoch 0 step 305000 avg_loss 2.0627\n",
      "epoch 0 step 305200 avg_loss 2.0627\n",
      "epoch 0 step 305400 avg_loss 2.0627\n",
      "epoch 0 step 305600 avg_loss 2.0627\n",
      "epoch 0 step 305800 avg_loss 2.0628\n",
      "epoch 0 step 306000 avg_loss 2.0627\n",
      "epoch 0 step 306200 avg_loss 2.0628\n",
      "epoch 0 step 306400 avg_loss 2.0629\n",
      "epoch 0 step 306600 avg_loss 2.0628\n",
      "epoch 0 step 306800 avg_loss 2.0628\n",
      "epoch 0 step 307000 avg_loss 2.0627\n",
      "epoch 0 step 307200 avg_loss 2.0627\n",
      "epoch 0 step 307400 avg_loss 2.0626\n",
      "epoch 0 step 307600 avg_loss 2.0626\n",
      "epoch 0 step 307800 avg_loss 2.0626\n",
      "epoch 0 step 308000 avg_loss 2.0626\n",
      "epoch 0 step 308200 avg_loss 2.0626\n",
      "epoch 0 step 308400 avg_loss 2.0625\n",
      "epoch 0 step 308600 avg_loss 2.0625\n",
      "epoch 0 step 308800 avg_loss 2.0625\n",
      "epoch 0 step 309000 avg_loss 2.0626\n",
      "epoch 0 step 309200 avg_loss 2.0626\n",
      "epoch 0 step 309400 avg_loss 2.0626\n",
      "epoch 0 step 309600 avg_loss 2.0625\n",
      "epoch 0 step 309800 avg_loss 2.0626\n",
      "epoch 0 step 310000 avg_loss 2.0626\n",
      "epoch 0 step 310200 avg_loss 2.0626\n",
      "epoch 0 step 310400 avg_loss 2.0624\n",
      "epoch 0 step 310600 avg_loss 2.0625\n",
      "epoch 0 step 310800 avg_loss 2.0626\n",
      "epoch 0 step 311000 avg_loss 2.0626\n",
      "epoch 0 step 311200 avg_loss 2.0626\n",
      "epoch 0 step 311400 avg_loss 2.0626\n",
      "epoch 0 step 311600 avg_loss 2.0626\n",
      "epoch 0 step 311800 avg_loss 2.0625\n",
      "epoch 0 step 312000 avg_loss 2.0625\n",
      "epoch 0 step 312200 avg_loss 2.0624\n",
      "epoch 0 step 312400 avg_loss 2.0624\n",
      "epoch 0 step 312600 avg_loss 2.0623\n",
      "epoch 0 step 312800 avg_loss 2.0623\n",
      "epoch 0 step 313000 avg_loss 2.0623\n",
      "epoch 0 step 313200 avg_loss 2.0623\n",
      "epoch 0 step 313400 avg_loss 2.0623\n",
      "epoch 0 step 313600 avg_loss 2.0623\n",
      "epoch 0 step 313800 avg_loss 2.0624\n",
      "epoch 0 step 314000 avg_loss 2.0624\n",
      "epoch 0 step 314200 avg_loss 2.0624\n",
      "epoch 0 step 314400 avg_loss 2.0624\n",
      "epoch 0 step 314600 avg_loss 2.0624\n",
      "epoch 0 step 314800 avg_loss 2.0626\n",
      "epoch 0 step 315000 avg_loss 2.0626\n",
      "epoch 0 step 315200 avg_loss 2.0626\n",
      "epoch 0 step 315400 avg_loss 2.0625\n",
      "epoch 0 step 315600 avg_loss 2.0625\n",
      "epoch 0 step 315800 avg_loss 2.0624\n",
      "epoch 0 step 316000 avg_loss 2.0625\n",
      "epoch 0 step 316200 avg_loss 2.0624\n",
      "epoch 0 step 316400 avg_loss 2.0623\n",
      "epoch 0 step 316600 avg_loss 2.0622\n",
      "epoch 0 step 316800 avg_loss 2.0622\n",
      "epoch 0 step 317000 avg_loss 2.0623\n",
      "epoch 0 step 317200 avg_loss 2.0623\n",
      "epoch 0 step 317400 avg_loss 2.0623\n",
      "epoch 0 step 317600 avg_loss 2.0623\n",
      "epoch 0 step 317800 avg_loss 2.0622\n",
      "epoch 0 step 318000 avg_loss 2.0623\n",
      "epoch 0 step 318200 avg_loss 2.0623\n",
      "epoch 0 step 318400 avg_loss 2.0623\n",
      "epoch 0 step 318600 avg_loss 2.0622\n",
      "epoch 0 step 318800 avg_loss 2.0622\n",
      "epoch 0 step 319000 avg_loss 2.0621\n",
      "epoch 0 step 319200 avg_loss 2.0621\n",
      "epoch 0 step 319400 avg_loss 2.0622\n",
      "epoch 0 step 319600 avg_loss 2.0623\n",
      "epoch 0 step 319800 avg_loss 2.0623\n",
      "epoch 0 step 320000 avg_loss 2.0623\n",
      "epoch 0 step 320200 avg_loss 2.0623\n",
      "epoch 0 step 320400 avg_loss 2.0622\n",
      "epoch 0 step 320600 avg_loss 2.0623\n",
      "epoch 0 step 320800 avg_loss 2.0622\n",
      "epoch 0 step 321000 avg_loss 2.0621\n",
      "epoch 0 step 321200 avg_loss 2.0621\n",
      "epoch 0 step 321400 avg_loss 2.0620\n",
      "epoch 0 step 321600 avg_loss 2.0620\n",
      "epoch 0 step 321800 avg_loss 2.0619\n",
      "epoch 0 step 322000 avg_loss 2.0619\n",
      "epoch 0 step 322200 avg_loss 2.0618\n",
      "epoch 0 step 322400 avg_loss 2.0618\n",
      "epoch 0 step 322600 avg_loss 2.0618\n",
      "epoch 0 step 322800 avg_loss 2.0617\n",
      "epoch 0 step 323000 avg_loss 2.0616\n",
      "epoch 0 step 323200 avg_loss 2.0615\n",
      "epoch 0 step 323400 avg_loss 2.0615\n",
      "epoch 0 step 323600 avg_loss 2.0615\n",
      "epoch 0 step 323800 avg_loss 2.0615\n",
      "epoch 0 step 324000 avg_loss 2.0613\n",
      "epoch 0 step 324200 avg_loss 2.0613\n",
      "epoch 0 step 324400 avg_loss 2.0613\n",
      "epoch 0 step 324600 avg_loss 2.0612\n",
      "epoch 0 step 324800 avg_loss 2.0611\n",
      "epoch 0 step 325000 avg_loss 2.0612\n",
      "epoch 0 step 325200 avg_loss 2.0612\n",
      "epoch 0 step 325400 avg_loss 2.0612\n",
      "epoch 0 step 325600 avg_loss 2.0613\n",
      "epoch 0 step 325800 avg_loss 2.0613\n",
      "epoch 0 step 326000 avg_loss 2.0612\n",
      "epoch 0 step 326200 avg_loss 2.0613\n",
      "epoch 0 step 326400 avg_loss 2.0613\n",
      "epoch 0 step 326600 avg_loss 2.0613\n",
      "epoch 0 step 326800 avg_loss 2.0613\n",
      "epoch 0 step 327000 avg_loss 2.0613\n",
      "epoch 0 step 327200 avg_loss 2.0613\n",
      "epoch 0 step 327400 avg_loss 2.0614\n",
      "epoch 0 step 327600 avg_loss 2.0614\n",
      "epoch 0 step 327800 avg_loss 2.0614\n",
      "epoch 0 step 328000 avg_loss 2.0614\n",
      "epoch 0 step 328200 avg_loss 2.0613\n",
      "epoch 0 step 328400 avg_loss 2.0612\n",
      "epoch 0 step 328600 avg_loss 2.0613\n",
      "epoch 0 step 328800 avg_loss 2.0612\n",
      "epoch 0 step 329000 avg_loss 2.0612\n",
      "epoch 0 step 329200 avg_loss 2.0611\n",
      "epoch 0 step 329400 avg_loss 2.0612\n",
      "epoch 0 step 329600 avg_loss 2.0611\n",
      "epoch 0 step 329800 avg_loss 2.0612\n",
      "epoch 0 step 330000 avg_loss 2.0613\n",
      "epoch 0 step 330200 avg_loss 2.0613\n",
      "epoch 0 step 330400 avg_loss 2.0613\n",
      "epoch 0 step 330600 avg_loss 2.0614\n",
      "epoch 0 step 330800 avg_loss 2.0613\n",
      "epoch 0 step 331000 avg_loss 2.0614\n",
      "epoch 0 step 331200 avg_loss 2.0614\n",
      "epoch 0 step 331400 avg_loss 2.0614\n",
      "epoch 0 step 331600 avg_loss 2.0614\n",
      "epoch 0 step 331800 avg_loss 2.0612\n",
      "epoch 0 step 332000 avg_loss 2.0612\n",
      "epoch 0 step 332200 avg_loss 2.0611\n",
      "epoch 0 step 332400 avg_loss 2.0610\n",
      "epoch 0 step 332600 avg_loss 2.0610\n",
      "epoch 0 step 332800 avg_loss 2.0609\n",
      "epoch 0 step 333000 avg_loss 2.0609\n",
      "epoch 0 step 333200 avg_loss 2.0610\n",
      "epoch 0 step 333400 avg_loss 2.0608\n",
      "epoch 0 step 333600 avg_loss 2.0607\n",
      "epoch 0 step 333800 avg_loss 2.0607\n",
      "epoch 0 step 334000 avg_loss 2.0608\n",
      "epoch 0 step 334200 avg_loss 2.0607\n",
      "epoch 0 step 334400 avg_loss 2.0608\n",
      "epoch 0 step 334600 avg_loss 2.0608\n",
      "epoch 0 step 334800 avg_loss 2.0607\n",
      "epoch 0 step 335000 avg_loss 2.0608\n",
      "epoch 0 step 335200 avg_loss 2.0608\n",
      "epoch 0 step 335400 avg_loss 2.0607\n",
      "epoch 0 step 335600 avg_loss 2.0606\n",
      "epoch 0 step 335800 avg_loss 2.0605\n",
      "epoch 0 step 336000 avg_loss 2.0606\n",
      "epoch 0 step 336200 avg_loss 2.0606\n",
      "epoch 0 step 336400 avg_loss 2.0607\n",
      "epoch 0 step 336600 avg_loss 2.0607\n",
      "epoch 0 step 336800 avg_loss 2.0606\n",
      "epoch 0 step 337000 avg_loss 2.0606\n",
      "epoch 0 step 337200 avg_loss 2.0606\n",
      "epoch 0 step 337400 avg_loss 2.0605\n",
      "epoch 0 step 337600 avg_loss 2.0605\n",
      "epoch 0 step 337800 avg_loss 2.0604\n",
      "epoch 0 step 338000 avg_loss 2.0605\n",
      "epoch 0 step 338200 avg_loss 2.0604\n",
      "epoch 0 step 338400 avg_loss 2.0604\n",
      "epoch 0 step 338600 avg_loss 2.0604\n",
      "epoch 0 step 338800 avg_loss 2.0605\n",
      "epoch 0 step 339000 avg_loss 2.0604\n",
      "epoch 0 step 339200 avg_loss 2.0604\n",
      "epoch 0 step 339400 avg_loss 2.0604\n",
      "epoch 0 step 339600 avg_loss 2.0604\n",
      "epoch 0 step 339800 avg_loss 2.0604\n",
      "epoch 0 step 340000 avg_loss 2.0605\n",
      "epoch 0 step 340200 avg_loss 2.0605\n",
      "epoch 0 step 340400 avg_loss 2.0605\n",
      "epoch 0 step 340600 avg_loss 2.0606\n",
      "epoch 0 step 340800 avg_loss 2.0606\n",
      "epoch 0 step 341000 avg_loss 2.0606\n",
      "epoch 0 step 341200 avg_loss 2.0606\n",
      "epoch 0 step 341400 avg_loss 2.0606\n",
      "epoch 0 step 341600 avg_loss 2.0608\n",
      "epoch 0 step 341800 avg_loss 2.0607\n",
      "epoch 0 step 342000 avg_loss 2.0608\n",
      "epoch 0 step 342200 avg_loss 2.0607\n",
      "epoch 0 step 342400 avg_loss 2.0607\n",
      "epoch 0 step 342600 avg_loss 2.0607\n",
      "epoch 0 step 342800 avg_loss 2.0606\n",
      "epoch 0 step 343000 avg_loss 2.0606\n",
      "epoch 0 step 343200 avg_loss 2.0607\n",
      "epoch 0 step 343400 avg_loss 2.0607\n",
      "epoch 0 step 343600 avg_loss 2.0607\n",
      "epoch 0 step 343800 avg_loss 2.0607\n",
      "epoch 0 step 344000 avg_loss 2.0607\n",
      "epoch 0 step 344200 avg_loss 2.0606\n",
      "epoch 0 step 344400 avg_loss 2.0606\n",
      "epoch 0 step 344600 avg_loss 2.0605\n",
      "epoch 0 step 344800 avg_loss 2.0605\n",
      "epoch 0 step 345000 avg_loss 2.0605\n",
      "epoch 0 step 345200 avg_loss 2.0604\n",
      "epoch 0 step 345400 avg_loss 2.0604\n",
      "epoch 0 step 345600 avg_loss 2.0604\n",
      "epoch 0 step 345800 avg_loss 2.0603\n",
      "epoch 0 step 346000 avg_loss 2.0602\n",
      "epoch 0 step 346200 avg_loss 2.0603\n",
      "epoch 0 step 346400 avg_loss 2.0602\n",
      "epoch 0 step 346600 avg_loss 2.0602\n",
      "epoch 0 step 346800 avg_loss 2.0601\n",
      "epoch 0 step 347000 avg_loss 2.0600\n",
      "epoch 0 step 347200 avg_loss 2.0599\n",
      "epoch 0 step 347400 avg_loss 2.0599\n",
      "epoch 0 step 347600 avg_loss 2.0598\n",
      "epoch 0 step 347800 avg_loss 2.0598\n",
      "epoch 0 step 348000 avg_loss 2.0598\n",
      "epoch 0 step 348200 avg_loss 2.0597\n",
      "Epoch 0 finished avg_loss 2.0597\n",
      "Epoch 0 FINISHED\n",
      "  Time:       10165.4s\n",
      "  Avg loss:   2.0597\n",
      "  Throughput: 2192.7 samples/sec\n",
      "Saved checkpoint: ..\\models\\sasrec_full_top200000_epoch0.pt\n",
      "=== Phase C COMPLETE ===\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# PHASE C — FULL PRETRAIN (90 SHARDS)\n",
    "# ============================\n",
    "\n",
    "print(\"=== Phase C: Full Pretraining ===\")\n",
    "\n",
    "# ---- CONFIG ----\n",
    "NUM_SHARDS_C = 90          # full number of shards\n",
    "EPOCHS_C = 1               # set to 1 or 2 depending on your time\n",
    "SAVE_EVERY_EPOCH = True\n",
    "MAX_STEPS_PER_EPOCH = None  # leave None for full epoch\n",
    "\n",
    "# Resume from your last/best Phase B checkpoint\n",
    "RESUME_CKPT = Path(\"../models/sasrec_phaseB_top200000_epoch1.pt\")  # <-- CHANGE if needed\n",
    "\n",
    "\n",
    "# ---- LOAD SHARDS ----\n",
    "shard_paths = sorted(SHARD_DIR.glob(\"shard_*.pt\"))\n",
    "assert len(shard_paths) >= NUM_SHARDS_C, f\"Need at least {NUM_SHARDS_C} shards\"\n",
    "\n",
    "train_shards = shard_paths[:NUM_SHARDS_C]\n",
    "\n",
    "print(f\"Training on {len(train_shards)} shards\")\n",
    "print(\"Example:\", train_shards[0].name, \"...\", train_shards[-1].name)\n",
    "\n",
    "\n",
    "# ---- RESUME MODEL & OPTIMIZER ----\n",
    "start_epoch = 0\n",
    "if RESUME_CKPT and RESUME_CKPT.exists():\n",
    "    ck = torch.load(RESUME_CKPT, map_location=device)\n",
    "    model.load_state_dict(ck[\"model_state\"])\n",
    "    optimizer.load_state_dict(ck[\"opt_state\"])\n",
    "    start_epoch = ck.get(\"epoch\", 0) + 1\n",
    "    print(f\"Resumed from {RESUME_CKPT} at epoch {start_epoch}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid RESUME_CKPT found — training from current model state.\")\n",
    "\n",
    "\n",
    "# ---- DATASET & LOADER ----\n",
    "def make_loader(shards, batch_size=BATCH_SIZE):\n",
    "    ds = TensorShardDataset(shards, shuffle_shards=True)\n",
    "    return DataLoader(ds, batch_size=batch_size, collate_fn=collate_shard_batch, num_workers=0)\n",
    "\n",
    "\n",
    "# ---- COUNT SAMPLES ----\n",
    "def total_samples(shard_list):\n",
    "    tot = 0\n",
    "    for sp in shard_list:\n",
    "        d = torch.load(sp)\n",
    "        tot += int(d[\"prefix\"].size(0))\n",
    "    return tot\n",
    "\n",
    "samples_in_shards = total_samples(train_shards)\n",
    "print(\"Total samples across shards:\", samples_in_shards)\n",
    "\n",
    "\n",
    "# ---- TRAIN EPOCHS ----\n",
    "for epoch in range(start_epoch, start_epoch + EPOCHS_C):\n",
    "\n",
    "    loader = make_loader(train_shards, batch_size=BATCH_SIZE)\n",
    "    t0 = time.time()\n",
    "\n",
    "    avg_loss, avg_rec = train_one_epoch(\n",
    "        model,\n",
    "        loader,\n",
    "        optimizer,\n",
    "        scaler,\n",
    "        epoch,\n",
    "        max_steps=MAX_STEPS_PER_EPOCH\n",
    "    )\n",
    "\n",
    "    t1 = time.time()\n",
    "    elapsed = t1 - t0\n",
    "    throughput = samples_in_shards / elapsed if elapsed > 0 else float(\"inf\")\n",
    "\n",
    "    print(f\"Epoch {epoch} FINISHED\")\n",
    "    print(f\"  Time:       {elapsed:.1f}s\")\n",
    "    print(f\"  Avg loss:   {avg_loss:.4f}\")\n",
    "    print(f\"  Throughput: {throughput:.1f} samples/sec\")\n",
    "\n",
    "    if SAVE_EVERY_EPOCH:\n",
    "        ck_path = CKPT_DIR / f\"sasrec_full_top{TOP_N_ITEMS}_epoch{epoch}.pt\"\n",
    "        save_checkpoint(model, optimizer, epoch, ck_path)\n",
    "        print(\"Saved checkpoint:\", ck_path)\n",
    "\n",
    "print(\"=== Phase C COMPLETE ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7081c93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = [0,1,2,   # warm-up\n",
    "          3,4,     # phase B\n",
    "          5]       # phase C\n",
    "\n",
    "losses = [\n",
    "    13.53,   # warm-up epoch 0\n",
    "    7.12,    # warm-up epoch 1\n",
    "    5.81,    # warm-up epoch 2\n",
    "    2.30,    # phase B epoch 0\n",
    "    2.08,    # phase B epoch 1\n",
    "    2.06     # phase C epoch 0\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(epochs, losses, marker='o')\n",
    "plt.title(\"Training Loss Across Pretraining Phases\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session-transfer-mooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
